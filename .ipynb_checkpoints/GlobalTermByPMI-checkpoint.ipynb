{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from TGA.utils import Dataset, GraphsizePretrained\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:23, 17351.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 727 ms, total: 23.4 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,stopwords='keep',\n",
    "                   pretrained_vec='/home/mangaravite/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test'), 22402)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/acm/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=False))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    self.N = len(X)\n",
    "    y_train = self.le.fit( sorted(list(set(y))) ).transform(y)\n",
    "    self.n_class = len(self.le.classes_)\n",
    "    \n",
    "    self.class_term_freqs = Counter()\n",
    "    self.term_freqs = Counter()\n",
    "    self.class_freqs = Counter(y_train)\n",
    "\n",
    "    docs = list(map(self.analyzer.build_analyzer(), self.progress_bar(X)))\n",
    "    self.node_mapper = dict()\n",
    "    pairs = list( zip( docs, y_train ) )\n",
    "    for (doc,y) in self.progress_bar(pairs):\n",
    "\n",
    "        doc_in_terms = set(filter( lambda x: x in self.embeddings_dict, doc))\n",
    "        terms_by_nid = list(map(lambda x: self.node_mapper.setdefault(x, len(self.node_mapper)), doc_in_terms))\n",
    "\n",
    "        self.term_freqs.update( terms_by_nid )\n",
    "        \n",
    "        list_of_edges = list(map( lambda x: (y, x), terms_by_nid ))\n",
    "        self.class_term_freqs.update( list_of_edges )\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22402/22402 [00:05<00:00, 3989.55it/s]\n",
      "100%|██████████| 22402/22402 [00:01<00:00, 19427.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(encoding=None,\n",
       "                    pretrained_vec='/home/mangaravite/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt',\n",
       "                    stopwords='keep', verbose=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(graph_builder,fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples= [ (x,y,v) for ((y,x), v) in graph_builder.class_term_freqs.items() ]\n",
    "terms, clss, fresq = list(zip(*triples))\n",
    "N_all  = np.sum(fresq)\n",
    "n_terms= len(graph_builder.node_mapper)\n",
    "n_class=graph_builder.n_class\n",
    "cjct_terms_lbls = coo_matrix((fresq, (terms, clss)), shape=(n_terms, n_class))/N_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_priors = cjct_terms_lbls.sum(axis=0)\n",
    "terms_priors  = cjct_terms_lbls.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of indepencende P(x)*P(y)\n",
    "indp_mtrx = terms_priors.reshape(-1,1)*labels_priors.reshape(-1,1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36426, 1), (1, 11), (36426, 11), (36426, 11))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_priors.shape, labels_priors.shape, cjct_terms_lbls.shape, indp_mtrx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.12506764, -1.67623058, ..., -0.55210237,\n",
       "         1.18316875, -0.83896323],\n",
       "       [ 0.        ,  1.55300426,  1.10137699, ...,  0.86293513,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        , -0.03195824, -0.48358551, ..., -0.72202737,\n",
       "         2.86124065,  0.57607427],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi = coo_matrix(cjct_terms_lbls/indp_mtrx)\n",
    "pmi.data = np.log2(pmi.data)\n",
    "pmi.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3959991800210737,\n",
       " 0.6589964563694204,\n",
       " array([[ 0.        , -0.0073101 , -0.09473529, ..., -0.03475297,\n",
       "          0.07006604, -0.0426003 ],\n",
       "        [ 0.        ,  0.08307575,  0.05891659, ...,  0.05529377,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        , -0.00162275, -0.03649457, ..., -0.03666261,\n",
       "          0.2142612 ,  0.03869782],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hxy = coo_matrix(cjct_terms_lbls)\n",
    "hxy.data = -np.log2(hxy.data) # h(x,y)= -log2 P(x,y)\n",
    "\n",
    "npmi= coo_matrix(pmi)\n",
    "npmi.data = pmi.data/hxy.data #npmi(x,y) = pmi(x,y)/h(x,y)\n",
    "npmi.min(), npmi.max(), npmi.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplos de termos disciminantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1533,  381, 1745, ...,   29,  604,   31])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_abs = np.abs(npmi.A).sum(axis=1)\n",
    "sorted_index = (-sum_abs).argsort()\n",
    "sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_mapper = { v:k for (k,v) in graph_builder.node_mapper.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1533,\n",
       " 'ada',\n",
       " array([-0.15804539,  0.        , -0.33056095,  0.13467022,  0.        ,\n",
       "         0.        , -0.21008386, -0.28767275, -0.31634962, -0.26262351,\n",
       "        -0.17111696]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_abs.argmax(), inverted_mapper[sum_abs.argmax()], npmi.A[sum_abs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ada', [-0.2, 0.0, -0.3, 0.1, 0.0, 0.0, -0.2, -0.3, -0.3, -0.3, -0.2]),\n",
       " ('circuits',\n",
       "  [-0.0, 0.2, -0.1, -0.3, 0.0, -0.0, -0.1, -0.4, -0.2, -0.0, -0.3]),\n",
       " ('packet', [0.1, -0.2, 0.2, -0.2, 0.0, -0.1, -0.2, -0.3, -0.3, 0.0, -0.2]),\n",
       " ('circuit', [-0.1, 0.2, 0.0, -0.3, 0.0, -0.0, -0.1, -0.4, -0.2, 0.1, -0.3]),\n",
       " ('relational',\n",
       "  [-0.0, -0.3, -0.2, -0.1, 0.0, 0.0, -0.3, 0.1, -0.1, 0.0, -0.3]),\n",
       " ('fortran', [-0.1, -0.1, -0.2, 0.1, 0.0, -0.2, 0.2, -0.3, -0.1, -0.1, -0.1]),\n",
       " ('students',\n",
       "  [-0.1, -0.2, -0.2, -0.1, 0.0, -0.2, -0.2, -0.1, -0.1, -0.1, 0.1]),\n",
       " ('database',\n",
       "  [0.1, -0.2, -0.1, -0.1, -0.1, -0.1, -0.3, 0.1, -0.1, -0.1, -0.1]),\n",
       " ('layout', [-0.1, 0.2, -0.2, -0.1, 0.0, -0.0, -0.2, -0.2, -0.2, 0.0, -0.2]),\n",
       " ('atm', [0.0, -0.2, 0.2, -0.2, 0.0, -0.1, -0.1, -0.3, -0.1, -0.0, -0.2]),\n",
       " ('protocol',\n",
       "  [-0.1, -0.1, 0.1, -0.1, 0.1, -0.1, -0.2, -0.1, -0.1, -0.1, -0.2]),\n",
       " ('databases',\n",
       "  [-0.1, 0.0, -0.1, -0.2, 0.0, -0.2, -0.2, 0.1, -0.1, -0.1, -0.2]),\n",
       " ('retrieval',\n",
       "  [-0.0, -0.3, -0.2, -0.2, 0.0, -0.1, 0.0, 0.1, -0.1, -0.1, -0.2]),\n",
       " ('course', [-0.0, -0.3, -0.2, -0.1, 0.0, -0.1, -0.2, -0.1, -0.1, -0.0, 0.1]),\n",
       " ('wireless', [0.0, -0.2, 0.2, -0.2, 0.0, 0.0, 0.0, -0.2, -0.3, 0.0, -0.2]),\n",
       " ('intersection',\n",
       "  [0.0, -0.2, -0.2, -0.3, 0.0, 0.2, -0.1, -0.0, 0.1, 0.0, -0.2]),\n",
       " ('acm', [0.3, -0.3, -0.2, -0.1, 0.1, 0.0, 0.0, -0.1, -0.1, 0.0, 0.1]),\n",
       " ('channel', [0.0, 0.1, 0.1, -0.2, 0.1, -0.2, 0.0, -0.2, -0.1, 0.0, -0.2]),\n",
       " ('private', [0.0, -0.2, 0.1, -0.0, 0.4, -0.1, -0.1, -0.1, -0.1, 0.0, 0.1]),\n",
       " ('student', [0.0, -0.2, -0.2, -0.1, 0.0, -0.1, -0.1, -0.1, -0.1, 0.0, 0.1]),\n",
       " ('curriculum', [0.0, 0.0, 0.0, -0.3, 0.0, 0.0, 0.0, -0.3, -0.2, -0.1, 0.2]),\n",
       " ('science', [-0.0, -0.2, -0.2, -0.1, -0.0, -0.1, -0.2, -0.1, -0.1, 0.0, 0.1]),\n",
       " ('queries', [-0.0, 0.0, -0.1, -0.2, 0.0, 0.0, -0.2, 0.1, -0.1, 0.0, -0.3]),\n",
       " ('cell', [-0.0, 0.1, 0.1, -0.2, 0.1, 0.0, -0.0, -0.3, -0.0, 0.1, -0.2]),\n",
       " ('object', [-0.1, -0.2, -0.1, 0.1, 0.0, -0.1, -0.2, 0.0, -0.0, -0.2, -0.1]),\n",
       " ('forum', [0.3, -0.2, -0.1, -0.0, 0.1, -0.1, 0.0, -0.2, -0.1, 0.0, 0.1]),\n",
       " ('image', [0.0, -0.2, -0.1, -0.1, 0.0, -0.1, -0.2, 0.0, 0.1, -0.0, -0.2]),\n",
       " ('graphics', [0.0, -0.2, -0.2, -0.1, 0.0, -0.1, -0.1, -0.1, 0.1, 0.1, -0.0]),\n",
       " ('rendering', [0.0, 0.0, -0.2, -0.3, 0.0, 0.0, 0.0, -0.3, 0.2, 0.0, -0.2]),\n",
       " ('newton', [0.0, 0.1, 0.0, -0.0, 0.0, 0.0, 0.3, -0.2, -0.2, 0.1, -0.1]),\n",
       " ('link', [-0.0, -0.1, 0.2, -0.1, 0.1, -0.2, -0.2, -0.0, -0.1, 0.0, -0.1]),\n",
       " ('geometry', [0.1, -0.0, -0.1, -0.2, 0.0, 0.1, 0.0, -0.2, 0.1, 0.1, -0.1]),\n",
       " ('journal', [-0.1, 0.0, -0.3, 0.1, 0.0, -0.1, -0.1, -0.1, -0.2, 0.0, -0.1]),\n",
       " ('tcp', [0.0, -0.2, 0.2, -0.2, 0.0, 0.0, 0.0, -0.2, 0.0, 0.0, -0.2]),\n",
       " ('query', [-0.0, 0.0, -0.1, -0.2, 0.0, -0.1, -0.2, 0.1, -0.1, 0.0, -0.2]),\n",
       " ('switch', [-0.0, 0.1, 0.1, -0.1, 0.0, 0.0, -0.1, -0.3, -0.2, -0.0, -0.0]),\n",
       " ('bryan', [0.5, -0.1, 0.0, -0.0, 0.2, 0.0, 0.0, -0.0, -0.0, 0.0, -0.2]),\n",
       " ('teaching', [-0.0, -0.2, -0.2, -0.1, 0.0, -0.2, 0.0, -0.2, -0.1, -0.0, 0.1]),\n",
       " ('delay', [0.0, 0.1, 0.1, -0.1, 0.0, -0.1, -0.1, -0.2, -0.1, -0.0, -0.2]),\n",
       " ('arrays', [-0.0, 0.1, -0.1, 0.0, 0.2, -0.0, -0.1, -0.2, -0.1, -0.0, -0.1]),\n",
       " ('cad', [0.2, 0.1, 0.0, -0.3, 0.0, -0.1, 0.0, -0.0, -0.0, 0.4, 0.0]),\n",
       " ('compilers', [0.0, -0.1, -0.1, 0.1, 0.0, -0.1, 0.0, -0.2, -0.2, -0.0, -0.2]),\n",
       " ('programmer',\n",
       "  [-0.0, -0.2, -0.1, 0.1, 0.1, -0.1, 0.0, -0.2, -0.1, -0.1, -0.0]),\n",
       " ('cs', [0.0, 0.0, -0.1, -0.2, 0.0, 0.0, -0.1, -0.2, -0.3, 0.0, 0.2]),\n",
       " ('hypermedia', [0.1, -0.2, -0.2, -0.2, 0.0, 0.0, 0.0, 0.1, -0.1, -0.0, -0.1]),\n",
       " ('attracted', [0.3, 0.0, 0.0, -0.1, 0.0, 0.2, 0.0, -0.0, -0.1, 0.3, 0.1]),\n",
       " ('corporate', [0.2, -0.2, -0.1, 0.1, 0.1, 0.0, 0.0, -0.1, -0.1, -0.1, -0.0]),\n",
       " ('linux', [-0.1, 0.0, -0.2, 0.1, 0.0, 0.0, -0.2, -0.2, 0.0, 0.0, -0.2]),\n",
       " ('images', [0.0, -0.2, -0.1, -0.2, 0.0, 0.0, -0.2, 0.0, 0.1, -0.1, -0.1]),\n",
       " ('apl', [0.0, 0.0, -0.2, 0.1, 0.0, -0.1, -0.2, -0.3, -0.2, 0.0, 0.0]),\n",
       " ('ic', [0.1, 0.2, -0.1, -0.2, 0.0, 0.0, 0.0, -0.1, -0.1, 0.1, -0.1]),\n",
       " ('linear', [-0.0, 0.0, -0.1, -0.1, 0.1, 0.1, 0.2, -0.1, 0.0, -0.1, -0.2]),\n",
       " ('compiler', [0.0, 0.0, -0.1, 0.1, 0.0, -0.2, -0.2, 0.0, -0.2, -0.1, -0.1]),\n",
       " ('education', [-0.0, 0.0, -0.3, -0.2, 0.0, 0.0, 0.0, -0.1, -0.1, 0.0, 0.2]),\n",
       " ('shortest', [0.0, 0.0, -0.0, -0.2, 0.0, 0.1, 0.2, -0.2, -0.1, 0.0, -0.1]),\n",
       " ('exponentially',\n",
       "  [0.2, -0.0, 0.1, -0.1, 0.2, 0.1, 0.1, -0.1, -0.1, 0.1, 0.0]),\n",
       " ('transmission',\n",
       "  [0.1, 0.0, 0.1, -0.2, 0.1, -0.1, 0.0, -0.1, -0.0, -0.0, -0.2]),\n",
       " ('reuse', [0.1, -0.0, -0.0, 0.1, 0.0, -0.2, -0.2, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('digital', [-0.0, 0.1, -0.1, -0.1, 0.1, -0.3, -0.1, 0.1, -0.0, 0.1, -0.1]),\n",
       " ('denning', [0.2, 0.0, -0.1, -0.1, 0.2, 0.0, 0.0, -0.1, -0.1, 0.0, 0.1]),\n",
       " ('technology', [0.0, 0.0, -0.0, -0.1, 0.0, -0.3, -0.2, 0.0, -0.1, 0.1, 0.1]),\n",
       " ('courses', [-0.1, 0.0, -0.2, -0.1, 0.0, 0.0, 0.0, -0.1, -0.2, -0.1, 0.2]),\n",
       " ('packets', [0.0, 0.0, 0.2, -0.3, 0.0, 0.0, -0.1, 0.0, -0.2, 0.0, -0.2]),\n",
       " ('chip', [0.0, 0.2, 0.0, -0.1, 0.0, -0.1, 0.0, -0.2, -0.1, 0.0, -0.2]),\n",
       " ('fabrication', [0.2, 0.2, 0.0, -0.1, 0.0, 0.0, 0.0, -0.1, -0.1, 0.2, 0.0]),\n",
       " ('mobile', [-0.1, 0.0, 0.2, -0.1, 0.0, -0.1, 0.0, -0.1, -0.1, -0.1, -0.2]),\n",
       " ('loop', [-0.0, 0.1, 0.0, 0.0, 0.1, 0.0, -0.2, -0.3, -0.0, -0.0, -0.2]),\n",
       " ('dimensional', [-0.1, 0.0, -0.1, -0.1, 0.0, 0.1, 0.1, -0.0, 0.1, 0.1, -0.3]),\n",
       " ('timing', [-0.0, 0.2, -0.0, -0.0, 0.0, -0.0, 0.0, -0.2, -0.2, -0.1, -0.2]),\n",
       " ('year', [0.1, -0.2, -0.1, -0.0, 0.0, -0.1, -0.1, -0.1, -0.0, 0.1, 0.1]),\n",
       " ('introductory',\n",
       "  [0.0, 0.0, -0.2, -0.1, 0.0, -0.1, 0.0, -0.2, -0.1, -0.0, 0.2]),\n",
       " ('logic', [-0.0, 0.1, -0.1, -0.0, 0.0, 0.1, -0.2, -0.1, -0.0, -0.1, -0.1]),\n",
       " ('differential', [0.0, -0.0, -0.1, -0.1, 0.2, 0.0, 0.2, -0.2, 0.0, 0.0, 0.0]),\n",
       " ('command', [0.0, -0.2, -0.2, 0.1, 0.0, 0.0, -0.1, 0.0, -0.1, -0.0, -0.1]),\n",
       " ('fastest', [0.0, 0.0, -0.1, -0.1, 0.3, 0.1, 0.2, -0.1, -0.1, 0.0, -0.0]),\n",
       " ('switches', [0.1, 0.1, 0.1, -0.1, 0.0, -0.1, -0.0, -0.1, -0.2, 0.0, -0.2]),\n",
       " ('topology', [-0.0, 0.0, 0.1, -0.3, 0.0, -0.1, -0.1, -0.1, 0.0, 0.0, -0.2]),\n",
       " ('architectures',\n",
       "  [0.0, 0.1, 0.1, 0.0, 0.0, -0.2, -0.0, -0.1, -0.1, -0.1, -0.3]),\n",
       " ('cellular', [0.0, -0.0, 0.1, -0.2, 0.0, -0.1, 0.0, -0.2, -0.1, 0.1, -0.2]),\n",
       " ('objects', [0.0, -0.2, -0.1, 0.0, -0.1, -0.0, -0.2, 0.0, 0.0, -0.1, -0.1]),\n",
       " ('polygon', [0.0, 0.0, 0.0, -0.3, 0.0, 0.1, -0.0, -0.2, 0.1, 0.0, -0.1]),\n",
       " ('simulation',\n",
       "  [-0.0, 0.1, 0.0, -0.1, 0.0, -0.1, -0.1, -0.2, 0.1, -0.0, -0.2]),\n",
       " ('video', [0.0, -0.1, 0.0, -0.2, 0.0, -0.1, -0.2, 0.1, -0.0, -0.1, -0.1]),\n",
       " ('verification',\n",
       "  [0.0, 0.1, -0.1, 0.0, 0.0, 0.0, -0.1, -0.2, -0.0, -0.1, -0.3]),\n",
       " ('learning', [-0.1, -0.2, -0.2, -0.1, 0.0, -0.0, -0.1, -0.0, 0.1, 0.0, 0.1]),\n",
       " ('university',\n",
       "  [-0.0, -0.1, -0.1, -0.1, 0.0, -0.2, -0.2, -0.0, -0.0, 0.1, 0.1]),\n",
       " ('world', [-0.1, -0.3, -0.1, -0.0, 0.1, -0.2, -0.1, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('architectural',\n",
       "  [-0.0, 0.1, 0.1, 0.0, 0.0, -0.1, 0.0, -0.1, -0.1, -0.1, -0.3]),\n",
       " ('chips', [0.0, 0.2, -0.1, -0.1, 0.0, -0.1, 0.0, -0.2, -0.1, 0.1, -0.1]),\n",
       " ('business', [0.1, 0.0, -0.1, -0.1, 0.0, -0.2, -0.2, 0.0, -0.1, 0.1, 0.1]),\n",
       " ('dense', [0.0, 0.0, -0.1, -0.1, 0.2, 0.1, 0.2, -0.1, 0.0, 0.1, 0.0]),\n",
       " ('minimization',\n",
       "  [0.0, 0.2, -0.2, -0.1, 0.0, -0.0, 0.1, -0.1, -0.1, 0.0, -0.2]),\n",
       " ('sparse', [0.0, -0.1, -0.1, -0.1, 0.1, 0.1, 0.2, -0.2, 0.0, 0.0, 0.0]),\n",
       " ('security', [0.0, -0.1, 0.0, 0.0, 0.2, -0.1, 0.0, -0.0, -0.2, -0.1, 0.1]),\n",
       " ('throughput',\n",
       "  [-0.0, 0.0, 0.2, -0.1, 0.0, -0.1, -0.1, -0.1, -0.2, -0.0, 0.0]),\n",
       " ('workshop', [0.2, -0.1, -0.2, 0.0, 0.0, -0.1, -0.1, 0.0, -0.1, 0.0, 0.0]),\n",
       " ('mechanism', [0.0, -0.0, 0.1, 0.0, 0.0, -0.1, -0.2, -0.0, -0.1, -0.1, -0.2]),\n",
       " ('equations', [0.0, -0.0, -0.1, -0.1, 0.1, 0.1, 0.2, -0.3, 0.0, -0.0, 0.0]),\n",
       " ('visual', [0.0, -0.2, -0.2, -0.0, 0.0, -0.1, -0.1, 0.1, 0.1, -0.1, -0.1]),\n",
       " ('experiences',\n",
       "  [-0.1, -0.2, -0.1, 0.0, 0.0, -0.2, -0.2, 0.0, -0.1, 0.1, 0.1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=100\n",
    "list(zip([inverted_mapper[x] for x in sorted_index[:n]], np.round(npmi.A[list(sorted_index[:n])], decimals=1).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering semantic-syntatic\n",
    "\n",
    "Infer relationship between nPMI and Label->Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(vec, mask=None, dim=1, epsilon=1e-5):\n",
    "    exps = torch.exp(vec)\n",
    "    if mask is None:\n",
    "        mask = vec > 0.\n",
    "    masked_exps = exps * mask.float()\n",
    "    masked_sums = masked_exps.sum(dim, keepdim=True) + epsilon\n",
    "    return (masked_exps/masked_sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalPMI(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=0.3, drop_global=.5, loss=None, device=torch.device('cuda')):\n",
    "        super(GlobalPMI, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.global_term_encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_global),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.local_term_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.label_encoder = nn.Sequential(\n",
    "            nn.Linear(n_classes, hidden_dim),\n",
    "            nn.BatchNorm1d( hidden_dim ),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.y = torch.eye( n_classes, n_classes ).to(device)\n",
    "        \n",
    "        self.att     = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        \"\"\"self.att_act = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\"\"\"\n",
    "        \n",
    "        self.doc_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, n_classes),\n",
    "            nn.Softmax()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.loss = loss.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, term_emb, term_mask, doc_mask):\n",
    "        batch_size = doc_mask.max()+1\n",
    "        \n",
    "        h_label = self.label_encoder(self.y)\n",
    "        h_term  = self.global_term_encoder(term_emb)\n",
    "        \n",
    "        h_local_term = self.local_term_encoder(h_term[term_mask])\n",
    "        docs = torch.rand((batch_size, self.hidden_dim))\n",
    "        \n",
    "        for docid in range(batch_size):\n",
    "            doc_term_emb = h_local_term[doc_mask == docid]\n",
    "            alpha = self.att(doc_term_emb)\n",
    "            alpha = F.softmax(doc_term_emb)\n",
    "            \n",
    "            #act = self.att_act(alpha)\n",
    "            #act = masked_softmax( act )\n",
    "            #docs[docid] = (act*alpha*doc_term_emb).mean(dim=0)\n",
    "            \n",
    "            docs[docid] = (alpha*doc_term_emb).mean(dim=0)\n",
    "            \n",
    "        docs = self.doc_encoder( docs.to(self.device) )\n",
    "        docs_probs = self.fc( docs )\n",
    "        return h_term, h_label, docs, docs_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy\n",
    "class NpairLoss(nn.Module):\n",
    "    \"\"\"the multi-class n-pair loss\"\"\"\n",
    "    def __init__(self, l2_reg=0.02):\n",
    "        super(NpairLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, anchor, target, positive=None):\n",
    "        batch_size = anchor.size(0)\n",
    "        target = target.view(target.size(0), 1)\n",
    "\n",
    "        target = (target == torch.transpose(target, 0, 1)).float()\n",
    "        target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
    "\n",
    "        if positive is not None:\n",
    "            logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
    "        else:\n",
    "            logit = torch.matmul(anchor, torch.transpose(anchor, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size\n",
    "        \n",
    "        loss_ce = cross_entropy(logit, target)\n",
    "\n",
    "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
    "        return loss\n",
    "class TGALoss(torch.nn.Module):\n",
    "    def __init__(self, l2_reg=5e-4):\n",
    "        super(TGALoss, self).__init__()\n",
    "        self.sigma    = nn.Parameter(torch.ones(3))\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss  = nn.CrossEntropyLoss()\n",
    "        self.npl_loss = NpairLoss(l2_reg=l2_reg)\n",
    "\n",
    "    def forward(self, loss_term_npmi, loss_doc_class, loss_cross_ent, explain=False):\n",
    "        \n",
    "        loss_term_npmi = loss_term_npmi/(self.sigma[0]**2) + torch.log(self.sigma[0])\n",
    "        loss_doc_class = loss_doc_class/(2.*self.sigma[1]**2) + torch.log(self.sigma[1])\n",
    "        loss_cross_ent = loss_cross_ent/(2.*self.sigma[2]**2) + torch.log(self.sigma[2])\n",
    "        \n",
    "        loss = (loss_term_npmi + loss_doc_class + loss_cross_ent).mean()\n",
    "        \n",
    "        if explain:\n",
    "            explanation = {\n",
    "                'term_npmi': (loss_term_npmi.mean().item(), self.sigma[0].item()),\n",
    "                'doc_class': (loss_doc_class.mean().item(), self.sigma[1].item()),\n",
    "                'cross_ent': (loss_cross_ent.mean().item(), self.sigma[2].item())\n",
    "            }\n",
    "            return loss, explanation\n",
    "        return loss\n",
    "class SelfDistLoss(nn.Module):\n",
    "    def __init__(self, l2_reg=0.02, eps = 0.00003):\n",
    "        super(SelfDistLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, hiddens):\n",
    "        L = torch.matmul(hiddens, hiddens.T)\n",
    "        L = F.sigmoid(L)\n",
    "        L = (L - L.diag()).float()\n",
    "        L = F.relu(L)\n",
    "        L = ( L > 0. ).float() * torch.exp( L )\n",
    "        #L = F.normalize(L)\n",
    "        \n",
    "        values = L.sum(axis=1)\n",
    "        svalue = max((values > 0.).sum(), self.eps)\n",
    "\n",
    "        return values.sum()/svalue # AVG of non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlobalPMI(\n",
       "  (global_term_encoder): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (local_term_encoder): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (label_encoder): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=300, bias=True)\n",
       "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (att): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (doc_encoder): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=300, out_features=11, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       "  (loss): TGALoss(\n",
       "    (mse_loss): MSELoss()\n",
       "    (ce_loss): CrossEntropyLoss()\n",
       "    (npl_loss): NpairLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgaloss = TGALoss()\n",
    "\n",
    "globalpmi = GlobalPMI(300, 300, dataset.nclass, drop=0.1, loss=tgaloss)\n",
    "globalpmi = globalpmi.cuda()\n",
    "globalpmi.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "    analyzer = self.analyzer.build_analyzer()\n",
    "    local_mapper = dict()\n",
    "    term_emb = []\n",
    "    term_mask = []\n",
    "    doc_mask = []\n",
    "    for i,doc in enumerate(X):\n",
    "        doc_in_terms = analyzer(doc)\n",
    "        doc_in_terms = set(filter( lambda x: x in self.embeddings_dict, doc_in_terms))\n",
    "        #print(doc_in_terms)\n",
    "        for tid in doc_in_terms:\n",
    "            if tid not in local_mapper:\n",
    "                local_mapper[tid] = len(local_mapper)\n",
    "                term_emb.append( self.embeddings_dict[tid] )\n",
    "            term_mask.append( local_mapper[tid] )\n",
    "            doc_mask.append( i )\n",
    "    return term_emb, term_mask, doc_mask, local_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    term_emb, term_mask, doc_mask, local_mapper = transform(graph_builder, X)\n",
    "    \n",
    "    return term_emb, term_mask, doc_mask, local_mapper, y\n",
    "\n",
    "def get_term_emb( local_mapper, term_emb ):\n",
    "    local_terms = list(zip(*sorted(local_mapper.items(), key=lambda x: x[1] )))[0]\n",
    "\n",
    "    idx_local  = [ local_mapper[x] for x in local_terms if x in graph_builder.node_mapper ] \n",
    "    idx_global = [ graph_builder.node_mapper[x] for x in local_terms if x in graph_builder.node_mapper ]\n",
    "\n",
    "    filtered_npmi = npmi.A[ idx_global ]\n",
    "\n",
    "    sum_abs = np.abs(filtered_npmi).sum(axis=1)\n",
    "    sorted_index = (-sum_abs).argsort()\n",
    "    sorted_index = sorted_index[ :int(p*len(idx_global)) ]\n",
    "\n",
    "    term_emb_pred  = term_emb[[idx_local[x] for x in sorted_index]]\n",
    "    term_npmi      = torch.Tensor(filtered_npmi[sorted_index])\n",
    "    \n",
    "    return term_emb_pred, term_npmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "p=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD( globalpmi.parameters(), lr=5e-3, momentum=0.9)\n",
    "optimizer = optim.AdamW( globalpmi.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "\n",
    "mse_loss = nn.MSELoss().to(device)\n",
    "ce_loss  = nn.CrossEntropyLoss().to(device)\n",
    "npl_loss = NpairLoss(l2_reg=5e-4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b24fad2ac14c9682aa36cc0b809b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a92956fbbcc4a109c72487ff4b07bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=22402.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.3072, Lss1: 0.6861, Lss2: 0.602, Lss2: 2.322\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba22a914f9154d0d8fa0e7a0910898da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=22402.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.3078, Lss1: 0.2591, Lss2: 0.6975, Lss2: 2.043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33f532e9aa74034948e4cbd7b3240c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=22402.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.2964, Lss1: 0.339, Lss2: 3.761, Lss2: 2.1457\r"
     ]
    }
   ],
   "source": [
    "best = None\n",
    "nepochs = 10\n",
    "globalpmi.train()\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_loss1 = 0\n",
    "    epoch_loss2 = 0\n",
    "    epoch_loss3 = 0\n",
    "    TP = 0\n",
    "    Ndocs = 0\n",
    "    y_train = graph_builder.le.transform(fold.y_train)\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=4)\n",
    "    with tqdm(total=len(fold.y_train), smoothing=0.) as pbar:\n",
    "        total = 1\n",
    "        correct = 0\n",
    "        for i, (term_emb, term_mask, doc_mask, local_mapper, y) in enumerate(data_loader):\n",
    "            term_emb = torch.Tensor(term_emb).to(device)\n",
    "            term_mask = torch.LongTensor(term_mask).to(device)\n",
    "            doc_mask = torch.LongTensor(doc_mask).to(device)\n",
    "            y = torch.LongTensor(y).to(device)\n",
    "            \n",
    "            h_term, h_label, h_docs, y_probs = globalpmi(term_emb, term_mask, doc_mask)\n",
    "            \n",
    "            h_term, h_label, h_docs, y_probs = h_term.to(device), h_label.to(device), h_docs.to(device), y_probs.to(device)\n",
    "            \n",
    "            term_emb_pred, term_npmi = get_term_emb( local_mapper, term_emb )\n",
    "            term_emb_pred, term_npmi = term_emb_pred.to(device), term_npmi.to(device)\n",
    "            \n",
    "            loss1 = mse_loss( term_npmi, torch.matmul( term_emb_pred, h_label.T ) )\n",
    "            loss2 = npl_loss(h_docs, y, h_label[y])\n",
    "            loss3 = ce_loss( y_probs, y )\n",
    "            \n",
    "            epoch_loss1 += loss1\n",
    "            epoch_loss2 += loss2\n",
    "            epoch_loss3 += loss3\n",
    "            \n",
    "            #loss = loss2 + loss3\n",
    "            loss = tgaloss( loss1, loss2, loss3 )\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            TP += (y_probs.argmax(dim=1)==y).sum()\n",
    "            Ndocs += len(y)\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'Lss: {epoch_loss/(i+1):.4} \\\n",
    "                    Lss_i: ({epoch_loss1/(i+1):.4};{epoch_loss2/(i+1):.4};{epoch_loss3/(i+1):.4})')\n",
    "            print(f'Acc: {TP/Ndocs:.4}, Lss1: {loss1:.4}, Lss2: {loss2:.4}, Lss2: {loss3:.4}' , end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACM: Acc: 0.5857, Lss1: 0.01755, Lss2: 0.004775\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_label.shape, y, graph_builder.n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_terms_emb = term_emb[ term_mask[doc_mask == 0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_terms_emb.sum(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(doc_terms_emb.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs.argmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
