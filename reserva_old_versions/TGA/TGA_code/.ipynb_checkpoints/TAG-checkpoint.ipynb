{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset, Graphsize\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documents/datasets/topics/acm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = next(dataset.get_fold_instances(10))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19907/19907 [00:13<00:00, 1449.35it/s]\n",
      "100%|██████████| 19907/19907 [00:05<00:00, 3861.63it/s]\n",
      "100%|██████████| 19907/19907 [01:06<00:00, 299.39it/s]\n",
      "100%|██████████| 19907/19907 [00:12<00:00, 1577.99it/s]\n",
      "100%|██████████| 2495/2495 [00:00<00:00, 3950.38it/s]\n",
      "100%|██████████| 2495/2495 [00:07<00:00, 325.67it/s]\n",
      "100%|██████████| 2495/2495 [00:01<00:00, 1630.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 932 ms, total: 1min 48s\n",
      "Wall time: 1min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_feats = 250000\n",
    "graph_builder = Graphsize(w=5, verbose=True, max_feat=max_feats)\n",
    "Gs_train = graph_builder.fit_transform(fold.X_train)\n",
    "Gs_val   = graph_builder.transform(fold.X_val)\n",
    "#Gs_test  = graph_builder.transform(fold.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_heads, n_classes, drop=0.5, k=2):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            #GraphConv(in_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            #GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))])\n",
    "            GATConv(in_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=drop).to(torch.device('cuda:0')),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=drop).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        self.lin = nn.Linear(n_heads*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.conv = nn.Linear(n_heads*hidden_dim, k*hidden_dim).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.classify = nn.Linear(k*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        hg = self.pooling(G, h)\n",
    "        #G.ndata['h'] = h\n",
    "        #w = self.lin( h )\n",
    "        #G.ndata['w'] = w\n",
    "        #hg = dgl.mean_nodes(G, 'h', weight='w')\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.conv( hg )\n",
    "        pred = self.classify( hg )\n",
    "        return pred\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        hg = self.pooling(G, h)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.conv(hg)\n",
    "        return hg\n",
    "    def predict(self, G):\n",
    "        hg = self.transform( G )\n",
    "        pred = self.classify( hg )\n",
    "        probs_Y = torch.softmax(pred, 1)\n",
    "        sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "        return sampled_Y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    Gs_Fs, labels = map(list, zip(*samples))\n",
    "    graphs = []\n",
    "    for g, f in Gs_Fs:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g)\n",
    "        g_dgl.ndata['f'] = torch.FloatTensor(f.A).to(torch.device('cuda:0'))\n",
    "        g_dgl.to(torch.device('cuda:0'))\n",
    "        graphs.append(g_dgl)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.to(torch.device('cuda:0'))\n",
    "    labels = torch.tensor(labels).to(torch.device('cuda:0'))\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # https://github.com/mbsariyildiz/focal-loss.pytorch\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val  = DataLoader(list(zip(Gs_val,  fold.y_val )), batch_size=test_val_batch_size,\n",
    "                              shuffle=True, collate_fn=collate)\n",
    "best_score = None\n",
    "for (noised,qtd_noised) in [ (False, 0), (True, 3), (True, 5) ]:\n",
    "    for lr in [ .01, .001, .0001 ]:\n",
    "        for drop in [ .1, .3, .5, .7 ]:\n",
    "            for l2 in [ 5e-1, 5e-3, 5e-5 ]:\n",
    "                # hyper-params\n",
    "                PATH = 'best_param.pth'\n",
    "                n_epochs = 100\n",
    "                patience = 10\n",
    "                hidden_dim = 300\n",
    "                n_heads = 16\n",
    "                train_batch_size = 16\n",
    "                test_val_batch_size = 256\n",
    "\n",
    "                model = Classifier(len(graph_builder.vocab), hidden_dim, n_heads, dataset.nclass, drop=drop).to(torch.device('cuda:0'))\n",
    "\n",
    "                loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "                loss_eval_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "\n",
    "                optimizer = optim.Adam( model.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "                model.train()\n",
    "                torch.cuda.synchronize()\n",
    "                epoch_losses = []\n",
    "                n_iters = 0\n",
    "                \n",
    "                for epoch in range(n_epochs):\n",
    "                    print(\"lr:{.3} drop:{.3} l2:{.3}\".format(lr, drop, l2))\n",
    "                    data_loader = DataLoader(list(zip(Gs_train, fold.y_train)), batch_size=train_batch_size,\n",
    "                                             shuffle=True, collate_fn=collate)\n",
    "                    epoch_loss = 0\n",
    "                    with tqdm_notebook(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "                        t0 = time()\n",
    "                        total = 0\n",
    "                        correct = 0\n",
    "                        model.train()\n",
    "                        for i, (bg, label) in enumerate(data_loader):\n",
    "                            if noised and i % qtd_noised == 0:\n",
    "                                np.random.shuffle(label)\n",
    "                            outputs = model(bg)\n",
    "                            probs_Y = torch.softmax(outputs, 1)\n",
    "                            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "                            loss = loss_func(outputs, label)\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            epoch_loss += loss.detach().item()\n",
    "\n",
    "                            qtd_docs = label.size(0)\n",
    "                            qtd_correct = (sampled_Y == label).sum().item()\n",
    "                            total += qtd_docs\n",
    "                            correct += qtd_correct\n",
    "\n",
    "                            del loss, outputs, bg, probs_Y, sampled_Y\n",
    "                            pbar.update( len(label) )\n",
    "                            print('iter {}, train loss {:.2f} train acc: {:.3f}'.format(epoch, epoch_loss / (epoch + 1), (correct/total)), end='\\r')\n",
    "                        score_train = correct/total\n",
    "                    with tqdm_notebook(total=len(data_loader_val.dataset), smoothing=0.) as pbar:\n",
    "                        with torch.no_grad():\n",
    "                            total = 0\n",
    "                            correct = 0\n",
    "                            epoch_loss = 0\n",
    "                            model.eval()\n",
    "                            for bg, label in data_loader_val:\n",
    "                                outputs = model(bg)\n",
    "                                probs_Y = torch.softmax(outputs, 1)\n",
    "                                sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "                                #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "                                loss = loss_eval_func(outputs, label)\n",
    "                                epoch_loss += loss.item()\n",
    "\n",
    "                                qtd_docs = label.size(0)\n",
    "                                qtd_correct = (sampled_Y == label).sum().item()\n",
    "                                total += qtd_docs\n",
    "                                correct += qtd_correct\n",
    "\n",
    "                                del probs_Y, outputs, bg, sampled_Y\n",
    "                                pbar.update( label.size(0) )\n",
    "                                score_val = correct/total\n",
    "\n",
    "                                print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "                                #break\n",
    "                            #break\n",
    "                            print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3}/{} )'.format(score_val, epoch_loss, correct, total, score_val/score_train, n_iters), end='\\r')\n",
    "                            score = correct/total\n",
    "                            if best_score is None or score > best_score:\n",
    "                                torch.save(model, PATH)\n",
    "                                best_score = score\n",
    "                                n_iters = 0\n",
    "                            else:\n",
    "                                n_iters += 1\n",
    "                                if n_iters >= patience:\n",
    "                                    print()\n",
    "                                    print('BEST val acc {:.3f}'.format(best_score), end='\\r')\n",
    "                                    break\n",
    "                            print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3}/{} )'.format(score_val, epoch_loss, correct, total, score_val/score_train, n_iters), end='\\r')\n",
    "                        epoch_loss /= (epoch + 1)\n",
    "                        epoch_losses.append(epoch_loss)\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "PATH = 'best_param.pth'\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "hidden_dim = 300\n",
    "n_heads = 16\n",
    "train_batch_size = 16\n",
    "test_val_batch_size = 256\n",
    "\n",
    "model = Classifier(len(graph_builder.vocab), hidden_dim, n_heads, dataset.nclass, drop=0.7).to(torch.device('cuda:0'))\n",
    "\n",
    "# DEPOIS TESTAR COM FOCAL-LOSS https://github.com/mbsariyildiz/focal-loss.pytorch/blob/master/focalloss.py\n",
    "\n",
    "#loss_func = FocalLoss().to(torch.device('cuda:0'))\n",
    "#loss_func = nn.NLLLoss().to(torch.device('cuda:0'))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "loss_eval_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "\n",
    "#optimizer = optim.Adam( model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "\n",
    "optimizer = optim.RMSprop( model.parameters(), lr=0.0001, weight_decay=5e-3 )\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()\n",
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader_test = DataLoader(list(zip(Gs_test, fold.y_test)), batch_size=test_val_batch_size,\n",
    "#                              shuffle=True, collate_fn=collate)\n",
    "data_loader_val  = DataLoader(list(zip(Gs_val,  fold.y_val )), batch_size=test_val_batch_size,\n",
    "                              shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a27a226725451185fede6faf56c827"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, train loss 2641.27 train acc: 0.372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51723cf49ee416b915af05a1d912023"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.193 val loss 24.8 ( 481/2495. over: 0.518/0 )\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GATConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LeakyReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GlobalAttentionPooling. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/mangaravite/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.193 val loss 24.8 ( 481/2495. over: 0.518/0 )\r\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab27e27ca064112a8a00396b160b94f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1, train loss 1013.45 train acc: 0.488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf5c20e6ad9437e848a13750b4968b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.192 val loss 24.6 ( 478/2495. over: 0.393/1 )\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1372079c03c947df98ef1aaf9da7a448"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2, train loss 321.02 train acc: 0.521\r"
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "n_iters = 0\n",
    "qtd_noised = 3\n",
    "noised = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(list(zip(Gs_train, fold.y_train)), batch_size=train_batch_size,\n",
    "                             shuffle=True, collate_fn=collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm_notebook(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for i, (bg, label) in enumerate(data_loader):\n",
    "            if noised and i % qtd_noised == 0:\n",
    "                np.random.shuffle(label)\n",
    "            outputs = model(bg)\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            loss = loss_func(outputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            qtd_docs = label.size(0)\n",
    "            qtd_correct = (sampled_Y == label).sum().item()\n",
    "            total += qtd_docs\n",
    "            correct += qtd_correct\n",
    "            \n",
    "            del loss, outputs, bg, probs_Y, sampled_Y\n",
    "            pbar.update( len(label) )\n",
    "            print('iter {}, train loss {:.2f} train acc: {:.3f}'.format(epoch, epoch_loss / (epoch + 1), (correct/total)), end='\\r')\n",
    "        score_train = correct/total\n",
    "    with tqdm_notebook(total=len(data_loader_val.dataset), smoothing=0.) as pbar:\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            epoch_loss = 0\n",
    "            model.eval()\n",
    "            for bg, label in data_loader_val:\n",
    "                outputs = model(bg)\n",
    "                probs_Y = torch.softmax(outputs, 1)\n",
    "                sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "                #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "                loss = loss_eval_func(outputs, label)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                qtd_docs = label.size(0)\n",
    "                qtd_correct = (sampled_Y == label).sum().item()\n",
    "                total += qtd_docs\n",
    "                correct += qtd_correct\n",
    "                \n",
    "                del probs_Y, outputs, bg, sampled_Y\n",
    "                pbar.update( label.size(0) )\n",
    "                score_val = correct/total\n",
    "                \n",
    "                print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "                #break\n",
    "            #break\n",
    "            print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3}/{} )'.format(score_val, epoch_loss, correct, total, score_val/score_train, n_iters), end='\\r')\n",
    "            score = correct/total\n",
    "            if best_score is None or score > best_score:\n",
    "                torch.save(model, PATH)\n",
    "                best_score = score\n",
    "                n_iters = 0\n",
    "            else:\n",
    "                n_iters += 1\n",
    "                if n_iters >= patience:\n",
    "                    print()\n",
    "                    print('BEST val acc {:.3f}'.format(best_score), end='\\r')\n",
    "                    break\n",
    "            print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3}/{} )'.format(score_val, epoch_loss, correct, total, score_val/score_train, n_iters), end='\\r')\n",
    "        epoch_loss /= (epoch + 1)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.203 val loss 22.8 ( 507/2495. over: 0.274 )\r"
     ]
    }
   ],
   "source": [
    "model = torch.load(PATH)\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    for bg, label in data_loader_val:\n",
    "        outputs = model(bg)\n",
    "        probs_Y = torch.softmax(outputs, 1)\n",
    "        sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "        #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "        loss = loss_eval_func(outputs, label)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        qtd_docs = label.size(0)\n",
    "        qtd_correct = (sampled_Y == label).sum().item()\n",
    "        total += qtd_docs\n",
    "        correct += qtd_correct\n",
    "\n",
    "        del probs_Y, outputs, bg, sampled_Y\n",
    "        pbar.update( label.size(0) )\n",
    "        score_val = correct/total\n",
    "\n",
    "        print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "    #break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19907 + 2495 = 22402\n",
      "19907 + 2495 = 22402\n"
     ]
    }
   ],
   "source": [
    "print( len(Gs_train), '+', len(Gs_val), end=' = ' )\n",
    "Gs_train_val = Gs_train + Gs_val\n",
    "print( len(Gs_train_val) )\n",
    "\n",
    "print( len(fold.y_train), '+', len(fold.y_val), end=' = ' )\n",
    "y_train_val = fold.y_train + fold.y_val\n",
    "print( len(y_train_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate2(samples):\n",
    "    Gs_Fs, labels = map(list, zip(*samples))\n",
    "    graphs = []\n",
    "    for g, f in Gs_Fs:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g)\n",
    "        g_dgl.ndata['f'] = torch.FloatTensor(f.A).to(torch.device('cuda:0'))\n",
    "        g_dgl.to(torch.device('cuda:0'))\n",
    "        graphs.append(g_dgl)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.to(torch.device('cuda:0'))\n",
    "    labels = torch.tensor(labels).to(torch.device('cuda:0'))\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3246999dc5f94c3cb012e47d7796cebe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader_train_val_t  = DataLoader(list(zip(Gs_train_val, y_train_val)), batch_size=test_val_batch_size,\n",
    "                              shuffle=False, collate_fn=collate2)\n",
    "X_train_val_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm_notebook(total=len(data_loader_train_val_t.dataset), smoothing=0.) as pbar:\n",
    "        for G, label in data_loader_train_val_t:\n",
    "            X_train_val_t = model.transform( G ).cpu().numpy()\n",
    "            X_train_val_all.append( X_train_val_t )\n",
    "            pbar.update( len(label) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2178266 , -0.09567951,  0.43155062, ...,  0.01292455,\n",
       "        -0.22324216, -0.58197975],\n",
       "       [-0.3348459 ,  0.01925122,  0.4095029 , ...,  0.29597798,\n",
       "        -0.09173325, -0.61389023],\n",
       "       [-0.2497353 ,  0.0744639 ,  0.2100017 , ...,  0.19944745,\n",
       "        -0.12531002, -0.115013  ],\n",
       "       ...,\n",
       "       [-0.17952038, -0.10148983,  0.7563992 , ...,  0.02897497,\n",
       "        -0.03622607, -0.26277977],\n",
       "       [ 0.180725  , -0.06271119,  0.24564497, ...,  0.00429483,\n",
       "         0.05862079, -0.2594659 ],\n",
       "       [-0.21408391,  0.18337913,  0.23021354, ...,  0.3431807 ,\n",
       "         0.30042037, -0.45386586]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val_all2 = np.concatenate( X_train_val_all )\n",
    "X_train_val_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done  16 out of  35 | elapsed: 22.1min remaining: 26.2min\n",
      "[Parallel(n_jobs=64)]: Done  35 out of  35 | elapsed: 28.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1, class_weight=None, dual=False,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.001, verbose=0),\n",
       "             iid=True, n_jobs=64,\n",
       "             param_grid=[{'C': array([3.125e-02, 1.250e-01, 5.000e-01, 2.000e+00, 8.000e+00, 3.200e+01,\n",
       "       1.280e+02])}],\n",
       "             pre_dispatch='2*n_jobs', refit=False, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'loss': 'squared_hinge', 'C': 1, 'verbose': 0,\n",
    "         'intercept_scaling': 1, 'fit_intercept': True,\n",
    "         'max_iter': 1000, 'penalty': 'l2', 'multi_class': 'ovr',\n",
    "         'random_state': None, 'dual': False,'tol': 0.001,\n",
    "         'class_weight': None}\n",
    "estimator = LinearSVC(**param)\n",
    "tunning = [{'C': 2.0 ** np.arange(-5, 9, 2)}]\n",
    "\n",
    "gs = GridSearchCV(estimator, tunning,\n",
    "                n_jobs=64, refit=False,\n",
    "                cv=5, iid=True,\n",
    "                verbose=2, scoring='f1_micro')\n",
    "\n",
    "gs.fit( X_train_val_all2, y_train_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [00:00<00:00, 3820.70it/s]\n",
      "100%|██████████| 2495/2495 [00:07<00:00, 335.46it/s]\n",
      "100%|██████████| 2495/2495 [00:01<00:00, 1676.05it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56556626fafc4c308e3c28cbc6e1a308"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.23167111,  0.21653058,  0.06046243, ..., -0.00840951,\n",
       "         0.20930912, -0.13022415],\n",
       "       [-0.21017554,  0.19031881,  0.38463926, ...,  0.29666758,\n",
       "         0.2398574 , -0.41712025],\n",
       "       [ 0.07892672, -0.12812844,  0.3588904 , ...,  0.20220874,\n",
       "        -0.03211524, -0.2635097 ],\n",
       "       ...,\n",
       "       [-0.31367537, -0.36216024,  0.43182194, ...,  0.6409572 ,\n",
       "        -0.09814946, -0.6874743 ],\n",
       "       [ 0.01328409,  0.22750883,  0.10006183, ...,  0.164902  ,\n",
       "         0.16365732, -0.52859443],\n",
       "       [-0.39270324,  0.19427915, -0.05538194, ...,  0.2295712 ,\n",
       "         0.31881106, -0.378664  ]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gs_test  = graph_builder.transform(fold.X_test)\n",
    "data_loader_test = DataLoader(list(zip(Gs_test, fold.y_test)), batch_size=test_val_batch_size,\n",
    "                              shuffle=False, collate_fn=collate)\n",
    "X_test_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm_notebook(total=len(data_loader_test.dataset), smoothing=0.) as pbar:\n",
    "        for G, label in data_loader_test:\n",
    "            X_test_t = model.transform( G ).cpu().numpy()\n",
    "            X_test_all.append( X_test_t )\n",
    "            pbar.update( len(label) )\n",
    "X_test_all = np.concatenate( X_test_all )\n",
    "X_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.03125, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvm = LinearSVC( **gs.best_params_ )\n",
    "lsvm.fit( X_train_val_all2, y_train_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lsvm.predict( X_test_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6256513026052104"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred == fold.y_test)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49218436873747495"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred == fold.y_test)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Counter(list(map(int,list(sampled_Y)))).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Counter(list(map(int,list(label)))).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "print(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
