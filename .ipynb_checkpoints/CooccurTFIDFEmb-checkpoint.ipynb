{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.tokenizer import Tokenizer\n",
    "from TGA.utils import Dataset\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mangaravite/Documentos/Universidade/Tese/TGA/TGA/tokenizer.py:25: RuntimeWarning: divide by zero encountered in log2\n",
      "  t = np.log2(fc/(c * f))\n",
      "/home/mangaravite/Documentos/Universidade/Tese/TGA/TGA/tokenizer.py:42: RuntimeWarning: divide by zero encountered in log2\n",
      "  t = np.log2(fnc/((1-c)*f))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(k=256, stopwordsSet=[], vocab_max_size=200000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(stopwordsSet=None, vocab_max_size=200000, k=256, ngram_range=(1,1))\n",
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/acm/')\n",
    "fold = next( dataset.get_fold_instances(10, with_val=True) )\n",
    "tokenizer.fit( fold.X_train, fold.y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidf,tfs,dfs = tokenizer.transform(fold.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(X):\n",
    "    tidf, tfs, dfs = tokenizer.transform(X)\n",
    "    docs_tidf = pad_sequence(list(map(torch.LongTensor, tidf)), batch_first=True, padding_value=0)\n",
    "    \n",
    "    docs_tfs  = pad_sequence(list(map(torch.LongTensor, tfs)), batch_first=True, padding_value=0)\n",
    "    docs_tfs = torch.LongTensor(torch.log2(docs_tfs+1).round().long())\n",
    "    \n",
    "    docs_dfs  = pad_sequence(list(map(torch.LongTensor, dfs)), batch_first=True, padding_value=0)\n",
    "    docs_dfs = torch.LongTensor(torch.log2(docs_dfs+1).round().long())\n",
    "    return docs_tidf, docs_tfs, docs_dfs\n",
    "def collate_train(param):\n",
    "    X, y = zip(*param)\n",
    "    return collate(X), torch.LongTensor(y)\n",
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.clamp(a_n, min=eps)\n",
    "    b_norm = b / torch.clamp(b_n, min=eps)\n",
    "    sim_mt = torch.bmm(a_norm, b_norm.transpose(1, 2))\n",
    "    return torch.cos(sim_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.masked_batch_norm import MaskedBatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, maxF=20, drop=.3 ,\n",
    "                 negative_slope=99., mask_layer=True):\n",
    "        super(TFIDFClassifier, self).__init__()\n",
    "        \n",
    "        self.hiddens        = hiddens\n",
    "        self.maxF           = maxF\n",
    "        self.negative_slope = negative_slope\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.drop_          = drop\n",
    "        self.nclass         = nclass\n",
    "        \n",
    "        self.TF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.DF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        \n",
    "        self.term_emb       = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.key_lin        = nn.Sequential(nn.Linear(hiddens, hiddens),\n",
    "                                            nn.Tanh())\n",
    "        self.query_lin      = nn.Sequential(nn.Linear(hiddens, hiddens),\n",
    "                                            nn.Tanh())\n",
    "        self.value_lin      = nn.Sequential(nn.Linear(hiddens, hiddens),\n",
    "                                            nn.Tanh())\n",
    "        \n",
    "        self.fc             = nn.Sequential(nn.Linear(hiddens, hiddens),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(hiddens, nclass))\n",
    "        \n",
    "        \n",
    "        self.latend_labels  = nn.Parameter( torch.rand(nclass, hiddens) )\n",
    "        if mask_layer:\n",
    "            self.mask_layers = self._build_mask(hiddens, nclass)\n",
    "        else:\n",
    "            self.mask_layers = 1.\n",
    "        \n",
    "        self.batchnorm = MaskedBatchNorm1d(1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.init_weights()\n",
    "    def _build_mask(self, hiddens, nclass):\n",
    "        q = hiddens // nclass\n",
    "        a = []\n",
    "        list(map(a.extend, [ q*[vc+1] for vc in range(nclass) ]))\n",
    "        a.extend((h-len(a))*[0])\n",
    "        a = np.array(a)\n",
    "        a = np.matrix([ a == (vc+1) for vc in range(nclass) ])\n",
    "        a[:,-(hiddens-(nclass*q)):] = True\n",
    "        \n",
    "        return torch.Tensor(a)\n",
    "    def forward(self, doc_tids, TFs, DFs, padding=None):\n",
    "        \n",
    "        if padding is None:\n",
    "            padding = doc_tids == 0\n",
    "            padding = padding.unsqueeze(2)\n",
    "        \n",
    "        TFs      = torch.clamp( TFs, max=self.maxF-1 )\n",
    "        TFs_h    = self.TF_emb( TFs ) # [B, S, H]\n",
    "        b_size, s_size, h_size = TFs_h.shape # Batch_size(B), Set_size(S), Hidden_size(H)\n",
    "        doc_sizes = torch.logical_not(padding).sum(dim=1).view(b_size, 1)\n",
    "        \n",
    "        DFs      = torch.clamp( DFs, max=self.maxF-1 )\n",
    "        DFs_h    = self.DF_emb(DFs) # [B, S, H]\n",
    "        \n",
    "        doc_tids = torch.clamp( doc_tids, max=self.vocab_size-1 )\n",
    "        term_h   = self.term_emb(doc_tids)\n",
    "        term_h   = term_h + TFs_h + DFs_h # [B, S, H]\n",
    "        \n",
    "        key_h      = self.key_lin(term_h)                  # [B, S, H]\n",
    "        key_h   = key_h.reshape( b_size, 1, s_size, h_size )         # [B, 1, S, H]\n",
    "        key_h   = F.dropout(key_h, self.drop_)\n",
    "        \n",
    "        query_h    = self.query_lin(term_h)                # [B, S, H]\n",
    "        query_h = query_h.reshape( b_size, s_size, 1, h_size )       # [B, S, 1, H]\n",
    "        query_h = F.dropout(query_h, self.drop_)\n",
    "        \n",
    "        coocc_h = (key_h + query_h) / 2\n",
    "        \n",
    "        value_h    = self.value_lin(term_h)                # [B, S, H]\n",
    "        value_h    = F.dropout(value_h, self.drop_)\n",
    "        \n",
    "        \n",
    "        padding2d = torch.logical_or(padding, padding.transpose(2,1))\n",
    "        padding = padding2d.reshape(b_size, 1, s_size*s_size)\n",
    "        padding = torch.logical_not(padding)\n",
    "        \n",
    "        # Depois posso tentar K-representações das Classes (L)\n",
    "        L = self.latend_labels#.weight         # [L, H]\n",
    "        L = L * self.mask_layers\n",
    "        l_size, _ = L.shape\n",
    "        L = L.unsqueeze(dim=0) # [1, L, H]\n",
    "        L = L.repeat(b_size, 1, 1) #            [B, L, H]\n",
    "\n",
    "        coocc_h = coocc_h.reshape( b_size, s_size*s_size, h_size  ) # [B, S*S, H]\n",
    "        term_dom = sim_matrix( coocc_h, L ) # [B, S*S, L]\n",
    "        coocc_h = coocc_h.reshape( b_size, s_size, s_size, h_size  ) # [B, S, S, H]\n",
    "        \n",
    "        p = term_dom.softmax(dim=2)                       # Convert to Distribution     [B, S*S, L]\n",
    "        logp = torch.log(p)                               # Compute log\n",
    "        cweight = 1./torch.sum(-p*logp, dim=2)                # Entropy of the Distribution [B, S*S]\n",
    "        cweight = cweight.reshape(b_size, s_size, s_size)   # [ B, S, S ]\n",
    "        cweight[padding2d] = -self.negative_slope          # zero padding # [ B, S, S ]\n",
    "        cweight = cweight.reshape(b_size, 1, s_size*s_size) # [ B, 1, S*S ]\n",
    "        \n",
    "        cweight = self.batchnorm(cweight, input_mask=padding) # [ B, 1, S*S ]\n",
    "        cweight = cweight.reshape(b_size, s_size, s_size) # [ B, S, S ]\n",
    "        cweight = torch.exp(cweight)\n",
    "        cweight = cweight / torch.clamp(cweight.sum(dim=1).unsqueeze(dim=1), min=0.00001)\n",
    "        cweight[padding2d] = 0 # zero padding # [ B, S, S ]\n",
    "        weight = cweight.sum(dim=2) # [ B, S ]\n",
    "        weight = weight.unsqueeze(dim=2) # [ B, S, 1 ]\n",
    "        \n",
    "        value_h = (value_h * weight).sum(dim=1)# [ B, S, H ] Representação media dos documentos\n",
    "        value_h = F.dropout(value_h, self.drop_)\n",
    "        docs_pred = self.fc(value_h)\n",
    "        \n",
    "        return docs_pred, cweight # [ B, S ]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \n",
    "        return\n",
    "        nn.init.xavier_normal_(self.DF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.TF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.key_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.query_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "batch_size=16\n",
    "num_workers=4\n",
    "k = 128\n",
    "nepochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'latend_labels' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-228b5eddef1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFIDFClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfidfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtfidfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-15a2794c891c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, hiddens, nclass, maxF, drop, negative_slope, mask_layer)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatend_labels\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatend_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatend_labels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskedBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[1;32m    969\u001b[0m                                 \u001b[0;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                                 .format(torch.typename(value), name))\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'latend_labels' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "tfidfc = TFIDFClassifier(vocab_size=tokenizer.vocab_size, hiddens=300, nclass=tokenizer.n_class).to( device )\n",
    "tfidfc.mask_layers = tfidfc.mask_layers.to(device)\n",
    "\n",
    "tokenizer.k = k\n",
    "optimizer = optim.AdamW( tfidfc.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "loss_func_cel = nn.CrossEntropyLoss().to( device )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.95,\n",
    "                                                       patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 300])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfc._build_mask(300, 11).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae18212801a43b5962b2e7d21229b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c266b65cc624698b7c93c7d8e43e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/19907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.2797/2.2097 ACC: 0.26306                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "dl_val = DataLoader(list(zip(fold.X_val, fold.y_val)), batch_size=batch_size,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=num_workers)\n",
    "total=1\n",
    "correct=1\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    dl_train = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_train, num_workers=num_workers)\n",
    "    loss_train  = 0.\n",
    "    with tqdm(total=len(fold.y_train), smoothing=0., desc=f\"Epoch {e+1}\") as pbar:\n",
    "        for i, (data, y) in enumerate(dl_train):\n",
    "            doc_tids, TFs, DFs = data\n",
    "            doc_tids = doc_tids.to( device )\n",
    "            TFs      = TFs.to( device )\n",
    "            DFs      = DFs.to( device )\n",
    "            y        = y.to( device )\n",
    "            \n",
    "            pred_docs,weights = tfidfc( doc_tids, TFs, DFs )\n",
    "            pred_docs = torch.softmax(pred_docs, dim=1)\n",
    "            loss      = loss_func_cel(pred_docs, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            total      += len(y)\n",
    "            y_pred      = pred_docs.argmax(axis=1)\n",
    "            correct    += (y_pred == y).sum().item()\n",
    "            \n",
    "            toprint  = f\"Train loss: {loss_train/(i+1):.5}/{loss.item():.5} \"\n",
    "            toprint += f'ACC: {correct/total:.5} '\n",
    "            \n",
    "            print(toprint, end=f\"{' '*100}\\r\")\n",
    "            pbar.update( len(y) )\n",
    "            \n",
    "            #break\n",
    "            del doc_tids, TFs\n",
    "            del DFs, y, pred_docs\n",
    "            del loss, y_pred\n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0164, 0.0164, 0.0164,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0164, 0.0164, 0.0164,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0164, 0.0164, 0.0164,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0116, 0.0116, 0.0116,  ..., 0.0116, 0.0116, 0.0116],\n",
       "         [0.0116, 0.0116, 0.0116,  ..., 0.0116, 0.0116, 0.0116],\n",
       "         [0.0116, 0.0116, 0.0116,  ..., 0.0116, 0.0116, 0.0116],\n",
       "         ...,\n",
       "         [0.0116, 0.0116, 0.0116,  ..., 0.0116, 0.0116, 0.0116],\n",
       "         [0.0116, 0.0116, 0.0116,  ..., 0.0116, 0.0116, 0.0116],\n",
       "         [0.0116, 0.0116, 0.0116,  ..., 0.0116, 0.0116, 0.0116]],\n",
       "\n",
       "        [[0.0166, 0.0166, 0.0166,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0166, 0.0166, 0.0166,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0166, 0.0166, 0.0166,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       device='cuda:0', grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 300\n",
    "c = 11\n",
    "q = h // c\n",
    "a = []\n",
    "list(map(a.extend, [ q*[vc+1] for vc in range(c) ]))\n",
    "a.extend((h-len(a))*[0])\n",
    "a = np.array(a)\n",
    "a = np.matrix([ a == (vc+1) for vc in range(c) ])\n",
    "a[:,-(h-(c*q)):] = True\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(h-(c*q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
