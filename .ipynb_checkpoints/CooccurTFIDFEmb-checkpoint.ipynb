{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.tokenizer import Tokenizer\n",
    "from TGA.utils import Dataset\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mangaravite/Documentos/Universidade/Tese/TGA/TGA/tokenizer.py:25: RuntimeWarning: divide by zero encountered in log2\n",
      "  t = np.log2(fc/(c * f))\n",
      "/home/mangaravite/Documentos/Universidade/Tese/TGA/TGA/tokenizer.py:42: RuntimeWarning: divide by zero encountered in log2\n",
      "  t = np.log2(fnc/((1-c)*f))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(stopwordsSet=[], vocab_max_size=300000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(stopwordsSet=None, vocab_max_size=300000)\n",
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/20ng/')\n",
    "fold = next( dataset.get_fold_instances(10, with_val=True) )\n",
    "tokenizer.fit( fold.X_train, fold.y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidf,tfs,dfs = tokenizer.transform(fold.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(X):\n",
    "    tidf, tfs, dfs = tokenizer.transform(X)\n",
    "    docs_tidf = pad_sequence(list(map(torch.LongTensor, tidf)), batch_first=True, padding_value=0)\n",
    "    \n",
    "    docs_tfs  = pad_sequence(list(map(torch.LongTensor, tfs)), batch_first=True, padding_value=0)\n",
    "    docs_tfs = torch.LongTensor(torch.log2(docs_tfs+1).round().long())\n",
    "    \n",
    "    docs_dfs  = pad_sequence(list(map(torch.LongTensor, dfs)), batch_first=True, padding_value=0)\n",
    "    docs_dfs = torch.LongTensor(torch.log2(docs_dfs+1).round().long())\n",
    "    return docs_tidf, docs_tfs, docs_dfs\n",
    "def collate_train(param):\n",
    "    X, y = zip(*param)\n",
    "    return collate(X), torch.LongTensor(y)\n",
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.clamp(a_n, min=eps)\n",
    "    b_norm = b / torch.clamp(b_n, min=eps)\n",
    "    sim_mt = torch.bmm(a_norm, b_norm.transpose(1, 2))\n",
    "    return torch.cos(sim_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, maxF=20, drop=.5,\n",
    "                 negative_slope=99.):\n",
    "        super(TFIDFClassifier, self).__init__()\n",
    "        \n",
    "        self.hiddens        = hiddens\n",
    "        self.maxF           = maxF\n",
    "        self.negative_slope = negative_slope\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.drop_          = drop\n",
    "        self.nclass         = nclass\n",
    "        \n",
    "        self.TF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.DF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        \n",
    "        self.term_emb       = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.key_lin        = nn.Sequential(nn.Linear(hiddens, hiddens),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(hiddens, hiddens),\n",
    "                                            nn.Tanh())\n",
    "        self.query_lin      = nn.Sequential(nn.Linear(hiddens, hiddens),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(hiddens, hiddens),\n",
    "                                            nn.Tanh())\n",
    "        \n",
    "        self.latend_labels  = nn.Parameter( torch.rand(nclass, hiddens) )\n",
    "        \n",
    "        self.init_weights()\n",
    "    def forward(self, doc_tids, TFs, DFs, padding):\n",
    "        \n",
    "        TFs      = torch.clamp( TFs, max=self.maxF-1 )\n",
    "        TFs_h    = self.TF_emb( TFs )\n",
    "        \n",
    "        DFs      = torch.clamp( DFs, max=self.maxF-1 )\n",
    "        DFs_h    = self.DF_emb(DFs)\n",
    "        \n",
    "        doc_tids = torch.clamp( doc_tids, max=self.vocab_size-1 )\n",
    "        term_h   = self.term_emb(doc_tids) + TFs_h + DFs_h\n",
    "        \n",
    "        key_h      = self.key_lin(term_h)       # [B, S, H]\n",
    "        key_h      = F.tanh(key_h)\n",
    "        \n",
    "        query_h    = self.query_lin(term_h)     # [B, S, H]\n",
    "        term_h     = F.tanh(term_h)\n",
    "        \n",
    "        b_size, s_size, h_size = key_h.shape # Batch_size(B), Set_size(S), Hidden_size(H)\n",
    "        \n",
    "        key_h_   = key_h.reshape( b_size, 1, s_size, h_size )         # [B, 1, S, H]\n",
    "        query_h_ = query_h.reshape( b_size, s_size, 1, h_size )       # [B, S, 1, H]\n",
    "        value_h_ = (key_h + query_h)/2.                               # [B, S, S, H]\n",
    "        value_h_ = value_h_.reshape( b_size, s_size*s_size, h_size  ) # [B, S*S, H]\n",
    "        \n",
    "        # Depois posso tentar K-representações das Classes (L)\n",
    "        L = self.latend_labels.weight         # [L, H]\n",
    "        L = L.reshape(1, self.nclass, h_size) # [1, L, H]\n",
    "        L = L.repeat(b_size, 1, 1) #            [B, L, H]\n",
    "        \n",
    "        term_dom = sim_matrix( value_h_, L ) # [B, S*S, L]\n",
    "        \n",
    "        p = term_dom.softmax(dim=2)          # Convert to Distribution     [B, S*S, L]\n",
    "        logp = torch.log(p)                  # Compute log\n",
    "        entropy = torch.sum(-p*logp, dim=2)  # Entropy of the Distribution [B, S*S]\n",
    "        entropy = entropy.reshape(b_size, s_size, s_size) # [ B, S, S ]\n",
    "        entropy = entropy / torch.log(self.nclass) # Normalizing by the uniform distribution\n",
    "        # [ B, S, S ]\n",
    "        \n",
    "        KQ_co_sim * entropy\n",
    "        \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def init_weights(self):\n",
    "        return\n",
    "        nn.init.xavier_normal_(self.DF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.TF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.key_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.query_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K =  torch.rand(2, 3, 4) \n",
    "Q =  torch.rand(2, 3, 4) \n",
    "L =  torch.rand(6, 4) \n",
    "l_size, _ = L.shape\n",
    "b_size, s_size, h_size = K.shape\n",
    "K_ = K.reshape( b_size, 1, s_size, h_size )\n",
    "Q_ = Q.reshape( b_size, s_size, 1, h_size )\n",
    "V_ = (K_ + Q_)/2.\n",
    "V_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6701, 0.1587, 0.4296, 0.7477],\n",
       "         [0.9616, 0.2689, 0.3783, 0.4618],\n",
       "         [0.8822, 0.4239, 0.5156, 0.7456],\n",
       "         [0.4565, 0.3206, 0.5908, 0.8869],\n",
       "         [0.7480, 0.4308, 0.5395, 0.6011],\n",
       "         [0.6686, 0.5858, 0.6768, 0.8848],\n",
       "         [0.5173, 0.4592, 0.5845, 0.7748],\n",
       "         [0.8088, 0.5694, 0.5332, 0.4890],\n",
       "         [0.7294, 0.7244, 0.6705, 0.7727]],\n",
       "\n",
       "        [[0.1495, 0.7801, 0.0956, 0.5595],\n",
       "         [0.5363, 0.6264, 0.1832, 0.6740],\n",
       "         [0.3969, 0.4996, 0.4097, 0.3782],\n",
       "         [0.1837, 0.5170, 0.0510, 0.7085],\n",
       "         [0.5705, 0.3633, 0.1387, 0.8230],\n",
       "         [0.4311, 0.2365, 0.3651, 0.5272],\n",
       "         [0.1347, 0.5839, 0.0401, 0.2601],\n",
       "         [0.5215, 0.4302, 0.1278, 0.3746],\n",
       "         [0.3821, 0.3034, 0.3543, 0.0788]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V2_ = V_.reshape( b_size, s_size*s_size, h_size  )\n",
    "V2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.reshape(1, l_size, h_size).repeat(2,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 6])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occur_class_sim = sim_matrix(V2_, L.reshape(1, l_size, h_size).repeat(b_size,1,1))\n",
    "p = co_occur_class_sim.softmax(dim=2)\n",
    "logp = torch.log(p)\n",
    "entropy = torch.sum(-p*logp, dim=2)\n",
    "entropy.reshape(b_size, s_size, s_size)\n",
    "co_occur_class_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.7915, 1.7908, 1.7906],\n",
       "         [1.7915, 1.7909, 1.7907],\n",
       "         [1.7913, 1.7905, 1.7902]],\n",
       "\n",
       "        [[1.7912, 1.7901, 1.7899],\n",
       "         [1.7913, 1.7906, 1.7905],\n",
       "         [1.7917, 1.7908, 1.7903]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = co_occur_class_sim.softmax(dim=2)\n",
    "logp = torch.log(p)\n",
    "entropy = torch.sum(-p*logp, dim=2)\n",
    "entropy.reshape(b_size, s_size, s_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entr(p):\n",
    "    p = np.exp(p)\n",
    "    p = p/p.sum()\n",
    "    return np.sum( -p*np.log(p) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 2.220446049250313e-16\n",
      "-0.99 1.1102230246251565e-16\n",
      "-0.98 1.1102230246251565e-16\n",
      "-0.97 2.220446049250313e-16\n",
      "-0.96 1.1102230246251565e-16\n",
      "-0.95 1.1102230246251565e-16\n",
      "-0.94 2.220446049250313e-16\n",
      "-0.93 2.220446049250313e-16\n",
      "-0.92 1.1102230246251565e-16\n",
      "-0.91 1.1102230246251565e-16\n",
      "-0.9 2.220446049250313e-16\n",
      "-0.89 1.1102230246251565e-16\n",
      "-0.88 2.220446049250313e-16\n",
      "-0.87 1.1102230246251565e-16\n",
      "-0.86 1.1102230246251565e-16\n",
      "-0.85 1.1102230246251565e-16\n",
      "-0.84 1.1102230246251565e-16\n",
      "-0.83 2.220446049250313e-16\n",
      "-0.82 2.220446049250313e-16\n",
      "-0.81 1.1102230246251565e-16\n",
      "-0.8 1.1102230246251565e-16\n",
      "-0.79 1.1102230246251565e-16\n",
      "-0.78 1.1102230246251565e-16\n",
      "-0.77 1.1102230246251565e-16\n",
      "-0.76 2.220446049250313e-16\n",
      "-0.75 1.1102230246251565e-16\n",
      "-0.74 1.1102230246251565e-16\n",
      "-0.73 1.1102230246251565e-16\n",
      "-0.72 1.1102230246251565e-16\n",
      "-0.71 1.1102230246251565e-16\n",
      "-0.7 1.1102230246251565e-16\n",
      "-0.69 2.220446049250313e-16\n",
      "-0.68 1.1102230246251565e-16\n",
      "-0.67 1.1102230246251565e-16\n",
      "-0.66 1.1102230246251565e-16\n",
      "-0.65 2.220446049250313e-16\n",
      "-0.64 1.1102230246251565e-16\n",
      "-0.63 1.1102230246251565e-16\n",
      "-0.62 2.220446049250313e-16\n",
      "-0.61 1.1102230246251565e-16\n",
      "-0.6 1.1102230246251565e-16\n",
      "-0.59 2.220446049250313e-16\n",
      "-0.58 2.220446049250313e-16\n",
      "-0.57 1.1102230246251565e-16\n",
      "-0.56 1.1102230246251565e-16\n",
      "-0.55 1.1102230246251565e-16\n",
      "-0.54 1.1102230246251565e-16\n",
      "-0.53 2.220446049250313e-16\n",
      "-0.52 2.220446049250313e-16\n",
      "-0.51 1.1102230246251565e-16\n",
      "-0.5 1.1102230246251565e-16\n",
      "-0.49 1.1102230246251565e-16\n",
      "-0.48 1.1102230246251565e-16\n",
      "-0.47 1.1102230246251565e-16\n",
      "-0.46 1.1102230246251565e-16\n",
      "-0.45 1.1102230246251565e-16\n",
      "-0.44 1.1102230246251565e-16\n",
      "-0.43 1.1102230246251565e-16\n",
      "-0.42 1.1102230246251565e-16\n",
      "-0.41 1.1102230246251565e-16\n",
      "-0.4 2.220446049250313e-16\n",
      "-0.39 1.1102230246251565e-16\n",
      "-0.38 1.1102230246251565e-16\n",
      "-0.37 2.220446049250313e-16\n",
      "-0.36 2.220446049250313e-16\n",
      "-0.35 1.1102230246251565e-16\n",
      "-0.34 1.1102230246251565e-16\n",
      "-0.33 1.1102230246251565e-16\n",
      "-0.32 1.1102230246251565e-16\n",
      "-0.31 2.220446049250313e-16\n",
      "-0.3 1.1102230246251565e-16\n",
      "-0.29 1.1102230246251565e-16\n",
      "-0.28 2.220446049250313e-16\n",
      "-0.27 1.1102230246251565e-16\n",
      "-0.26 1.1102230246251565e-16\n",
      "-0.25 1.1102230246251565e-16\n",
      "-0.24 1.1102230246251565e-16\n",
      "-0.23 1.1102230246251565e-16\n",
      "-0.22 2.220446049250313e-16\n",
      "-0.21 1.1102230246251565e-16\n",
      "-0.2 1.1102230246251565e-16\n",
      "-0.19 1.1102230246251565e-16\n",
      "-0.18 1.1102230246251565e-16\n",
      "-0.17 1.1102230246251565e-16\n",
      "-0.16 1.1102230246251565e-16\n",
      "-0.15 2.220446049250313e-16\n",
      "-0.14 1.1102230246251565e-16\n",
      "-0.13 1.1102230246251565e-16\n",
      "-0.12 2.220446049250313e-16\n",
      "-0.11 1.1102230246251565e-16\n",
      "-0.1 1.1102230246251565e-16\n",
      "-0.09 1.1102230246251565e-16\n",
      "-0.08 1.1102230246251565e-16\n",
      "-0.07 1.1102230246251565e-16\n",
      "-0.06 2.220446049250313e-16\n",
      "-0.05 1.1102230246251565e-16\n",
      "-0.04 1.1102230246251565e-16\n",
      "-0.03 1.1102230246251565e-16\n",
      "-0.02 1.1102230246251565e-16\n",
      "-0.01 1.1102230246251565e-16\n",
      "0.0 1.1102230246251565e-16\n",
      "0.01 2.220446049250313e-16\n",
      "0.02 1.1102230246251565e-16\n",
      "0.03 1.1102230246251565e-16\n",
      "0.04 1.1102230246251565e-16\n",
      "0.05 1.1102230246251565e-16\n",
      "0.06 1.1102230246251565e-16\n",
      "0.07 1.1102230246251565e-16\n",
      "0.08 1.1102230246251565e-16\n",
      "0.09 1.1102230246251565e-16\n",
      "0.1 1.1102230246251565e-16\n",
      "0.11 1.1102230246251565e-16\n",
      "0.12 1.1102230246251565e-16\n",
      "0.13 1.1102230246251565e-16\n",
      "0.14 1.1102230246251565e-16\n",
      "0.15 1.1102230246251565e-16\n",
      "0.16 1.1102230246251565e-16\n",
      "0.17 1.1102230246251565e-16\n",
      "0.18 1.1102230246251565e-16\n",
      "0.19 1.1102230246251565e-16\n",
      "0.2 1.1102230246251565e-16\n",
      "0.21 2.220446049250313e-16\n",
      "0.22 1.1102230246251565e-16\n",
      "0.23 1.1102230246251565e-16\n",
      "0.24 2.220446049250313e-16\n",
      "0.25 1.1102230246251565e-16\n",
      "0.26 2.220446049250313e-16\n",
      "0.27 2.220446049250313e-16\n",
      "0.28 2.220446049250313e-16\n",
      "0.29 2.220446049250313e-16\n",
      "0.3 1.1102230246251565e-16\n",
      "0.31 1.1102230246251565e-16\n",
      "0.32 1.1102230246251565e-16\n",
      "0.33 1.1102230246251565e-16\n",
      "0.34 1.1102230246251565e-16\n",
      "0.35 1.1102230246251565e-16\n",
      "0.36 1.1102230246251565e-16\n",
      "0.37 1.1102230246251565e-16\n",
      "0.38 1.1102230246251565e-16\n",
      "0.39 1.1102230246251565e-16\n",
      "0.4 1.1102230246251565e-16\n",
      "0.41 1.1102230246251565e-16\n",
      "0.42 1.1102230246251565e-16\n",
      "0.43 1.1102230246251565e-16\n",
      "0.44 1.1102230246251565e-16\n",
      "0.45 1.1102230246251565e-16\n",
      "0.46 1.1102230246251565e-16\n",
      "0.47 1.1102230246251565e-16\n",
      "0.48 1.1102230246251565e-16\n",
      "0.49 1.1102230246251565e-16\n",
      "0.5 1.1102230246251565e-16\n",
      "0.51 1.1102230246251565e-16\n",
      "0.52 1.1102230246251565e-16\n",
      "0.53 1.1102230246251565e-16\n",
      "0.54 1.1102230246251565e-16\n",
      "0.55 1.1102230246251565e-16\n",
      "0.56 1.1102230246251565e-16\n",
      "0.57 1.1102230246251565e-16\n",
      "0.58 1.1102230246251565e-16\n",
      "0.59 1.1102230246251565e-16\n",
      "0.6 1.1102230246251565e-16\n",
      "0.61 1.1102230246251565e-16\n",
      "0.62 1.1102230246251565e-16\n",
      "0.63 1.1102230246251565e-16\n",
      "0.64 1.1102230246251565e-16\n",
      "0.65 1.1102230246251565e-16\n",
      "0.66 1.1102230246251565e-16\n",
      "0.67 1.1102230246251565e-16\n",
      "0.68 1.1102230246251565e-16\n",
      "0.69 1.1102230246251565e-16\n",
      "0.7 1.1102230246251565e-16\n",
      "0.71 1.1102230246251565e-16\n",
      "0.72 1.1102230246251565e-16\n",
      "0.73 1.1102230246251565e-16\n",
      "0.74 1.1102230246251565e-16\n",
      "0.75 1.1102230246251565e-16\n",
      "0.76 1.1102230246251565e-16\n",
      "0.77 1.1102230246251565e-16\n",
      "0.78 1.1102230246251565e-16\n",
      "0.79 1.1102230246251565e-16\n",
      "0.8 1.1102230246251565e-16\n",
      "0.81 1.1102230246251565e-16\n",
      "0.82 1.1102230246251565e-16\n",
      "0.83 1.1102230246251565e-16\n",
      "0.84 1.1102230246251565e-16\n",
      "0.85 1.1102230246251565e-16\n",
      "0.86 1.1102230246251565e-16\n",
      "0.87 1.1102230246251565e-16\n",
      "0.88 1.1102230246251565e-16\n",
      "0.89 1.1102230246251565e-16\n",
      "0.9 1.1102230246251565e-16\n",
      "0.91 1.1102230246251565e-16\n",
      "0.92 1.1102230246251565e-16\n",
      "0.93 1.1102230246251565e-16\n",
      "0.94 1.1102230246251565e-16\n",
      "0.95 1.1102230246251565e-16\n",
      "0.96 1.1102230246251565e-16\n",
      "0.97 1.1102230246251565e-16\n",
      "0.98 1.1102230246251565e-16\n",
      "0.99 1.1102230246251565e-16\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_U = np.array([  1, 1, 1, 1, 1])\n",
    "entropy_U = np.log(5)\n",
    "for i in (np.array(range(201))-100)/100:\n",
    "    p_L = np.array([  i, -1, -1, -1, -1 ])\n",
    "    print( i, (1.-entr(p_L)/entr(p_U)), (1.-entr(p_L)/entropy_U) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9, 0, 0, 0, 0],\n",
       "         [9, 3, 4, 0, 0],\n",
       "         [6, 7, 8, 3, 3]]),\n",
       " tensor([[1, 0, 0, 0, 0],\n",
       "         [3, 4, 9, 0, 0],\n",
       "         [9, 3, 6, 8, 2]]),\n",
       " tensor([[6, 0, 0, 0, 0],\n",
       "         [6, 8, 5, 0, 0],\n",
       "         [1, 5, 7, 6, 6]]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_tfidf = RecursiformTFIDF(vocab_size=10, hiddens=10, nclass=2)\n",
    "docs_ids = torch.LongTensor(np.random.randint(1, 10, size=(3,5)))\n",
    "TFs = torch.LongTensor(np.random.randint(1, 10, size=(3,5)))\n",
    "DFs = torch.LongTensor(np.random.randint(1, 10, size=(3,5)))\n",
    "\n",
    "pad      = torch.Tensor([[True, False, False, False, False],\n",
    "            [True, True, True, False, False],\n",
    "            [True, True, True, True, True]])\n",
    "docs_ids[pad.logical_not()] = 0\n",
    "TFs[pad.logical_not()] = 0\n",
    "DFs[pad.logical_not()] = 0\n",
    "docs_ids, TFs, DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2852],\n",
       "        [0.0000, 1.2344],\n",
       "        [0.0000, 1.5600]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_tfidf(docs_ids, TFs, DFs, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 10])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_tfidf.latend_labels.repeat(4, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiformTFIDF(\n",
       "  (TF_emb): Embedding(20, 10, padding_idx=0, scale_grad_by_freq=True)\n",
       "  (DF_emb): Embedding(20, 10, padding_idx=0, scale_grad_by_freq=True)\n",
       "  (key_emb): Embedding(10, 10, padding_idx=0, scale_grad_by_freq=True)\n",
       "  (query_emb): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (trns_terms): RecursiformLayer(\n",
       "    (act): Sequential(\n",
       "      (0): Tanh()\n",
       "      (1): LeakyReLU(negative_slope=99.0)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_tfidf.trns_terms.sim_matrix(  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[8.0040e-02, 2.7830e-01, 4.6090e-01, 5.2138e-01, 8.4391e-01, 6.3859e-01,\n",
       "         6.7657e-01, 4.3884e-01, 5.5556e-01, 3.8151e-01, 3.0248e-01, 6.4172e-01,\n",
       "         2.3923e-01, 3.9826e-01, 3.9894e-01, 8.5290e-01, 8.5151e-01, 6.3447e-01,\n",
       "         2.9338e-02, 4.8218e-02, 5.3354e-01, 5.9143e-01, 7.0413e-01, 1.3721e-01,\n",
       "         6.9633e-01, 9.9602e-01, 2.8773e-02, 8.2851e-01, 4.7191e-01, 4.2895e-01],\n",
       "        [5.2241e-01, 6.1488e-01, 4.4912e-01, 1.5563e-01, 7.6636e-01, 9.9459e-01,\n",
       "         1.4288e-03, 1.3221e-01, 6.1341e-01, 6.5157e-01, 8.5758e-01, 6.5390e-01,\n",
       "         3.7188e-01, 6.0490e-01, 7.9080e-01, 6.7799e-02, 1.9263e-01, 8.7974e-01,\n",
       "         8.2336e-01, 7.3677e-01, 9.3091e-01, 8.5285e-01, 8.6042e-01, 1.1362e-01,\n",
       "         7.4830e-01, 4.3722e-01, 4.1882e-01, 3.7110e-01, 7.2956e-01, 4.1405e-02],\n",
       "        [4.2147e-01, 1.3623e-01, 9.2432e-01, 9.9206e-01, 7.7708e-01, 4.6847e-01,\n",
       "         3.8212e-02, 9.0346e-01, 7.6598e-01, 5.2105e-01, 6.7566e-01, 1.4836e-01,\n",
       "         5.3961e-01, 5.9027e-01, 1.4792e-01, 3.2105e-01, 8.6455e-01, 8.8006e-01,\n",
       "         6.9618e-01, 8.4985e-01, 6.4696e-01, 4.5340e-01, 7.7935e-01, 2.0213e-01,\n",
       "         7.2411e-01, 2.2249e-01, 8.6439e-01, 6.0657e-01, 1.1687e-02, 2.4121e-01],\n",
       "        [4.9585e-01, 8.5006e-01, 5.8200e-01, 6.4519e-01, 6.9982e-01, 8.5539e-01,\n",
       "         2.0124e-01, 9.0820e-01, 9.0443e-01, 3.5411e-01, 6.7509e-01, 5.5684e-01,\n",
       "         6.7029e-01, 9.3944e-01, 9.8103e-01, 6.5766e-01, 5.2879e-01, 4.4973e-01,\n",
       "         8.1194e-01, 8.6849e-01, 6.9330e-01, 1.8697e-01, 1.3280e-01, 3.5172e-01,\n",
       "         4.0288e-01, 2.9670e-01, 4.9865e-01, 5.9588e-01, 7.7541e-01, 5.1903e-01],\n",
       "        [4.6224e-01, 4.8178e-01, 3.7257e-01, 8.9934e-01, 1.3396e-01, 3.8319e-01,\n",
       "         4.5053e-01, 9.8349e-01, 5.7650e-01, 6.5261e-01, 2.2261e-01, 7.4552e-01,\n",
       "         7.0719e-01, 7.2329e-01, 1.9669e-01, 4.7568e-01, 8.5634e-01, 8.5974e-01,\n",
       "         1.7100e-01, 7.6131e-01, 2.2726e-01, 7.8408e-01, 6.4474e-01, 7.4263e-01,\n",
       "         3.8628e-02, 5.7495e-01, 5.4863e-01, 9.1648e-01, 1.1152e-01, 2.1971e-01],\n",
       "        [8.3523e-01, 4.4446e-01, 6.9257e-01, 6.7014e-01, 5.4643e-01, 4.7352e-01,\n",
       "         4.5579e-02, 4.9314e-02, 1.8465e-01, 3.3505e-01, 4.9433e-02, 5.9439e-01,\n",
       "         4.8550e-02, 8.5883e-01, 2.0666e-01, 7.6161e-02, 6.4119e-01, 3.6697e-01,\n",
       "         6.5968e-01, 1.6286e-01, 2.1643e-01, 6.6488e-01, 4.5223e-01, 1.6109e-01,\n",
       "         3.3522e-01, 1.7704e-01, 5.7681e-01, 9.6275e-01, 9.8785e-01, 4.2479e-01],\n",
       "        [9.9553e-01, 9.6442e-01, 5.6303e-02, 7.3786e-01, 3.8060e-01, 6.3388e-01,\n",
       "         3.7660e-01, 4.9550e-01, 5.9832e-01, 7.7273e-01, 9.6364e-01, 1.2440e-01,\n",
       "         6.6443e-01, 3.9313e-01, 9.5871e-01, 3.3148e-01, 4.7583e-01, 2.7674e-02,\n",
       "         5.6884e-01, 4.4148e-01, 5.7004e-01, 9.6495e-01, 7.9929e-01, 6.3766e-01,\n",
       "         7.9657e-01, 5.6702e-01, 2.1840e-01, 2.5030e-01, 6.2548e-01, 1.3159e-01],\n",
       "        [9.7264e-01, 3.0309e-01, 1.5918e-01, 2.6249e-01, 5.2354e-01, 3.9428e-01,\n",
       "         8.9846e-01, 1.2093e-01, 8.2086e-01, 8.2540e-01, 9.2308e-01, 5.4379e-01,\n",
       "         7.6804e-01, 2.3410e-01, 5.1656e-02, 5.7975e-01, 4.5426e-02, 2.4636e-01,\n",
       "         2.1175e-01, 8.2688e-01, 5.7918e-01, 6.2944e-01, 3.7013e-01, 7.2566e-02,\n",
       "         7.6751e-01, 3.7064e-01, 5.1542e-01, 3.6065e-01, 9.4521e-01, 9.0363e-01],\n",
       "        [5.3304e-02, 1.2575e-01, 7.5639e-01, 8.6610e-01, 6.9148e-01, 4.2386e-01,\n",
       "         6.8205e-01, 5.7460e-01, 1.5020e-01, 4.0461e-01, 6.6829e-01, 9.5974e-01,\n",
       "         5.8712e-01, 6.4953e-01, 7.3872e-01, 2.8816e-01, 2.0037e-01, 7.7928e-01,\n",
       "         3.9107e-01, 2.5056e-01, 4.0115e-01, 8.8571e-01, 3.8534e-01, 7.6895e-01,\n",
       "         6.7436e-01, 2.5783e-01, 3.9958e-01, 5.0303e-01, 9.1554e-01, 1.9611e-01],\n",
       "        [8.5572e-01, 5.2945e-01, 7.0031e-01, 7.7863e-01, 4.3671e-01, 2.8591e-01,\n",
       "         8.4277e-01, 8.1772e-01, 1.4895e-01, 2.2752e-01, 2.1677e-02, 1.4451e-01,\n",
       "         1.8779e-01, 4.0733e-01, 2.4773e-01, 5.8649e-02, 7.7494e-02, 1.8239e-01,\n",
       "         8.1676e-01, 1.0658e-01, 8.5242e-01, 9.0607e-01, 1.2216e-01, 6.2855e-01,\n",
       "         1.7555e-01, 9.6068e-01, 7.1327e-02, 5.7962e-01, 7.0246e-01, 2.9163e-01],\n",
       "        [2.1583e-01, 5.1156e-01, 4.7632e-01, 5.8968e-01, 2.8166e-01, 8.7957e-01,\n",
       "         5.4879e-01, 3.7275e-02, 4.1423e-01, 7.1360e-02, 1.1382e-01, 8.8522e-01,\n",
       "         3.1458e-01, 9.7749e-01, 2.6806e-01, 2.2110e-01, 5.2376e-01, 2.3436e-01,\n",
       "         9.9624e-01, 4.9173e-01, 4.1648e-01, 3.7761e-01, 1.0127e-01, 7.3329e-01,\n",
       "         8.4834e-02, 7.3146e-01, 4.9079e-01, 7.2938e-01, 9.3871e-01, 2.3200e-01],\n",
       "        [8.8128e-01, 6.5297e-01, 1.0748e-01, 3.6773e-01, 4.7954e-01, 9.9453e-01,\n",
       "         8.2893e-01, 1.7657e-01, 5.3956e-02, 8.3741e-01, 5.3531e-01, 7.9503e-02,\n",
       "         8.2603e-01, 9.9800e-01, 6.8112e-01, 6.5975e-01, 7.7908e-01, 2.9079e-01,\n",
       "         7.1002e-01, 6.1756e-01, 9.9393e-02, 2.6376e-01, 7.9440e-01, 8.1908e-01,\n",
       "         7.5356e-01, 1.7041e-01, 9.8294e-01, 7.0304e-01, 8.0921e-01, 2.3420e-01],\n",
       "        [6.3656e-01, 7.6350e-01, 8.9269e-01, 2.2569e-01, 3.1924e-01, 5.4111e-01,\n",
       "         6.6866e-01, 2.2335e-01, 4.8012e-01, 3.3232e-01, 6.2442e-01, 9.2982e-01,\n",
       "         8.1888e-01, 4.4980e-01, 6.2631e-02, 1.3773e-01, 7.2157e-01, 3.7240e-01,\n",
       "         3.3942e-01, 4.4098e-01, 6.3994e-02, 8.6567e-01, 9.2861e-02, 8.9061e-01,\n",
       "         3.9454e-01, 6.8186e-01, 2.1968e-01, 5.9115e-01, 7.8995e-01, 1.3583e-01],\n",
       "        [3.4085e-01, 9.6494e-01, 5.7793e-01, 8.9885e-01, 2.5368e-01, 5.2928e-01,\n",
       "         1.4676e-01, 2.8153e-01, 5.3751e-01, 4.4225e-01, 2.0971e-01, 2.9836e-01,\n",
       "         5.7387e-01, 9.2779e-01, 2.3539e-02, 6.5883e-01, 1.3029e-01, 3.7797e-01,\n",
       "         4.0973e-01, 7.1476e-01, 1.6476e-01, 2.8487e-01, 6.7002e-01, 2.4640e-01,\n",
       "         7.7904e-02, 3.5245e-01, 1.2499e-01, 5.7185e-01, 7.8126e-01, 9.0426e-01],\n",
       "        [8.2854e-01, 4.8177e-01, 6.5937e-01, 4.6311e-01, 7.1616e-01, 5.2494e-01,\n",
       "         5.4269e-02, 7.3792e-01, 6.6066e-01, 1.0229e-01, 2.9805e-01, 8.0159e-01,\n",
       "         2.3981e-01, 5.1775e-01, 6.8117e-01, 2.1390e-01, 7.7003e-02, 1.8133e-01,\n",
       "         6.4535e-01, 3.4378e-02, 7.3310e-01, 4.5351e-01, 2.4719e-01, 7.7317e-01,\n",
       "         2.3223e-01, 8.7903e-01, 6.0931e-01, 5.4379e-01, 8.8271e-01, 7.2639e-01],\n",
       "        [9.0968e-01, 7.6317e-01, 4.0649e-01, 2.8244e-01, 9.6217e-01, 8.4291e-01,\n",
       "         2.2745e-01, 6.5357e-01, 7.0912e-01, 6.2063e-01, 5.0576e-01, 7.3889e-01,\n",
       "         1.9606e-01, 8.6606e-02, 6.9660e-01, 4.9337e-01, 3.4483e-01, 7.9528e-01,\n",
       "         1.5859e-01, 2.1227e-01, 2.7473e-01, 6.8772e-01, 9.4636e-01, 2.8718e-01,\n",
       "         4.9879e-01, 3.6109e-01, 1.5625e-01, 8.8502e-01, 1.6815e-01, 4.7352e-01],\n",
       "        [4.4649e-01, 9.4022e-01, 4.5121e-01, 2.6219e-01, 6.1764e-01, 5.8748e-01,\n",
       "         4.3060e-01, 3.6319e-01, 7.6724e-01, 2.6709e-01, 9.5574e-01, 8.3754e-03,\n",
       "         6.5347e-01, 5.8237e-01, 9.9168e-02, 4.0573e-01, 7.6732e-01, 7.3497e-01,\n",
       "         9.6488e-01, 2.8783e-01, 2.3590e-01, 5.3694e-02, 1.7898e-01, 4.9833e-01,\n",
       "         7.8872e-01, 6.5636e-01, 2.7564e-01, 1.1173e-01, 9.3718e-01, 1.0970e-01],\n",
       "        [9.9950e-01, 2.2113e-02, 5.2078e-01, 2.8428e-01, 8.9434e-01, 9.3621e-01,\n",
       "         9.1386e-01, 9.0106e-01, 6.0257e-01, 6.6957e-01, 5.4559e-01, 4.3436e-01,\n",
       "         4.1605e-01, 3.9302e-01, 3.4247e-01, 4.8369e-01, 4.8097e-02, 3.8152e-01,\n",
       "         1.6388e-01, 1.6906e-01, 5.1052e-01, 6.5947e-01, 2.2670e-02, 5.8361e-01,\n",
       "         3.4267e-01, 7.9459e-01, 8.6577e-01, 9.0822e-01, 8.1770e-01, 8.3169e-01],\n",
       "        [4.8840e-01, 6.7747e-01, 5.1552e-01, 3.3332e-01, 7.5024e-01, 6.8393e-01,\n",
       "         7.4465e-01, 3.5196e-01, 9.4170e-02, 4.8153e-01, 1.0415e-01, 4.3376e-01,\n",
       "         5.4774e-01, 8.1461e-01, 5.3753e-01, 3.0105e-01, 7.0162e-02, 8.6711e-01,\n",
       "         1.5008e-01, 7.2913e-01, 3.4921e-01, 1.7856e-01, 5.3238e-01, 8.2198e-03,\n",
       "         2.0490e-01, 1.7480e-01, 3.6527e-01, 9.5349e-01, 1.4852e-01, 6.0913e-01],\n",
       "        [9.0529e-01, 4.8837e-02, 2.0176e-01, 2.1117e-01, 7.2161e-01, 9.2027e-01,\n",
       "         8.6609e-01, 9.9759e-01, 7.9735e-01, 8.3124e-01, 7.6259e-01, 2.9608e-01,\n",
       "         5.3687e-01, 9.2850e-01, 2.1600e-01, 6.3224e-01, 6.5354e-01, 4.3886e-01,\n",
       "         9.2183e-02, 6.1299e-01, 3.0685e-01, 3.0322e-02, 6.2484e-01, 3.7049e-01,\n",
       "         6.7021e-01, 7.2525e-01, 6.3058e-01, 8.7358e-01, 8.8915e-01, 6.0150e-01],\n",
       "        [2.4947e-01, 8.9063e-01, 9.9593e-01, 3.7736e-01, 5.1268e-01, 5.8554e-01,\n",
       "         4.5220e-01, 1.8308e-01, 1.7928e-01, 9.7574e-01, 7.9798e-01, 2.4871e-01,\n",
       "         9.2796e-01, 7.1902e-01, 8.7007e-01, 8.1231e-02, 5.2094e-01, 6.1682e-01,\n",
       "         5.9580e-01, 6.5564e-01, 8.8818e-01, 7.0197e-01, 6.9671e-01, 5.4987e-02,\n",
       "         4.7064e-01, 7.1530e-01, 5.1092e-01, 2.6297e-01, 1.1448e-01, 9.8182e-01],\n",
       "        [4.3923e-03, 5.7072e-01, 3.7506e-01, 8.5088e-01, 2.1105e-01, 7.3200e-01,\n",
       "         3.8661e-01, 6.8577e-01, 3.9872e-01, 8.5997e-01, 9.2249e-01, 5.3887e-01,\n",
       "         8.6235e-01, 3.3918e-01, 7.7905e-01, 5.0935e-01, 8.7381e-01, 6.8487e-01,\n",
       "         7.2623e-01, 8.5156e-01, 5.0590e-01, 4.3005e-01, 3.8627e-01, 2.9017e-01,\n",
       "         4.2922e-01, 8.7002e-01, 3.7177e-01, 3.3411e-01, 4.7038e-01, 9.1782e-01],\n",
       "        [4.6160e-01, 8.3057e-02, 2.2676e-01, 6.4743e-01, 1.7842e-01, 2.5360e-01,\n",
       "         7.7085e-01, 1.7828e-01, 2.6818e-01, 9.3139e-01, 5.6154e-01, 8.4733e-01,\n",
       "         2.1509e-01, 3.1002e-01, 3.8383e-01, 7.1599e-01, 7.5458e-01, 1.4230e-01,\n",
       "         6.4508e-01, 4.7448e-01, 2.0162e-01, 5.7591e-01, 1.4687e-01, 2.6417e-01,\n",
       "         2.1853e-01, 9.0257e-01, 8.3062e-01, 9.3620e-02, 5.8772e-01, 9.4447e-01],\n",
       "        [4.6872e-01, 4.2933e-02, 7.8918e-01, 6.3536e-01, 1.4042e-01, 2.4601e-01,\n",
       "         8.1875e-01, 8.9290e-01, 2.1056e-01, 1.2502e-01, 9.5786e-01, 8.6639e-01,\n",
       "         2.4429e-01, 6.0545e-01, 5.7739e-01, 2.4082e-01, 2.6824e-01, 9.3858e-01,\n",
       "         5.2573e-01, 2.4469e-01, 9.9600e-01, 9.8838e-01, 5.9778e-01, 2.5725e-01,\n",
       "         9.7663e-01, 8.1540e-01, 1.6593e-01, 2.9279e-01, 7.5714e-01, 2.5291e-01],\n",
       "        [9.6000e-01, 4.3503e-01, 3.2416e-01, 3.4374e-01, 5.1972e-01, 3.1891e-01,\n",
       "         1.3590e-01, 8.1894e-01, 3.8414e-01, 1.8232e-01, 2.4057e-01, 6.2089e-01,\n",
       "         4.6347e-01, 7.3876e-01, 4.3702e-01, 4.5881e-01, 7.5100e-01, 9.3538e-01,\n",
       "         2.8605e-01, 1.7017e-01, 9.7920e-01, 7.7804e-01, 9.2336e-01, 3.1799e-03,\n",
       "         5.0979e-02, 1.4509e-01, 3.3722e-01, 3.3704e-01, 5.3952e-01, 2.8570e-01],\n",
       "        [5.4297e-01, 8.7570e-01, 2.0972e-01, 1.9623e-01, 1.7524e-02, 4.4969e-01,\n",
       "         6.0783e-01, 7.9193e-01, 2.9057e-01, 1.1919e-01, 4.9522e-01, 4.3231e-01,\n",
       "         3.2786e-01, 5.6341e-01, 1.5789e-01, 1.1557e-01, 4.1618e-01, 2.8448e-01,\n",
       "         5.3330e-02, 1.2685e-01, 7.6214e-03, 4.3091e-01, 4.6593e-01, 2.9982e-01,\n",
       "         7.4663e-01, 3.3741e-01, 3.1957e-01, 7.2895e-01, 4.9075e-01, 3.9812e-01],\n",
       "        [7.9663e-02, 4.3940e-01, 5.8050e-01, 6.2771e-01, 8.3081e-01, 2.1843e-01,\n",
       "         9.9400e-01, 5.6077e-01, 9.0949e-01, 8.4601e-01, 8.0448e-01, 9.4248e-01,\n",
       "         2.7158e-01, 9.0208e-02, 7.9825e-01, 6.5031e-01, 1.6499e-01, 1.0613e-01,\n",
       "         6.5937e-01, 7.4614e-01, 5.8139e-02, 1.6045e-02, 2.8867e-03, 3.7916e-01,\n",
       "         6.6052e-01, 7.2532e-01, 5.8375e-01, 7.6264e-01, 1.2201e-01, 8.8459e-01],\n",
       "        [6.4950e-01, 6.9850e-01, 7.8357e-01, 8.8495e-04, 1.3479e-01, 9.0640e-01,\n",
       "         4.0908e-01, 3.0459e-01, 3.8155e-01, 8.3968e-01, 2.8458e-01, 6.4347e-01,\n",
       "         8.7055e-01, 3.4716e-01, 3.8393e-01, 2.1043e-01, 6.8557e-01, 5.0500e-01,\n",
       "         8.7480e-01, 4.9472e-01, 8.9628e-01, 6.8616e-01, 2.4741e-01, 3.1116e-01,\n",
       "         2.5900e-01, 6.3003e-01, 7.7997e-01, 9.3923e-01, 4.8390e-01, 1.0018e-01],\n",
       "        [9.2866e-01, 6.3671e-01, 4.1899e-01, 7.1232e-02, 1.1056e-01, 6.9412e-01,\n",
       "         3.3656e-02, 9.2417e-01, 4.3428e-01, 1.3827e-01, 6.3875e-01, 9.2986e-01,\n",
       "         3.5587e-01, 4.7146e-01, 6.2704e-01, 1.7368e-01, 8.1207e-01, 3.7984e-02,\n",
       "         2.7608e-02, 2.6408e-02, 6.4345e-01, 5.8754e-01, 8.5933e-01, 9.3681e-01,\n",
       "         6.6416e-01, 4.8431e-01, 1.6574e-02, 5.1552e-01, 3.7553e-01, 8.8669e-01],\n",
       "        [8.6191e-01, 7.2245e-01, 3.3217e-01, 8.4463e-01, 8.2111e-02, 9.6033e-01,\n",
       "         6.8087e-01, 9.9383e-01, 8.4575e-01, 8.8118e-01, 5.9772e-01, 7.6118e-01,\n",
       "         1.8973e-01, 5.0622e-02, 9.1626e-01, 7.8757e-01, 4.2927e-01, 1.0246e-01,\n",
       "         1.5149e-01, 6.6994e-01, 4.0085e-01, 2.7688e-01, 1.3003e-01, 7.2210e-01,\n",
       "         5.2399e-01, 5.5119e-01, 4.8252e-01, 3.1443e-01, 6.5944e-02, 9.9590e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "batch_size=16\n",
    "num_workers=4\n",
    "nepochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 3.95 GiB total capacity; 2.23 GiB already allocated; 194.69 MiB free; 2.29 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-354aa629354a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m ttfidf = TransformTFIDF(vocab_size=tokenizer.vocab_size,\n\u001b[1;32m      2\u001b[0m                         \u001b[0mnclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                        nhead=4, hiddens=128 ).to( device )\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mttfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 3.95 GiB total capacity; 2.23 GiB already allocated; 194.69 MiB free; 2.29 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "ttfidf = TransformTFIDF(vocab_size=tokenizer.vocab_size,\n",
    "                        nclass=tokenizer.n_class,\n",
    "                       nhead=4, hiddens=128 ).to( device )\n",
    "\n",
    "optimizer = optim.AdamW( ttfidf.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "loss_func_cel = nn.CrossEntropyLoss().to( device )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.95,\n",
    "                                                       patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = DataLoader(list(zip(fold.X_val, fold.y_val)), batch_size=batch_size,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=num_workers)\n",
    "total=1\n",
    "correct=1\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    dl_train = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_train, num_workers=num_workers)\n",
    "    loss_train  = 0.\n",
    "    with tqdm(total=len(fold.y_train), smoothing=0., desc=f\"Epoch {e+1}\") as pbar:\n",
    "        for i, (data, y) in enumerate(dl_train):\n",
    "            doc_tids, TFs, DFs = data\n",
    "            doc_tids = doc_tids.to( device )\n",
    "            TFs      = TFs.to( device )\n",
    "            DFs      = DFs.to( device )\n",
    "            y        = y.to( device )\n",
    "            \n",
    "            pred_docs = ttfidf( doc_tids, TFs, DFs )\n",
    "            pred_docs = torch.softmax(pred_docs, dim=1)\n",
    "            loss      = loss_func_cel(pred_docs, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            total      += len(y)\n",
    "            y_pred      = pred_docs.argmax(axis=1)\n",
    "            correct    += (y_pred == y).sum().item()\n",
    "            \n",
    "            toprint  = f\"Train loss: {loss_train/(i+1):.5}/{loss.item():.5} \"\n",
    "            toprint += f'ACC: {correct/total:.5} '\n",
    "            \n",
    "            print(toprint, end=f\"{' '*100}\\r\")\n",
    "            pbar.update( len(y) )\n",
    "            \n",
    "            del doc_tids, TFs\n",
    "            del DFs, y, pred_docs\n",
    "            del loss, y_pred\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
