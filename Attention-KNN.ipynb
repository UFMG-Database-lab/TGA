{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from TGA.utils import Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from TGA.utils import preprocessor\n",
    "import copy\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "from collections import Counter\n",
    "from segtok import tokenizer as tk\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val'), 6553)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/datasets/webkb/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=True))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mindf=2, stopwords='remove', model='list', lan='english', verbose=False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "        self.mindf = mindf\n",
    "        self.le = LabelEncoder()\n",
    "        self.verbose = verbose\n",
    "        self.stopwords = stopwords\n",
    "        self.stopwordsSet = stop_words\n",
    "        self.lan = lan\n",
    "        self.model = model\n",
    "        self.analyzer = TfidfVectorizer(preprocessor=preprocessor).build_analyzer()\n",
    "        #self.analyzer = tk.web_tokenizer\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.N = len(X)\n",
    "        self.le.fit( y )\n",
    "        self.n_class = len(self.le.classes_)\n",
    "\n",
    "        self.term_freqs = Counter()\n",
    "        docs = map(self.analyzer, X)\n",
    "        for doc_in_terms in tqdm(docs, total=self.N, disable=not self.verbose):\n",
    "            doc_in_terms = list(map( self._filter_fit_, doc_in_terms ))\n",
    "            self.term_freqs.update(list(set(doc_in_terms)))\n",
    "        self.node_mapper      = {'<BLANK>': 0}\n",
    "        self.term_freqs       = { term:v for (term,v) in self.term_freqs.items() if v >= self.mindf }    \n",
    "        self.node_mapper      = { term:self._get_idx_(term) for term in self.term_freqs.keys() if self._isrel_(term) }\n",
    "        self.node_mapper['<UNK>'] = len(self.node_mapper)\n",
    "        self.node_mapper['<BLANK>'] = 0\n",
    "        self.vocab_size = len(self.node_mapper)\n",
    "        \n",
    "        return self\n",
    "    def _isrel_(self, term):\n",
    "        if self.stopwords == 'remove' and term in self.stopwordsSet:\n",
    "            return False\n",
    "        # put here your filter_functions\n",
    "        return True\n",
    "    def _get_idx_(self, term):\n",
    "        # put here your idx_set_functions\n",
    "        if self.stopwords == 'mark' and term in self.stopwordsSet:\n",
    "            print('is stop', term)\n",
    "            return self.node_mapper.setdefault('<STPW>', len(self.node_mapper))\n",
    "        return self.node_mapper.setdefault(term, len(self.node_mapper))\n",
    "    def _filter_transform_(self, term):\n",
    "        if self.stopwords == 'mark' and term in self.stopwordsSet:\n",
    "            return '<STPW>'\n",
    "        if term not in self.node_mapper:\n",
    "            return '<UNK>'\n",
    "        return term\n",
    "    def _filter_fit_(self, term):\n",
    "        if self.stopwords == 'mark' and term in self.stopwordsSet:\n",
    "            return '<STPW>'\n",
    "        return term\n",
    "    def _model_(self, doc):\n",
    "        if self.model == 'set':\n",
    "            return set(doc)\n",
    "        return list(doc)\n",
    "    def transform(self, X, verbose=None):\n",
    "        verbose = verbose if verbose is not None else self.verbose\n",
    "        n = len(X)\n",
    "        doc_off = [0]\n",
    "        terms_idx = []\n",
    "        for i,doc_in_terms in tqdm(enumerate(map(self.analyzer, X)), total=n, disable=not verbose):\n",
    "            doc_in_terms = filter( self._isrel_, doc_in_terms )\n",
    "            doc_in_terms = map( self._filter_transform_, doc_in_terms )\n",
    "            doc_in_terms = self._model_(doc_in_terms)\n",
    "            doc_in_terms = [ self.node_mapper[tid] for tid in doc_in_terms ]\n",
    "            if self.model == 'sorted':\n",
    "                doc_in_terms = sorted(doc_in_terms)\n",
    "            doc_off.append( len(doc_in_terms) )\n",
    "            terms_idx.extend( doc_in_terms )\n",
    "        return np.array( terms_idx ), np.array(doc_off)[:-1].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e8c73959244cbd8aa65ce8f02128e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(mindf=1, model='set', stopwords='keep', verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(mindf=1, stopwords='keep', model='set', verbose=True)\n",
    "tokenizer.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tokenizer.le.transform( fold.y_train )\n",
    "y_val   = tokenizer.le.transform( fold.y_val )\n",
    "y_test  = tokenizer.le.transform( fold.y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3db8284d7c4f7f8a789d7a2c66a599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1427,    19,    38,  3165,  3221,  5618,    93,    95,  5017,\n",
       "        17099,  6651, 21694,   809,  6580,  2402,    28, 10396,   426,\n",
       "           45,    74,  5623,  4632,    75,   747,   235,   271, 11071,\n",
       "          115,  6329,    14,  1719,   122, 19692,   125,  8617, 14274,\n",
       "         1225,   129,   446,   307,   329,   211,    17,   476,  1095,\n",
       "        11770,  7110,  2232,    93,     3,  1125,  1019, 23061,  5452,\n",
       "        21403,  1589,  1021, 13806,  2142,  6867,  2500,  4847,   484,\n",
       "         3436, 20145,   893,  2600,   144, 11996,  4725, 14190,   201,\n",
       "         2012,  3980, 15824, 10603,  1112,   467,   203,  6134,  5481,\n",
       "         5045,  8366, 10605,  3238,  7252,    15,  2246,   509,   511,\n",
       "         7254,  2398,  1425,  7474,   282,    19,  1150,  2250,  6700,\n",
       "         1751,  1972,  4409,  7137,  2255,  3785,  8187,    25,  1038,\n",
       "         5418, 10127,  1100,  1620,    28,   103, 10610,   537,  6117,\n",
       "         4615, 10182,  3659,   429,   921,  2100,  4807, 11614,  8709,\n",
       "         5534,  2261,  1804,  4018,  1048,   548,  6550,  1628, 21084,\n",
       "         6230,   554,  2614,   559,  5741,  1053,  2034,  6635,  1963,\n",
       "          563,  6233,   307,  2859,  5230,  4681,  1470,  1722,  3050,\n",
       "        10549,  7851,   577,   937,  1057,   227,  5695,  1059,  2280,\n",
       "          582,  1179,    41,  5540, 18885,  4286,  4152,  7688,  1949,\n",
       "         1652,  1487,  2184,    45,  5816,  7585,  2619,  3128,  7027,\n",
       "         3272,  2189,  1066,  9110,  4229,  3460, 10195,  1187,  4291,\n",
       "         6202,   597,  1068,  6408,  1070, 10616,  3313, 10618, 10617,\n",
       "         7321, 12219,  7143,  1195,   609,  2892, 17999,  5404,   612,\n",
       "        21471,  3134,  1509,  4376,  4969,  7855, 16540,   134,   328,\n",
       "        10622,  3138,   842,  1887,  1834,    60,  1673,  1205,   623,\n",
       "         1869,  7126,   696,   416,  7127,  1775, 15605,  1085,  7857,\n",
       "         8139,  4702,  1841,  6736,   649,  1710, 17814,  1687,    70,\n",
       "          643, 13953,   969,  2688,   646,  1781,  7859,   648, 14189,\n",
       "        21577,    74,  4863,  1846, 10113,   857,  1533,   341,  2725,\n",
       "          861,  3261, 23045,   660,  6164,   464,  7239,  5413,  1006,\n",
       "         3225,  3415,  3226,  1697,  8533,  3004,   252,  5915, 10204,\n",
       "         2220,  7627, 10626, 21085,   675,  6091,  4712,  1115,   256,\n",
       "         3413,  2694,    89,  1016]),\n",
       " array([ 0, 43]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.transform(fold.X_val[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'frank tropschuh 1 512 459 cs utexas usa 10 3 10 3 1071 clayton ln apt 333 15 austin tx 4310 curriculum vitae english deutsch links carnegie mellon university 1991 1995 undergraduate student school computer science universita erlangen nu 1993 1994 junior year abroad institut fu und department computer science iv operating systems university texas austin 1995 1996 graduate student department computer science frank tropschuh cs utexas \\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.node_mapper['<BLANK>'], fold.X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train(param):\n",
    "    X, y = zip(*param)\n",
    "    terms_ids, docs_offsets = tokenizer.transform(X, verbose=False)\n",
    "    return torch.LongTensor(terms_ids), torch.LongTensor(docs_offsets), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(nn.Module):\n",
    "    def __init__(self, negative_slope=1000, kappa=2.):\n",
    "        super(Mask, self).__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "        self.kappa = kappa\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, h):\n",
    "        w = F.leaky_relu( h, negative_slope=self.negative_slope)\n",
    "        w = self.sig(w-self.kappa)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function embedding_bag:\n",
      "\n",
      "embedding_bag(...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.embedding_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttentionBag(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, drop=.5, initrange=.5, negative_slope=99.):\n",
    "        super(SimpleAttentionBag, self).__init__()\n",
    "        self.hiddens    = hiddens\n",
    "        self.dt_emb     = nn.Embedding(vocab_size, hiddens)\n",
    "        self.tt_emb     = nn.Embedding(vocab_size, hiddens)\n",
    "        self.tt_dir_map = nn.Linear(hiddens, hiddens)\n",
    "        self.fc         = nn.Linear(hiddens, nclass)\n",
    "        self.initrange  = initrange \n",
    "        self.negative_slope = negative_slope\n",
    "        self.drop       = nn.Dropout(drop)\n",
    "        self.drop_      = drop\n",
    "        self.sig        = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "    def forward(self, terms_idx, docs_offsets, return_mask=False):\n",
    "        n = terms_idx.shape[0]\n",
    "        batch_size = docs_offsets.shape[0]\n",
    "        \n",
    "        k         = [ terms_idx[ docs_offsets[i-1]:docs_offsets[i] ] for i in range(1, batch_size) ]\n",
    "        k.append( terms_idx[ docs_offsets[-1]: ] )\n",
    "        x_packed  = pad_sequence(k, batch_first=True, padding_value=0)\n",
    "\n",
    "        bx_packed = x_packed == 0\n",
    "        doc_sizes = bx_packed.logical_not().sum(dim=1).view(batch_size, 1)\n",
    "        pad_mask  = bx_packed.logical_not()\n",
    "        pad_mask  = pad_mask.view(*bx_packed.shape, 1)\n",
    "        pad_mask  = pad_mask.logical_and(pad_mask.transpose(1, 2))\n",
    "        \n",
    "        dt_h     = self.dt_emb( x_packed )\n",
    "        dt_h     = F.dropout( dt_h, p=self.drop_, training=self.training )\n",
    "        #dt_h     = self.drop(dt_h)\n",
    "        \n",
    "        tt_h     = self.tt_emb( x_packed )\n",
    "        tt_h     = F.dropout( tt_h, p=self.drop_, training=self.training )\n",
    "        #tt_h     = self.drop(tt_h)\n",
    "        dir_tt_h = self.tt_dir_map( tt_h )\n",
    "\n",
    "        weights = torch.bmm( tt_h, dir_tt_h.transpose( 1, 2 ) )\n",
    "        weights = F.leaky_relu( weights, negative_slope=self.negative_slope)\n",
    "        \n",
    "        weights[pad_mask.logical_not()] = float('-inf') # Set the 3D-pad mask values to -inf (=0 in softmax)\n",
    "        weights = F.sigmoid(weights)\n",
    "        #weights = F.softmax(weights, dim=2) # Normalize the neighbors weights\n",
    "        weights = torch.where(torch.isnan(weights), torch.zeros_like(weights), weights) #replace nan to zero\n",
    "        weights = weights.sum(axis=2) / doc_sizes\n",
    "        weights[bx_packed] = float('-inf') # Set the 2D-pad mask values to -inf  (=0 in softmax)\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        #weights = F.sigmoid(weights)\n",
    "        weights = weights.view( *weights.shape, 1 )\n",
    "        \n",
    "        docs_h = dt_h * weights\n",
    "        docs_h = docs_h.sum(axis=1)\n",
    "        docs_h = F.dropout( docs_h, p=self.drop_, training=self.training )\n",
    "        #docs_h = self.drop(docs_h)\n",
    "        return self.fc(docs_h), weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.dt_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.tt_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.tt_dir_map.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.fc.weight.data.uniform_(-self.initrange, self.initrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 1000\n",
    "max_epochs = 50\n",
    "drop=0.5\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b6c03d3b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SimpleClassifier(tokenizer.vocab_size, 300, tokenizer.n_class, dropout=drop).to( device )\n",
    "ab = SimpleAttentionBag(tokenizer.vocab_size, 300, tokenizer.n_class, drop=drop).to( device )\n",
    "\n",
    "optimizer = optim.AdamW( ab.parameters(), lr=6e-3, weight_decay=5e-3)\n",
    "loss_func_cel = nn.CrossEntropyLoss().to( device )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.95,\n",
    "                                                       patience=10, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=.98, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb163fc51844c6496df19812b5ba989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935a450904fb4054a7b041ecd2c79281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.5128/2.0151 ACC: 0.67786                                                                                                    \n",
      "Val loss: 1.4275 ACC: 0.74484                                                                                                    \n",
      "New Best Val loss: 1.4275                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e525387cf7d94781ae3aadac37c9a01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3513/1.1783 ACC: 0.82191                                                                                                    \n",
      "Val loss: 1.4001 ACC: 0.77643                                                                                                    \n",
      "New Best Val loss: 1.4001                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb438ea1bd7c424bafdd93d6f01d2bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2875/1.1655 ACC: 0.88799                                                                                                    \n",
      "Val loss: 1.3903 ACC: 0.78493                                                                                                    \n",
      "New Best Val loss: 1.3903                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f3bf6a2f57458d80b25eb15cbb2c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2602/1.2088 ACC: 0.90783                                                                                                    \n",
      "Val loss: 1.398 ACC: 0.77157                                                                                                     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfddc6a00c3148b291f82f0fd293d79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2425/1.1707 ACC: 0.92599                                                                                                    \n",
      "Val loss: 1.3848 ACC: 0.79465                                                                                                    \n",
      "New Best Val loss: 1.3848                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da8cfc5403e46868a8cf830751632d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2241/1.1656 ACC: 0.94384                                                                                                    \n",
      "Val loss: 1.3929 ACC: 0.77764                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b26084c0a947dcbbf9569258d82ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2163/1.1655 ACC: 0.95025                                                                                                    \n",
      "Val loss: 1.3892 ACC: 0.78493                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800519657548427e9e90cb1ccd79aadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2086/1.1655 ACC: 0.95864                                                                                                    \n",
      "Val loss: 1.3969 ACC: 0.77035                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6c30127a8b4e6abcd23ce89029f457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1989/1.1654 ACC: 0.96826                                                                                                    \n",
      "Val loss: 1.3932 ACC: 0.77764                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289abcf9756f4dc6887da123e37a265a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1943/1.1654 ACC: 0.97192                                                                                                    \n",
      "Val loss: 1.3995 ACC: 0.77157                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538ed615a7a64a608cee02b34f5f98ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1915/1.1654 ACC: 0.97497                                                                                                    \n",
      "Val loss: 1.3996 ACC: 0.76671                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663737c66c3343ddb85884bc688176cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1902/1.1654 ACC: 0.97543                                                                                                    \n",
      "Val loss: 1.4038 ACC: 0.76306                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d99aa38c2b415f8e826e2d91cced4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1901/1.1661 ACC: 0.97619                                                                                                    \n",
      "Val loss: 1.4086 ACC: 0.75942                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8fdd6492ed48f688d28f43a7f4772a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1888/1.2061 ACC: 0.97711                                                                                                    \n",
      "Val loss: 1.4084 ACC: 0.75942                                                                                                    \n",
      "Set dropout to 0.5442 (prop=1.184,1.001=1.088)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915834e433644161ab1fd7075bf3fe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1892/1.1654 ACC: 0.9765                                                                                                     \n",
      "Val loss: 1.4075 ACC: 0.76306                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3f87d44ac74d83aac41c34df0000e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1887/1.1654 ACC: 0.97711                                                                                                    \n",
      "Val loss: 1.4044 ACC: 0.76671                                                                                                    \n",
      "Epoch    16: reducing learning rate of group 0 to 5.7000e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221be755ced34f80a5e9d62ef18f0dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1873/1.1654 ACC: 0.97848                                                                                                    \n",
      "Val loss: 1.4035 ACC: 0.76549                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa7fc438dd34c359652b50b3fb52d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1876/1.1654 ACC: 0.97787                                                                                                    \n",
      "Val loss: 1.4111 ACC: 0.75577                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d85912f2ca9489d9e0dcb3cd15dcaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1862/1.1654 ACC: 0.9794                                                                                                     \n",
      "Val loss: 1.4069 ACC: 0.7582                                                                                                     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639cd16aa2944cddad567e974b86e73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1866/1.1654 ACC: 0.97909                                                                                                    \n",
      "Val loss: 1.4079 ACC: 0.75334                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bae4b87ae244a28af1b06a08e32b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1863/1.1654 ACC: 0.97909                                                                                                    \n",
      "Val loss: 1.4032 ACC: 0.76549                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde85dbae46746b0b57997e525eb6365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1856/1.1654 ACC: 0.98001                                                                                                    \n",
      "Val loss: 1.4097 ACC: 0.75334                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48d2a8188564fe098d6faab181d2294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1853/1.1654 ACC: 0.98047                                                                                                    \n",
      "Val loss: 1.4066 ACC: 0.7582                                                                                                     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0914cc18c84d28a9f339952ec29894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1852/1.1654 ACC: 0.98047                                                                                                    \n",
      "Val loss: 1.4065 ACC: 0.75577                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03a25b974f543f3b7ffd5129e6fb4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1852/1.1654 ACC: 0.98047                                                                                                    \n",
      "Val loss: 1.4028 ACC: 0.76549                                                                                                    \n",
      "Set dropout to 0.5921 (prop=1.184,1.0=1.088)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35e8caaab144a5492c3ff80a3163efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1861/1.1654 ACC: 0.97955                                                                                                    \n",
      "Val loss: 1.4056 ACC: 0.76063                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e38a10dfee4380b254b9dbf1519745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1845/1.1654 ACC: 0.98108                                                                                                    \n",
      "Val loss: 1.4082 ACC: 0.75699                                                                                                    \n",
      "Epoch    27: reducing learning rate of group 0 to 5.4150e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d7ec1d732249358640188a7c3a34d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1857/1.1654 ACC: 0.98001                                                                                                    \n",
      "Val loss: 1.4054 ACC: 0.75334                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f439ec781d4607ae974123b98a6bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1837/1.1654 ACC: 0.98199                                                                                                    \n",
      "Val loss: 1.4081 ACC: 0.75334                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b979cd8869534767be1b3b58555dcbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.185/1.1654 ACC: 0.98031                                                                                                     \n",
      "Val loss: 1.4117 ACC: 0.74605                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23e00c7a3be4005816b2797c56b4864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1835/1.1654 ACC: 0.98199                                                                                                    \n",
      "Val loss: 1.4184 ACC: 0.73755                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1877294a114b2fb05b602e43cb0c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 32:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1844/2.1654 ACC: 0.9823                                                                                                     \n",
      "Val loss: 1.4085 ACC: 0.7497                                                                                                     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c658ca39b114754abce2f21186e46f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 33:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1835/1.1654 ACC: 0.98199                                                                                                    \n",
      "Val loss: 1.4144 ACC: 0.74362                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6172e7a2cd42879374a62a291eb4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 34:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1835/1.1654 ACC: 0.98199                                                                                                    \n",
      "Val loss: 1.4121 ACC: 0.74848                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a7ffcc35f74a3f8b9d7b9c0c3a9ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 35:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1826/1.1654 ACC: 0.98321                                                                                                    \n",
      "Val loss: 1.4186 ACC: 0.74727                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43af87cecbd442b858a2bb960400e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 36:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1825/1.1654 ACC: 0.98306                                                                                                    \n",
      "Val loss: 1.4157 ACC: 0.74605                                                                                                    \n",
      "Set dropout to 0.6478 (prop=1.197,1.0=1.094)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b5e9cccb44f59be96e6ffa125d39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 37:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1839/1.1654 ACC: 0.98184                                                                                                    \n",
      "Val loss: 1.4127 ACC: 0.75334                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d69b022c854a2da8572a15b3b66088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 38:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1838/1.1654 ACC: 0.98199                                                                                                    \n",
      "Val loss: 1.4134 ACC: 0.75091                                                                                                    \n",
      "Epoch    38: reducing learning rate of group 0 to 5.1442e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f2ad4c9ab54a7da374c45a39f5d5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 39:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1834/1.1654 ACC: 0.9823                                                                                                     \n",
      "Val loss: 1.4151 ACC: 0.75213                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fbb5b51d1c42faa5bddba7ad33f39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 40:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1835/1.1654 ACC: 0.98245                                                                                                    \n",
      "Val loss: 1.4133 ACC: 0.75091                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4a377be12b4220b6eec95c922c8436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 41:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1825/1.1654 ACC: 0.98306                                                                                                    \n",
      "Val loss: 1.4158 ACC: 0.7497                                                                                                     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f10db0325334f7aa5b009c856db771c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 42:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1829/1.1654 ACC: 0.98291                                                                                                    \n",
      "Val loss: 1.4218 ACC: 0.74362                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f163afdcbc4828ab1ecc6b0a7c8dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 43:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1822/1.1654 ACC: 0.98337                                                                                                    \n",
      "Val loss: 1.4213 ACC: 0.74241                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb65adaa300497aab67ba4237eed4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 44:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1829/1.1655 ACC: 0.9826                                                                                                     \n",
      "Val loss: 1.418 ACC: 0.7497                                                                                                      \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093410b7011c403897c2467488d37465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 45:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1816/1.1654 ACC: 0.98382                                                                                                    \n",
      "Val loss: 1.4167 ACC: 0.75456                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c577ff2caba34e99a1a654fa817d90e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 46:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1821/1.1654 ACC: 0.98352                                                                                                    \n",
      "Val loss: 1.421 ACC: 0.7497                                                                                                      \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e99b33a6e941669402c0377c715e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 47:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1816/1.1654 ACC: 0.98413                                                                                                    \n",
      "Val loss: 1.4267 ACC: 0.73998                                                                                                    \n",
      "Set dropout to 0.7118 (prop=1.207,1.0=1.099)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a9327ef8bc46aa9293001feb882276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 48:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1856/1.1654 ACC: 0.98016                                                                                                    \n",
      "Val loss: 1.4214 ACC: 0.7497                                                                                                     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2cea71664b4bc4b7ad5c22e03fc553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 49:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1844/1.1654 ACC: 0.98215                                                                                                    \n",
      "Val loss: 1.423 ACC: 0.7497                                                                                                      \n",
      "Epoch    49: reducing learning rate of group 0 to 4.8870e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6708f4a756f747dd8e94ed54f468bb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 50:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1841/1.1655 ACC: 0.98199                                                                                                    \n",
      "Val loss: 1.4248 ACC: 0.74362                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe70e7c97f945cfabeeb877026539a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 51:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1851/1.1654 ACC: 0.98062                                                                                                    \n",
      "Val loss: 1.4254 ACC: 0.73998                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3e86d3c56741999ce30aac15fe2d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 52:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1879/1.1754 ACC: 0.97787                                                                                                    \n",
      "Val loss: 1.4248 ACC: 0.73755                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8681e26fcc91440ba3deb24e6cf12d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 53:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1848/1.1654 ACC: 0.98123                                                                                                    \n",
      "Val loss: 1.43 ACC: 0.73512                                                                                                      \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16919e8f3e2495485477c0b8249202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 54:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1855/1.1654 ACC: 0.98108                                                                                                    \n",
      "Val loss: 1.4313 ACC: 0.73269                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081fc94ca9e9443282380dc4b8dee009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 55:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1865/1.1654 ACC: 0.9794                                                                                                     \n",
      "Val loss: 1.4273 ACC: 0.73633                                                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a67c6fa5dd74f38b7fe83ca1207859c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 56:   0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.187/1.1654 ACC: 0.97894                                                                                                     \n",
      "Val loss: 1.4315 ACC: 0.73269                                                                                                    \n",
      "\n",
      "Best Val loss: 1.3848                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "best = 99999.\n",
    "counter = 1\n",
    "eps=0.6\n",
    "old_loss_train = 1.\n",
    "dl_val = DataLoader(list(zip(fold.X_val, y_val)), batch_size=batch_size,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=12)\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    dl_train = DataLoader(list(zip(fold.X_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_train, num_workers=12)\n",
    "    loss_train  = 0.\n",
    "    with tqdm(total=len(y_train)+len(y_val), smoothing=0., desc=f\"Epoch {e+1}\") as pbar:\n",
    "        total = 0\n",
    "        correct  = 0\n",
    "        ab.train()\n",
    "        for i, (terms_idx, docs_offsets, y) in enumerate(dl_train):\n",
    "            terms_idx    = terms_idx.to( device )\n",
    "            docs_offsets = docs_offsets.to( device )\n",
    "            y            = y.to( device )\n",
    "            \n",
    "            pred_docs,_ = ab( terms_idx, docs_offsets)\n",
    "            pred_docs = F.softmax(pred_docs)\n",
    "            loss = loss_func_cel(pred_docs, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            total      += len(y)\n",
    "            y_pred      = pred_docs.argmax(axis=1)\n",
    "            correct    += (y_pred == y).sum().item()\n",
    "            \n",
    "            toprint  = f\"Train loss: {loss_train/(i+1):.5}/{loss.item():.5} \"\n",
    "            toprint += f'ACC: {correct/total:.5}'\n",
    "            \n",
    "            print(toprint, end=f\"{' '*100}\\r\")\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            del pred_docs, loss\n",
    "            del terms_idx, docs_offsets, y\n",
    "            del y_pred\n",
    "        loss_train = loss_train/(i+1)\n",
    "        total = 0\n",
    "        correct  = 0\n",
    "        ab.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.\n",
    "            print()\n",
    "            for i, (terms_idx, docs_offsets, y) in enumerate(dl_val):\n",
    "                terms_idx    = terms_idx.to( device )\n",
    "                docs_offsets = docs_offsets.to( device )\n",
    "                y            = y.to( device )\n",
    "\n",
    "                pred_docs,weights = ab( terms_idx, docs_offsets )\n",
    "                pred_docs   = F.softmax(pred_docs)\n",
    "\n",
    "                y_pred      = pred_docs.argmax(axis=1)\n",
    "                correct    += (y_pred == y).sum().item()\n",
    "                total      += len(y)\n",
    "                loss2       = loss_func_cel(pred_docs, y)\n",
    "                loss_val   += loss2\n",
    "\n",
    "                print(f'Val loss: {loss_val.item()/(i+1):.5} ACC: {correct/total:.5}', end=f\"{' '*100}\\r\")\n",
    "   \n",
    "                pbar.update( len(y) )\n",
    "\n",
    "            del terms_idx, docs_offsets, y\n",
    "            del y_pred\n",
    "            \n",
    "            loss_val   = loss_val/(i+1)\n",
    "            \n",
    "            old_loss_train = (1.-eps)*old_loss_train + eps*loss_train\n",
    "            \n",
    "            #prop_val   = loss_val.cpu() / (old_loss_train+optimizer.param_groups[0]['lr'])\n",
    "            prop_val   = loss_val.cpu() / old_loss_train\n",
    "            ## if loss_train < loss_val, prop_val > 1 --  Overfitting\n",
    "            ## if loss_train > loss_val, prop_val < 1 -- Underfitting\n",
    "            \n",
    "            prop_learn = old_loss_train / loss_train\n",
    "            ## if loss_train < old_loss_train, prop_learn > 1 -- Aprendendo\n",
    "            ## if loss_train > old_loss_train, prop_learn < 1 -- Desaprendendo\n",
    "            \n",
    "            #prop = (2.*prop_val*prop_learn)/(prop_val+prop_learn)\n",
    "            prop = np.sqrt(prop_val*prop_learn)\n",
    "            print()\n",
    "            scheduler.step(loss_val)\n",
    "            if scheduler.num_bad_epochs == (scheduler.patience-1):\n",
    "                ab.drop_ *= prop\n",
    "                ab.drop_ = max(0.05, min(ab.drop_, 0.95))\n",
    "                print(f\"Set dropout to {ab.drop_:.4} (prop={prop_val:.4},{prop_learn:.4}={prop:.4})\")\n",
    "                \n",
    "            #scheduler.step()\n",
    "\n",
    "            if best-loss_val > 0.0001 :\n",
    "                best = loss_val.item()\n",
    "                counter = 1\n",
    "                print(f'New Best Val loss: {best:.5}', end=f\"{' '*100}\\n\")\n",
    "                best_model = copy.deepcopy(ab).to('cpu')\n",
    "            elif counter > max_epochs:\n",
    "                print()\n",
    "                print(f'Best Val loss: {best:.5}', end=f\"{' '*100}\\n\")\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "            del pred_docs, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'terms_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bcb2f1ff5ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterms_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_offsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'terms_idx' is not defined"
     ]
    }
   ],
   "source": [
    "terms_idx.shape, docs_offsets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3754 ACC: 0.78979                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "ab = copy.deepcopy(best_model).to(device)\n",
    "loss_total = 0\n",
    "correct_t = 0\n",
    "total_t = 0\n",
    "dl_test = DataLoader(list(zip(fold.X_test, y_test)), batch_size=batch_size,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=2)\n",
    "for i, (terms_idx_t, docs_offsets_t, y_t) in enumerate(dl_test):\n",
    "    terms_idx_t    = terms_idx_t.to( device )\n",
    "    docs_offsets_t = docs_offsets_t.to( device )\n",
    "    y_t            = y_t.to( device )\n",
    "\n",
    "    pred_docs_t,weigths = ab( terms_idx_t, docs_offsets_t )\n",
    "    pred_docs_t = F.softmax(pred_docs_t)\n",
    "\n",
    "    y_pred_t    = pred_docs_t.argmax(axis=1)\n",
    "    correct_t  += (y_pred_t == y_t).sum().item()\n",
    "    total_t    += len(y_t)\n",
    "    loss_total += loss_func_cel(pred_docs_t, y_t)\n",
    "\n",
    "print(f'Test loss: {loss_total.item()/(i+1):.5} ACC: {correct_t/total_t:.5}', end=f\"{' '*100}\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "acm ####################################################################################\n",
    "Train loss: 1.6009/1.6089 ACC: 0.94886                                                                                                    \n",
    "Val loss: 1.7718 ACC: 0.77475                                                                                                    \n",
    "New Best Val loss: 1.7718                                                                                                    \n",
    "Test loss: 1.7678 ACC: 0.78557  79.92\n",
    "\n",
    "Train loss: 1.7209/1.6095 ACC: 0.82338                                                                                                    \n",
    "Val loss: 1.7595 ACC: 0.78236                                                                                                    \n",
    "New Best Val loss: 1.7595                                                                                             \n",
    "Test loss: 1.7585 ACC: 0.78557                                                                                                    \n",
    "\n",
    "20ng ####################################################################################\n",
    "Train loss: 2.0907/2.0787 ACC: 0.98845                                                                                                    \n",
    "Val loss: 2.1869 ACC: 0.90803                                                                                                    \n",
    "New Best Val loss: 2.1869                                                                                                    \n",
    "Test loss: 2.178 ACC: 0.91068   92.65\n",
    "\n",
    "reut ####################################################################################\n",
    "Train loss: 3.7735/3.5191 ACC: 0.74734                                                                                                    \n",
    "Val loss: 3.8554 ACC: 0.6763                                                                                                    \n",
    "New Best Val loss: 3.8554                                                                                                    \n",
    "Test loss: 3.8493 ACC: 0.6837  72.67\n",
    "\n",
    "Train loss: 3.7489/3.8717 ACC: 0.77265                                                                                                    \n",
    "Val loss: 3.8084 ACC: 0.71037                                                                                                    \n",
    "Adjusting learning rate of group 0 to 5.1046e-03.\n",
    "New Best Val loss: 3.8084 \n",
    "\n",
    "webkb ####################################################################################\n",
    "Train loss: 1.2228/1.2037 ACC: 0.9504                                                                                                     \n",
    "Val loss: 1.3787 ACC: 0.80316                                                                                                    \n",
    "New Best Val loss: 1.3787                                                                                                    \n",
    "Test loss: 1.3857 ACC: 0.78858   81.53\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBag(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, drop=.5, initrange=.5):\n",
    "        super(AttentionBag, self).__init__()\n",
    "        self.hiddens    = hiddens\n",
    "        self.mask       = Mask()\n",
    "        self.dt_emb     = nn.Embedding(vocab_size, hiddens)\n",
    "        self.dt_dir_map = nn.Linear(hiddens, hiddens)\n",
    "        self.drop       = nn.Dropout(drop)\n",
    "        self.ma_term    = nn.MultiheadAttention(hiddens, 1)\n",
    "        self.fc         = nn.Linear(hiddens, nclass)\n",
    "        self.initrange  = initrange \n",
    "        self.init_weights()\n",
    "    def forward(self, terms_idx, docs_offsets):\n",
    "        n = terms_idx.shape[0]\n",
    "        batch_size = docs_offsets.shape[0]\n",
    "        \n",
    "        k         = [ terms_idx[ docs_offsets[i-1]:docs_offsets[i] ] for i in range(1, batch_size) ]\n",
    "        k.append( terms_idx[ docs_offsets[-1]: ] )\n",
    "        x_packed  = pad_sequence(k, batch_first=True, padding_value=0)\n",
    "\n",
    "        bx_packed = x_packed == 0\n",
    "        pad_mask  = bx_packed.logical_not()\n",
    "        pad_mask  = pad_mask.view(*bx_packed.shape, 1)\n",
    "        pad_mask  = pad_mask.logical_and(pad_mask.transpose(1, 2))\n",
    "        \n",
    "        dt_h      = self.dt_emb( x_packed )\n",
    "        dt_h      = self.drop(dt_h)\n",
    "        dir_dt_h  = self.dt_dir_map( dt_h )\n",
    "\n",
    "        weights = torch.bmm( dt_h, dir_dt_h.transpose( 1, 2 ) )\n",
    "        weights = self.mask(weights)\n",
    "        \n",
    "        weights_disc = (weights * pad_mask)\n",
    "        weights_disc = weights_disc.sum(axis=1)\n",
    "        weights_disc = F.softmax(weights_disc, dim=1)\n",
    "        weights_disc = weights_disc.view( *weights_disc.shape, 1 )\n",
    "        \n",
    "        attn_mask = weights != 0\n",
    "        attn_mask = attn_mask.logical_and( pad_mask ).logical_not()\n",
    "        \n",
    "        dt_h     = dt_h.transpose(0,1)\n",
    "        dir_dt_h = dir_dt_h.transpose(0,1)\n",
    "        docs_att, weigths_att = self.ma_term( dt_h, dir_dt_h, dt_h,\n",
    "                                  key_padding_mask=bx_packed, \n",
    "                                  attn_mask=attn_mask )\n",
    "\n",
    "        weigths_att = torch.where(torch.isnan(weigths_att), torch.zeros_like(weigths_att), weigths_att)\n",
    "        weigths_att = (weigths_att * pad_mask)\n",
    "        weigths_att = weigths_att.sum(axis=1)\n",
    "        weigths_att = F.softmax(weigths_att, dim=1)\n",
    "        weigths_att = weigths_att.view( *weigths_att.shape, 1 )\n",
    "        \n",
    "        weigths = weights_disc + weigths_att\n",
    "\n",
    "        docs_att = docs_att.transpose(0,1)\n",
    "        docs_att = torch.where(torch.isnan(docs_att), torch.zeros_like(docs_att), docs_att)\n",
    "        \n",
    "        docs_h = docs_att * weigths\n",
    "        docs_h = docs_h.sum(axis=1)\n",
    "        docs_h = docs_h / bx_packed.logical_not().sum(dim=1).view(batch_size, 1)\n",
    "        docs_h = torch.where(torch.isnan(docs_h), torch.zeros_like(docs_h), docs_h)\n",
    "        docs_h = self.drop(docs_h)\n",
    "        return self.fc(docs_h)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.dt_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dt_dir_map.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.fc.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.ma_term.in_proj_weight.data.uniform_(-self.initrange, self.initrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t      = pred_docs_t.argmax(axis=1)\n",
    "correct_t     = (y_pred_t == y_t).sum().item()\n",
    "total_t       = len(y_t)\n",
    "correct_t/total_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_offsets_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_idx_t[:docs_offsets_t[batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_off_test  = docs_offsets_t[batch_size]\n",
    "batch_tidx_test = terms_idx_t[:batch_off_test]\n",
    "h_terms_test    = sc.tt_emb( batch_tidx_test )\n",
    "dirh_terms_test = sc.tt_dir_map( h_terms_test )\n",
    "\n",
    "W = torch.matmul( h_terms_test, dirh_terms_test.T )\n",
    "W = F.leaky_relu( W, negative_slope=sc.mask.negative_slope)\n",
    "W = F.sigmoid(W)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [ batch_tidx_test[ docs_offsets_t[i-1]:docs_offsets_t[i] ] for i in range(1, batch_size) ]\n",
    "k.append( batch_tidx_test[ docs_offsets_t[batch_size-1]:docs_offsets_t[batch_size] ] )\n",
    "x_packed = pad_sequence(k, batch_first=True, padding_value=0)\n",
    "tt_emb = sc.tt_emb( x_packed )\n",
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_packed = pad_sequence(k, batch_first=True, padding_value=0)\n",
    "tt_emb = sc.tt_emb( x_packed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_emb.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [terms_idx, terms_idx]\n",
    "torch.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F.softmax(pred_docs_t).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_t == F.softmax(pred_docs_t).argmax(axis=1)).sum().item()/y_t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_idx_t, docs_offsets_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = sc._get_shift_(docs_offsets_t, terms_idx_t.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipado = zip(docs_offsets_t, shifts)\n",
    "#next(zipado)\n",
    "start,size = next(zipado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sc.tt_emb( terms_idx_t[start:start+size] )\n",
    "w1 = sc.tt_dir_map( w )\n",
    "w = torch.matmul( w, w1.T )\n",
    "w = F.leaky_relu( w, negative_slope=sc.negative_slope)\n",
    "w = F.sigmoid(w)\n",
    "#w = F.tanh(w)\n",
    "#w = F.relu(w)\n",
    "#w = w.mean(axis=1)\n",
    "#w = F.softmax(w)\n",
    "w,w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.round().sum() / (w.shape[0]*w.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapper = { v:k for (k,v) in tokenizer.node_mapper.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_idx[start:start+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold.X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = w.mean(axis=1)\n",
    "bla = F.softmax(bla)\n",
    "#bla = bla/torch.clamp(bla.sum(), 0.0001)\n",
    "bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ (i, tid.item(),inv_mapper[tid.item()], wei.item()) for i, (tid, wei) in enumerate(zip(terms_idx[start:start+size], bla)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = nn.BatchNorm1d(num_features=1).to(device)\n",
    "\n",
    "bla2 = norm(bla.view(-1, 1)).squeeze()\n",
    "bla2 = F.sigmoid(bla2)\n",
    "bla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = nn.MultiheadAttention(300, 300).to(device)\n",
    "ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = w.shape\n",
    "w_ = w.view(a,1,b)\n",
    "w1_ = w1.view(a,1,b)\n",
    "\n",
    "attn_output = ma(w_, w1_, w_, need_weights=False)\n",
    "\n",
    "attn_output.view(a,b)\n",
    "attn_output_weights.view(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output.view(a,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_weights.view(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.Tensor([[0.0718, 0.0716, 0.0712, 0.0714, 0.0721, 0.0710, 0.0712, 0.0714, 0.0710,\n",
    "         0.0719, 0.0711, 0.0712, 0.0718, 0.0714],\n",
    "        [0.0722, 0.0709, 0.0710, 0.0709, 0.0728, 0.0723, 0.0709, 0.0709, 0.0719,\n",
    "         0.0712, 0.0709, 0.0707, 0.0725, 0.0707],\n",
    "        [0.0711, 0.0715, 0.0710, 0.0713, 0.0710, 0.0712, 0.0718, 0.0713, 0.0709,\n",
    "         0.0725, 0.0716, 0.0719, 0.0710, 0.0719],\n",
    "        [0.0721, 0.0717, 0.0714, 0.0713, 0.0717, 0.0714, 0.0711, 0.0710, 0.0713,\n",
    "         0.0717, 0.0710, 0.0712, 0.0720, 0.0711],\n",
    "        [0.0722, 0.0711, 0.0710, 0.0710, 0.0737, 0.0716, 0.0709, 0.0705, 0.0714,\n",
    "         0.0719, 0.0707, 0.0707, 0.0731, 0.0702],\n",
    "        [0.0714, 0.0716, 0.0708, 0.0711, 0.0718, 0.0713, 0.0712, 0.0717, 0.0712,\n",
    "         0.0724, 0.0713, 0.0712, 0.0716, 0.0714],\n",
    "        [0.0712, 0.0714, 0.0713, 0.0713, 0.0722, 0.0716, 0.0709, 0.0714, 0.0714,\n",
    "         0.0717, 0.0713, 0.0713, 0.0716, 0.0713],\n",
    "        [0.0716, 0.0707, 0.0712, 0.0714, 0.0717, 0.0715, 0.0714, 0.0715, 0.0712,\n",
    "         0.0721, 0.0711, 0.0714, 0.0717, 0.0715],\n",
    "        [0.0717, 0.0713, 0.0708, 0.0710, 0.0725, 0.0717, 0.0714, 0.0709, 0.0712,\n",
    "         0.0712, 0.0712, 0.0717, 0.0720, 0.0714],\n",
    "        [0.0709, 0.0712, 0.0713, 0.0712, 0.0713, 0.0713, 0.0714, 0.0719, 0.0712,\n",
    "         0.0724, 0.0715, 0.0715, 0.0717, 0.0713],\n",
    "        [0.0715, 0.0716, 0.0712, 0.0708, 0.0725, 0.0717, 0.0712, 0.0714, 0.0714,\n",
    "         0.0717, 0.0713, 0.0706, 0.0718, 0.0712],\n",
    "        [0.0726, 0.0711, 0.0713, 0.0706, 0.0737, 0.0721, 0.0709, 0.0707, 0.0714,\n",
    "         0.0708, 0.0706, 0.0705, 0.0730, 0.0707],\n",
    "        [0.0718, 0.0711, 0.0714, 0.0715, 0.0725, 0.0707, 0.0707, 0.0711, 0.0712,\n",
    "         0.0728, 0.0711, 0.0713, 0.0719, 0.0709],\n",
    "        [0.0712, 0.0716, 0.0713, 0.0712, 0.0717, 0.0707, 0.0708, 0.0721, 0.0709,\n",
    "         0.0728, 0.0713, 0.0716, 0.0711, 0.0715]]).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotTooSimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_l, nclass, dropout1=0.1, dropout2=0.1, negative_slope=99,\n",
    "                 initrange = 0.5, scale_grad_by_freq=False, device='cuda:0'):\n",
    "        super(NotTooSimpleClassifier, self).__init__()\n",
    "        \n",
    "        self.dt_emb = nn.Embedding(vocab_size, hidden_l, scale_grad_by_freq=scale_grad_by_freq)\n",
    "        self.tt_emb = nn.Embedding(vocab_size, hidden_l, scale_grad_by_freq=scale_grad_by_freq)\n",
    "        \n",
    "        self.undirected_map = nn.Linear(hidden_l, hidden_l)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_l, nclass)\n",
    "        self.drop1 = nn.Dropout(dropout1)\n",
    "        self.drop2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d(1)\n",
    "        \n",
    "        self.initrange = initrange\n",
    "        self.nclass = nclass\n",
    "        self.negative_slope = negative_slope\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        #self.labls_emb = nn.Embedding(graph_builder.n_class, 300)\n",
    "    \n",
    "    def forward(self, terms_idxs, docs_offsets):\n",
    "        n = terms_idxs.shape[0]\n",
    "        weights = []\n",
    "        shifts = self._get_shift_(docs_offsets, n)\n",
    "        \n",
    "        terms_h1 = self.tt_emb(terms_idxs)\n",
    "        terms_h1 = self.drop1(terms_h1)\n",
    "        \n",
    "        terms_h2 = self.undirected_map( terms_h1 )\n",
    "        #terms_h2 = self.drop1( terms_h2 )\n",
    "        for start,size in zip(docs_offsets, shifts):\n",
    "            w  = terms_h1[start:start+size]\n",
    "            w1 = terms_h2[start:start+size]\n",
    "            w = torch.matmul( w, w1.T )\n",
    "            w = F.leaky_relu( w, negative_slope=self.negative_slope)\n",
    "            w = F.sigmoid(w-5.5)\n",
    "            w = w.mean(axis=1)\n",
    "            w = F.softmax(w)\n",
    "            #w = w / torch.clamp(w.sum(), 0.0001)\n",
    "            weights.append( w )\n",
    "        \n",
    "        weights = torch.cat(weights)\n",
    "        #weights = self.norm(weights.view(-1, 1)).squeeze()\n",
    "        #weights = F.sigmoid(weights)\n",
    "        \n",
    "        h_docs  = F.embedding_bag(self.dt_emb.weight, terms_idxs, docs_offsets, per_sample_weights=weights, mode='sum')\n",
    "        h_docs = self.drop2( h_docs )\n",
    "        pred_docs = self.fc( h_docs )\n",
    "        return pred_docs\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.dt_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.tt_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "    def _get_shift_(self, offsets, lenght):\n",
    "        shifts = offsets[1:] - offsets[:-1]\n",
    "        last = torch.LongTensor([lenght - offsets[-1]]).to( offsets.device )\n",
    "        return torch.cat([shifts, last])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
