{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset, Graphsize\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from time import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "webkb = Dataset('/home/mangaravite/Documents/datasets/topics/webkb/')\n",
    "_20ng = Dataset('/home/mangaravite/Documents/datasets/topics/20ng/')\n",
    "acm   = Dataset('/home/mangaravite/Documents/datasets/topics/acm/')\n",
    "reut  = Dataset('/home/mangaravite/Documents/datasets/topics/reut/')\n",
    "\n",
    "dataset = reut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = next(dataset.get_fold_instances(5))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7923/7923 [00:13<00:00, 584.11it/s]\n",
      "100%|██████████| 7923/7923 [00:04<00:00, 1744.35it/s]\n",
      "100%|██████████| 7923/7923 [01:15<00:00, 104.84it/s]\n",
      "100%|██████████| 7923/7923 [00:14<00:00, 562.40it/s]\n",
      "100%|██████████| 2702/2702 [00:01<00:00, 1790.30it/s]\n",
      "100%|██████████| 2702/2702 [00:26<00:00, 101.96it/s]\n",
      "100%|██████████| 2702/2702 [00:04<00:00, 573.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 19s, sys: 1.22 s, total: 2min 21s\n",
      "Wall time: 2min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_feats = 25000\n",
    "graph_builder = Graphsize(w=5, verbose=True, max_feat=max_feats, feature_type='full_weight_prob')\n",
    "Gs_train = graph_builder.fit_transform(fold.X_train)\n",
    "Gs_val   = graph_builder.transform(fold.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2702/2702 [00:01<00:00, 1732.69it/s]\n",
      "100%|██████████| 2702/2702 [00:26<00:00, 102.76it/s]\n",
      "100%|██████████| 2702/2702 [00:04<00:00, 564.19it/s]\n"
     ]
    }
   ],
   "source": [
    "Gs_test  = graph_builder.transform(fold.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifierGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=.5):\n",
    "        super(SimpleClassifierGCN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            #GraphConv(in_dim, hidden_dim, activation=F.leaky_relu).to(torch.device('cuda:0')),\n",
    "            #GraphConv(hidden_dim, hidden_dim, activation=F.leaky_relu).to(torch.device('cuda:0'))\n",
    "            GraphConv(in_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            #h = h.view(h.shape[0], -1)\n",
    "        G.ndata['h'] = h\n",
    "        w = self.lin( h )\n",
    "        G.ndata['w'] = w\n",
    "        hg = dgl.mean_nodes(G, 'h', weight='w')\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        #pred = torch.softmax(pred, 1)\n",
    "        return pred\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            #h = h.view(h.shape[0], -1)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        return hg\n",
    "class ClassifierGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=.5):\n",
    "        super(ClassifierGCN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(2*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( 2*hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(2*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((h,he), 1)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred\n",
    "    \n",
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, n_heads=16, drop=.5):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(in_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=drop).to(torch.device('cuda:0')),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=drop).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(n_heads*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(n_heads*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        pred = self.classify( hg )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor( [[1,2,3,4,5],[5,4,3,2,1]] )\n",
    "b = torch.tensor( [[-1,-2,-3,-4,-5],[-5,-4,-3,-2,-1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    Gs_Fs, labels = map(list, zip(*samples))\n",
    "    graphs = []\n",
    "    for g, f in Gs_Fs:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g)\n",
    "        g_dgl.ndata['f'] = torch.FloatTensor(f.A).to(torch.device('cuda:0'))\n",
    "        g_dgl.to(torch.device('cuda:0'))\n",
    "        graphs.append(g_dgl)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.to(torch.device('cuda:0'))\n",
    "    labels = torch.tensor(labels).to(torch.device('cuda:0'))\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # https://github.com/mbsariyildiz/focal-loss.pytorch\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'best_param_simple_reut.pth'\n",
    "n_epochs = 100\n",
    "patience = 25\n",
    "hidden_dim = 300\n",
    "train_batch_size = 16\n",
    "test_val_batch_size = 256\n",
    "\n",
    "#model = SimpleClassifierGCN(len(graph_builder.vocab), hidden_dim, dataset.nclass, drop=.5).to(torch.device('cuda:0'))\n",
    "#model = ClassifierGAT(len(graph_builder.vocab), hidden_dim, dataset.nclass, n_heads=2, drop=.5).to(torch.device('cuda:0'))\n",
    "model = ClassifierGCN(len(graph_builder.vocab), hidden_dim, dataset.nclass, drop=.5).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_func = FocalLoss().to(torch.device('cuda:0'))\n",
    "loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()\n",
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val  = DataLoader(list(zip(Gs_val,  fold.y_val )), batch_size=test_val_batch_size,\n",
    "                              shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter 0, train acc 0.628 train loss 843.63: 100%|██████████| 7923/7923 [00:40<00:00, 196.06it/s]\n",
      "iter 0, val   acc 0.674 ( over: 1.07/0 ): 100%|██████████| 2702/2702 [00:12<00:00, 222.05it/s]\n",
      "iter 1, train acc 0.739 train loss 240.16: 100%|██████████| 7923/7923 [00:40<00:00, 193.87it/s]\n",
      "iter 1, val   acc 0.685 ( over: 0.928/0 ): 100%|██████████| 2702/2702 [00:12<00:00, 215.45it/s]\n",
      "iter 2, train acc 0.766 train loss 130.30: 100%|██████████| 7923/7923 [00:41<00:00, 188.84it/s]\n",
      "iter 2, val   acc 0.687 ( over: 0.897/0 ): 100%|██████████| 2702/2702 [00:12<00:00, 222.28it/s]\n",
      "iter 3, train acc 0.778 train loss 88.48: 100%|██████████| 7923/7923 [00:39<00:00, 199.82it/s]\n",
      "iter 3, val   acc 0.684 ( over: 0.88/1 ): 100%|██████████| 2702/2702 [00:11<00:00, 226.81it/s]\n",
      "iter 4, train acc 0.777 train loss 69.43: 100%|██████████| 7923/7923 [00:42<00:00, 188.13it/s]\n",
      "iter 4, val   acc 0.681 ( over: 0.877/2 ): 100%|██████████| 2702/2702 [00:11<00:00, 231.41it/s]\n",
      "iter 5, train acc 0.781 train loss 56.29: 100%|██████████| 7923/7923 [00:41<00:00, 189.16it/s]\n",
      "iter 5, val   acc 0.681 ( over: 0.872/3 ): 100%|██████████| 2702/2702 [00:11<00:00, 233.11it/s]\n",
      "iter 6, train acc 0.788 train loss 45.71: 100%|██████████| 7923/7923 [00:40<00:00, 196.61it/s]\n",
      "iter 6, val   acc 0.689 ( over: 0.875/0 ): 100%|██████████| 2702/2702 [00:11<00:00, 225.28it/s]\n",
      "iter 7, train acc 0.787 train loss 39.28: 100%|██████████| 7923/7923 [00:41<00:00, 190.36it/s]\n",
      "iter 7, val   acc 0.692 ( over: 0.88/0 ): 100%|██████████| 2702/2702 [00:13<00:00, 207.37it/s]\n",
      "iter 8, train acc 0.787 train loss 35.47: 100%|██████████| 7923/7923 [00:40<00:00, 194.14it/s]\n",
      "iter 8, val   acc 0.679 ( over: 0.863/1 ): 100%|██████████| 2702/2702 [00:12<00:00, 224.41it/s]\n",
      "iter 9, train acc 0.789 train loss 31.52: 100%|██████████| 7923/7923 [00:40<00:00, 194.55it/s]\n",
      "iter 9, val   acc 0.681 ( over: 0.863/2 ): 100%|██████████| 2702/2702 [00:12<00:00, 218.99it/s]\n",
      "iter 10, train acc 0.788 train loss 29.00: 100%|██████████| 7923/7923 [00:40<00:00, 195.53it/s]\n",
      "iter 10, val   acc 0.687 ( over: 0.871/3 ): 100%|██████████| 2702/2702 [00:11<00:00, 226.24it/s]\n",
      "iter 11, train acc 0.793 train loss 24.87: 100%|██████████| 7923/7923 [00:39<00:00, 199.86it/s]\n",
      "iter 11, val   acc 0.698 ( over: 0.88/0 ): 100%|██████████| 2702/2702 [00:12<00:00, 220.47it/s]\n",
      "iter 12, train acc 0.788 train loss 23.25: 100%|██████████| 7923/7923 [00:41<00:00, 191.17it/s]\n",
      "iter 12, val   acc 0.697 ( over: 0.884/1 ): 100%|██████████| 2702/2702 [00:11<00:00, 234.48it/s]\n",
      "iter 13, train acc 0.789 train loss 22.09: 100%|██████████| 7923/7923 [00:40<00:00, 196.36it/s]\n",
      "iter 13, val   acc 0.684 ( over: 0.867/2 ): 100%|██████████| 2702/2702 [00:11<00:00, 234.46it/s]\n",
      "iter 14, train acc 0.789 train loss 20.11: 100%|██████████| 7923/7923 [00:40<00:00, 197.92it/s]\n",
      "iter 14, val   acc 0.678 ( over: 0.859/3 ): 100%|██████████| 2702/2702 [00:12<00:00, 213.00it/s]\n",
      "iter 15, train acc 0.792 train loss 18.53: 100%|██████████| 7923/7923 [00:41<00:00, 192.15it/s]\n",
      "iter 15, val   acc 0.681 ( over: 0.86/4 ): 100%|██████████| 2702/2702 [00:11<00:00, 226.77it/s]\n",
      "iter 16, train acc 0.797 train loss 17.60: 100%|██████████| 7923/7923 [00:40<00:00, 194.42it/s]\n",
      "iter 16, val   acc 0.694 ( over: 0.87/5 ): 100%|██████████| 2702/2702 [00:12<00:00, 224.11it/s]\n",
      "iter 17, train acc 0.791 train loss 16.61: 100%|██████████| 7923/7923 [00:40<00:00, 195.98it/s]\n",
      "iter 17, val   acc 0.682 ( over: 0.862/6 ): 100%|██████████| 2702/2702 [00:12<00:00, 222.83it/s]\n",
      "iter 18, train acc 0.790 train loss 15.82: 100%|██████████| 7923/7923 [00:42<00:00, 185.71it/s]\n",
      "iter 18, val   acc 0.670 ( over: 0.849/7 ): 100%|██████████| 2702/2702 [00:13<00:00, 207.78it/s]\n",
      "iter 19, train acc 0.794 train loss 14.60: 100%|██████████| 7923/7923 [00:43<00:00, 181.45it/s]\n",
      "iter 19, val   acc 0.682 ( over: 0.858/8 ): 100%|██████████| 2702/2702 [00:12<00:00, 221.81it/s]\n",
      "iter 20, train acc 0.795 train loss 14.42: 100%|██████████| 7923/7923 [00:43<00:00, 180.40it/s]\n",
      "iter 20, val   acc 0.682 ( over: 0.859/9 ): 100%|██████████| 2702/2702 [00:11<00:00, 227.94it/s]\n",
      "iter 21, train acc 0.797 train loss 13.05: 100%|██████████| 7923/7923 [00:40<00:00, 196.07it/s]\n",
      "iter 21, val   acc 0.691 ( over: 0.866/10 ): 100%|██████████| 2702/2702 [00:11<00:00, 233.57it/s]\n",
      "iter 22, train acc 0.790 train loss 12.75: 100%|██████████| 7923/7923 [00:40<00:00, 195.27it/s]\n",
      "iter 22, val   acc 0.692 ( over: 0.875/11 ): 100%|██████████| 2702/2702 [00:12<00:00, 222.57it/s]\n",
      "iter 23, train acc 0.793 train loss 11.82: 100%|██████████| 7923/7923 [00:39<00:00, 198.56it/s]\n",
      "iter 23, val   acc 0.681 ( over: 0.859/12 ): 100%|██████████| 2702/2702 [00:12<00:00, 217.80it/s]\n",
      "iter 24, train acc 0.796 train loss 11.42: 100%|██████████| 7923/7923 [00:41<00:00, 193.20it/s]\n",
      "iter 24, val   acc 0.686 ( over: 0.862/13 ): 100%|██████████| 2702/2702 [00:11<00:00, 226.57it/s]\n",
      "iter 25, train acc 0.796 train loss 11.02: 100%|██████████| 7923/7923 [00:40<00:00, 197.78it/s]\n",
      "iter 25, val   acc 0.682 ( over: 0.858/14 ): 100%|██████████| 2702/2702 [00:12<00:00, 224.57it/s]\n",
      "iter 26, train acc 0.802 train loss 10.30: 100%|██████████| 7923/7923 [00:42<00:00, 187.14it/s]\n",
      "iter 26, val   acc 0.679 ( over: 0.847/15 ): 100%|██████████| 2702/2702 [00:12<00:00, 209.58it/s]\n",
      "iter 27, train acc 0.799 train loss 9.98: 100%|██████████| 7923/7923 [00:40<00:00, 194.78it/s]\n",
      "iter 27, val   acc 0.676 ( over: 0.847/16 ): 100%|██████████| 2702/2702 [00:12<00:00, 220.88it/s]\n",
      "iter 28, train acc 0.799 train loss 9.65: 100%|██████████| 7923/7923 [00:40<00:00, 194.67it/s]\n",
      "iter 28, val   acc 0.684 ( over: 0.856/17 ): 100%|██████████| 2702/2702 [00:11<00:00, 227.38it/s]\n",
      "iter 29, train acc 0.801 train loss 9.06: 100%|██████████| 7923/7923 [00:41<00:00, 190.39it/s]\n",
      "iter 29, val   acc 0.685 ( over: 0.855/18 ): 100%|██████████| 2702/2702 [00:12<00:00, 220.84it/s]\n",
      "iter 30, train acc 0.802 train loss 8.71: 100%|██████████| 7923/7923 [00:42<00:00, 185.20it/s]\n",
      "iter 30, val   acc 0.685 ( over: 0.854/19 ): 100%|██████████| 2702/2702 [00:12<00:00, 215.91it/s]\n",
      "iter 31, train acc 0.799 train loss 8.38: 100%|██████████| 7923/7923 [00:40<00:00, 196.06it/s]\n",
      "iter 31, val   acc 0.676 ( over: 0.846/20 ): 100%|██████████| 2702/2702 [00:12<00:00, 223.03it/s]\n",
      "iter 32, train acc 0.801 train loss 8.29: 100%|██████████| 7923/7923 [00:40<00:00, 193.77it/s]\n",
      "iter 32, val   acc 0.678 ( over: 0.846/21 ): 100%|██████████| 2702/2702 [00:11<00:00, 226.80it/s]\n",
      "iter 33, train acc 0.800 train loss 7.97: 100%|██████████| 7923/7923 [00:39<00:00, 198.33it/s]\n",
      "iter 33, val   acc 0.680 ( over: 0.85/22 ): 100%|██████████| 2702/2702 [00:13<00:00, 207.47it/s]\n",
      "iter 34, train acc 0.798 train loss 7.64: 100%|██████████| 7923/7923 [00:40<00:00, 195.26it/s]\n",
      "iter 34, val   acc 0.676 ( over: 0.847/23 ): 100%|██████████| 2702/2702 [00:12<00:00, 215.62it/s]\n",
      "iter 35, train acc 0.803 train loss 7.38: 100%|██████████| 7923/7923 [00:40<00:00, 198.02it/s]\n",
      "iter 35, val   acc 0.680 ( over: 0.847/24 ): 100%|██████████| 2702/2702 [00:11<00:00, 227.91it/s]\n",
      "iter 36, train acc 0.801 train loss 7.26: 100%|██████████| 7923/7923 [00:41<00:00, 192.02it/s]\n",
      "iter 36, val  acc 0.682 ( over: 0.851/24 ): 100%|██████████| 2702/2702 [00:11<00:00, 231.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST val acc 0.698\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "n_iters = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(list(zip(Gs_train, fold.y_train)), batch_size=train_batch_size,\n",
    "                             shuffle=True, collate_fn=collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for i, (bg, label) in enumerate(data_loader):\n",
    "            outputs = model(bg)\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            # Train eval phase\n",
    "            total += label.size(0)\n",
    "            correct += (sampled_Y == label).sum().item()\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            del loss, outputs, bg, probs_Y, sampled_Y\n",
    "            pbar.update( len(label) )\n",
    "            pbar.set_description_str('iter {}, train acc {:.3f} train loss {:.2f}'.format(epoch, (correct/total), epoch_loss / (epoch + 1)))\n",
    "        \n",
    "        score_train = correct/total\n",
    "    with tqdm(total=len(data_loader_val.dataset), smoothing=0.) as pbar:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.\n",
    "        for bg, label in data_loader_val:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(bg)\n",
    "            \n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "\n",
    "            # Validation eval phase\n",
    "            total += label.size(0)\n",
    "            correct += (sampled_Y == label).sum().item()\n",
    "            \n",
    "            #break\n",
    "            \n",
    "            del probs_Y, outputs, bg, sampled_Y\n",
    "            pbar.update( label.size(0) )\n",
    "            score_val = correct/total\n",
    "\n",
    "            pbar.set_description_str('iter {}, val   acc {:.3f} ( over: {:.3} )'.format(epoch, score_val, score_val/score_train))\n",
    "            \n",
    "        #break\n",
    "        pbar.set_description_str('iter {}, val  acc {:.3f} ( over: {:.3}/{} )'.format(epoch, score_val, score_val/score_train, n_iters))\n",
    "        score = correct/total\n",
    "        if best_score is None or score > best_score:\n",
    "            torch.save(model, PATH)\n",
    "            best_score = score\n",
    "            n_iters = 0\n",
    "        else:\n",
    "            n_iters += 1\n",
    "            if n_iters >= patience:\n",
    "                print()\n",
    "                print('BEST val acc {:.3f}'.format(best_score), end='\\r')\n",
    "                break\n",
    "        pbar.set_description_str('iter {}, val   acc {:.3f} ( over: {:.3}/{} )'.format(epoch, score_val, score_val/score_train, n_iters))\n",
    "        epoch_loss /= (epoch + 1)\n",
    "        epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaky_relu\n",
    "\n",
    "#ReLU\n",
    "#BEST val acc 0.702 reut  w=5 | full_weight_prob\n",
    "#BEST val acc 0.625 acm   w=5 | full_weight_prob (max_feat=25k)\n",
    "\n",
    "#BEST val acc 0.732 webkb w=5 | full_weight\n",
    "#BEST val acc 0.700 reut  w=5 | full_weight\n",
    "#BEST val acc 0.754 20ng  w=5 | full_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST val acc 0.623: \n",
    "PATH = 'best_param_simple_acm.pth'\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "hidden_dim = 300\n",
    "train_batch_size = 16\n",
    "test_val_batch_size = 256\n",
    "loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-2, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.613 val loss 0.0 ( 3057/4983. over: 0.674 )\r"
     ]
    }
   ],
   "source": [
    "model = torch.load(PATH)\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0.\n",
    "    model.eval()\n",
    "    for bg, label in data_loader_val:\n",
    "        outputs = model(bg)\n",
    "        probs_Y = torch.softmax(outputs, 1)\n",
    "        sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "        #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "\n",
    "        qtd_docs = label.size(0)\n",
    "        qtd_correct = (sampled_Y == label).sum().item()\n",
    "        total += qtd_docs\n",
    "        correct += qtd_correct\n",
    "\n",
    "        del probs_Y, outputs, bg, sampled_Y\n",
    "        pbar.update( label.size(0) )\n",
    "        score_val = correct/total\n",
    "\n",
    "        print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(Gs_train), '+', len(Gs_val), end=' = ' )\n",
    "Gs_train_val = Gs_train + Gs_val\n",
    "print( len(Gs_train_val) )\n",
    "\n",
    "print( len(fold.y_train), '+', len(fold.y_val), end=' = ' )\n",
    "y_train_val = fold.y_train + fold.y_val\n",
    "print( len(y_train_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train_val_t  = DataLoader(list(zip(Gs_train_val, y_train_val)), batch_size=test_val_batch_size,\n",
    "                              shuffle=False, collate_fn=collate)\n",
    "X_train_val_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm_notebook(total=len(data_loader_train_val_t.dataset), smoothing=0.) as pbar:\n",
    "        for G, label in data_loader_train_val_t:\n",
    "            X_train_val_t = model.transform( G ).cpu().numpy()\n",
    "            X_train_val_all.append( X_train_val_t )\n",
    "            pbar.update( len(label) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_all2 = np.concatenate( X_train_val_all )\n",
    "X_train_val_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'loss': 'squared_hinge', 'C': 1, 'verbose': 0,\n",
    "         'intercept_scaling': 1, 'fit_intercept': True,\n",
    "         'max_iter': 1000, 'penalty': 'l2', 'multi_class': 'ovr',\n",
    "         'random_state': None, 'dual': False,'tol': 0.001,\n",
    "         'class_weight': None}\n",
    "estimator = LinearSVC(**param)\n",
    "tunning = [{'C': 2.0 ** np.arange(-5, 9, 2)}]\n",
    "\n",
    "gs = GridSearchCV(estimator, tunning,\n",
    "                n_jobs=64, refit=False,\n",
    "                cv=5, iid=True,\n",
    "                verbose=2, scoring='f1_micro')\n",
    "\n",
    "gs.fit( X_train_val_all2, y_train_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs_test  = graph_builder.transform(fold.X_test)\n",
    "data_loader_test = DataLoader(list(zip(Gs_test, fold.y_test)), batch_size=test_val_batch_size,\n",
    "                              shuffle=False, collate_fn=collate)\n",
    "X_test_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm_notebook(total=len(data_loader_test.dataset), smoothing=0.) as pbar:\n",
    "        for G, label in data_loader_test:\n",
    "            X_test_t = model.transform( G ).cpu().numpy()\n",
    "            X_test_all.append( X_test_t )\n",
    "            pbar.update( len(label) )\n",
    "X_test_all = np.concatenate( X_test_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm = LinearSVC( **gs.best_params_ )\n",
    "lsvm.fit( X_train_val_all2, y_train_val )\n",
    "\n",
    "y_pred = lsvm.predict( X_test_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred == fold.y_test)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH)\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0.\n",
    "    model.eval()\n",
    "    for bg, label in data_loader_test:\n",
    "        outputs = model(bg)\n",
    "        probs_Y = torch.softmax(outputs, 1)\n",
    "        sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "        #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "\n",
    "        qtd_docs = label.size(0)\n",
    "        qtd_correct = (sampled_Y == label).sum().item()\n",
    "        total += qtd_docs\n",
    "        correct += qtd_correct\n",
    "\n",
    "        del probs_Y, outputs, bg, sampled_Y\n",
    "        pbar.update( label.size(0) )\n",
    "        score_val = correct/total\n",
    "\n",
    "        print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
