{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import Dataset, GraphsizePretrained\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/reut/')\n",
    "fold = next(dataset.get_fold_instances(10))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:23, 17314.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 761 ms, total: 23.4 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10627/10627 [00:05<00:00, 1847.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.95 s, sys: 47.8 ms, total: 6.99 s\n",
      "Wall time: 6.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt',\n",
       "          stopwords='remove', verbose=None, w=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164318, 22962)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 239),\n",
       " (1, 77),\n",
       " (2, 454),\n",
       " (3, 2040),\n",
       " (4, 1129),\n",
       " (5, 626),\n",
       " (6, 2396),\n",
       " (7, 1077),\n",
       " (8, 1374),\n",
       " (9, 58),\n",
       " (10, 490),\n",
       " (11, 527),\n",
       " (12, 1469),\n",
       " (13, 1061),\n",
       " (14, 212),\n",
       " (15, 2121),\n",
       " (16, 5903),\n",
       " (17, 5511),\n",
       " (18, 3000),\n",
       " (19, 651),\n",
       " (20, 272),\n",
       " (21, 339),\n",
       " (22, 801),\n",
       " (23, 490),\n",
       " (24, 4236),\n",
       " (25, 616),\n",
       " (26, 2894),\n",
       " (27, 1716),\n",
       " (28, 762),\n",
       " (29, 1622),\n",
       " (30, 1784),\n",
       " (31, 6477),\n",
       " (32, 678),\n",
       " (33, 2005),\n",
       " (34, 402),\n",
       " (35, 108),\n",
       " (36, 399),\n",
       " (37, 621),\n",
       " (38, 113),\n",
       " (39, 10572),\n",
       " (40, 4550),\n",
       " (41, 9420),\n",
       " (42, 292),\n",
       " (43, 3248),\n",
       " (44, 2754),\n",
       " (45, 42),\n",
       " (46, 302),\n",
       " (47, 907),\n",
       " (48, 1138),\n",
       " (49, 494),\n",
       " (50, 170),\n",
       " (51, 610),\n",
       " (52, 5991),\n",
       " (53, 140),\n",
       " (54, 772),\n",
       " (55, 1598),\n",
       " (56, 498),\n",
       " (57, 1034),\n",
       " (58, 640),\n",
       " (59, 1353),\n",
       " (60, 1672),\n",
       " (61, 355),\n",
       " (62, 901),\n",
       " (63, 3315),\n",
       " (64, 513),\n",
       " (65, 372),\n",
       " (66, 379),\n",
       " (67, 2356),\n",
       " (68, 869),\n",
       " (69, 1307),\n",
       " (70, 460),\n",
       " (71, 3545),\n",
       " (72, 1663),\n",
       " (73, 1378),\n",
       " (74, 66),\n",
       " (75, 956),\n",
       " (76, 3284),\n",
       " (77, 1867),\n",
       " (78, 479),\n",
       " (79, 768),\n",
       " (80, 239),\n",
       " (81, 889),\n",
       " (82, 94),\n",
       " (83, 2373),\n",
       " (84, 1166),\n",
       " (85, 2113),\n",
       " (86, 132),\n",
       " (87, 2402),\n",
       " (88, 278),\n",
       " (89, 2070)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 n_heads=8, n_convs=2, drop=.5, first_hidden='emb', attn_drop=.5,\n",
    "                 encoders={'term','label'}, device='cuda:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        h = G.ndata[self.first_hidden].float()\n",
    "        for (k, mask) in kwargs.items():\n",
    "            if k in self.encoders:\n",
    "                if mask is not None:\n",
    "                    h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                else:\n",
    "                    h = self.encoders[k]( h )\n",
    "        \n",
    "        for l, conv in enumerate(self.layers):\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "            h = self.down_proj[l]( h )\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_class,\n",
    "                  n_heads=8, drop=.5, attn_drop=.5,\n",
    "                  device='cuda:0'):\n",
    "        super(TGA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.device = torch.device(device)\n",
    "        self.gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                                     encoders={'label'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        self.gat_local  = GenericGAT(hidden_dim, hidden_dim, \n",
    "                                     encoders={'term'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     first_hidden='emb',\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "\n",
    "        self.lin = nn.Linear( 2*hidden_dim, 1).to(self.device)\n",
    "        # Depois tentar alguma ativação (ReLU, por exemplo, pode \"desativar\" alguns termos no softmax)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear( 2*hidden_dim, self.n_class).to(self.device)\n",
    "        #self.fc2 = nn.Linear( hidden_dim, self.n_class).to(self.device)\n",
    "        #self.fc3 = nn.Linear( hidden_dim, self.n_class).to(self.device)\n",
    "    def forward(self, G, gs):\n",
    "        h_global           = self.gat_global( G, label=G.ndata['label'].nonzero().flatten() )\n",
    "        #gs.ndata['weight'] = h_global[ gs.ndata['idx'] ] # Tentar concatenando\n",
    "        h_local            = self.gat_local(gs, term=None)\n",
    "        h_local            = torch.cat((h_local, h_global[ gs.ndata['idx'] ]), 1)\n",
    "        h_local            = self.pooling( gs, h_local )\n",
    "        return self.fc1( h_local )\n",
    "# torch.Size([3652, 300]) torch.Size([3652, 300]) torch.Size([128, 300])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=300\n",
    "hidden_dim=300\n",
    "n_heads=8\n",
    "drop=0.3\n",
    "attn_drop=0.5\n",
    "batch_size=32\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat_global): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gat_local): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (term): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lin): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (pooling): GlobalAttentionPooling(\n",
       "    (gate_nn): Linear(in_features=600, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=600, out_features=90, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TGA( in_dim, hidden_dim, graph_builder.n_class,\n",
    "            n_heads=n_heads, drop=drop, attn_drop=attn_drop )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl = []\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl.append( g_dgl )\n",
    "        \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, dgl.batch(Gs_dgl), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3027ab02d63d404180fa9b28eef32b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10627.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=2)\n",
    "    with tqdm(total=len(fold.y_train)) as pbar:\n",
    "        total = 1\n",
    "        correct = 1\n",
    "        model.train()\n",
    "        for G, gs, y in data_loader:\n",
    "            G = G.to( torch.device('cuda:0') )\n",
    "            gs = gs.to( torch.device('cuda:0') )\n",
    "            y = y.to( torch.device('cuda:0') )\n",
    "            \n",
    "            #outputs = model( G, gs )\n",
    "            #probs_Y = torch.softmax(outputs, 1)\n",
    "            #sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            #total += y.size(0)\n",
    "            #correct += (sampled_Y == y).sum().item()\n",
    "            \n",
    "            # NN backprop phase\n",
    "            #loss = loss_func(outputs, y)\n",
    "            #optimizer.zero_grad()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            #epoch_loss += loss.detach().item()\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {epoch} Acc train: {correct/total:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
