{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset, Graphsize\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from time import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "webkb = Dataset('/home/mangaravite/Documents/datasets/topics/webkb/')\n",
    "_20ng = Dataset('/home/mangaravite/Documents/datasets/topics/20ng/')\n",
    "acm   = Dataset('/home/mangaravite/Documents/datasets/topics/acm/')\n",
    "reut  = Dataset('/home/mangaravite/Documents/datasets/topics/reut/')\n",
    "\n",
    "dataset = reut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = next(dataset.get_fold_instances(5))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7923/7923 [00:13<00:00, 590.32it/s]\n",
      "100%|██████████| 7923/7923 [00:04<00:00, 1607.15it/s]\n",
      "100%|██████████| 7923/7923 [01:15<00:00, 104.52it/s]\n",
      "100%|██████████| 7923/7923 [00:14<00:00, 542.23it/s]\n",
      "100%|██████████| 2702/2702 [00:01<00:00, 1782.50it/s]\n",
      "100%|██████████| 2702/2702 [00:25<00:00, 107.61it/s]\n",
      "100%|██████████| 2702/2702 [00:05<00:00, 508.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 19s, sys: 1.41 s, total: 2min 21s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_feats = 25000\n",
    "graph_builder = Graphsize(w=5, verbose=True, max_feat=max_feats, feature_type='full_weight_prob')\n",
    "Gs_train = graph_builder.fit_transform(fold.X_train)\n",
    "Gs_val   = graph_builder.transform(fold.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2702/2702 [00:01<00:00, 1722.08it/s]\n",
      "100%|██████████| 2702/2702 [00:26<00:00, 102.88it/s]\n",
      "100%|██████████| 2702/2702 [00:04<00:00, 569.50it/s]\n"
     ]
    }
   ],
   "source": [
    "Gs_test  = graph_builder.transform(fold.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifierGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=.5):\n",
    "        super(SimpleClassifierGCN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            #GraphConv(in_dim, hidden_dim, activation=F.leaky_relu).to(torch.device('cuda:0')),\n",
    "            #GraphConv(hidden_dim, hidden_dim, activation=F.leaky_relu).to(torch.device('cuda:0'))\n",
    "            GraphConv(in_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            #h = h.view(h.shape[0], -1)\n",
    "        G.ndata['h'] = h\n",
    "        w = self.lin( h )\n",
    "        G.ndata['w'] = w\n",
    "        hg = dgl.mean_nodes(G, 'h', weight='w')\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        #pred = torch.softmax(pred, 1)\n",
    "        return pred\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            #h = h.view(h.shape[0], -1)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        return hg\n",
    "class ClassifierGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=.5):\n",
    "        super(ClassifierGCN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(2*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(2*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((hg,he), 1)\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        return hg\n",
    "    \n",
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, n_heads=16, drop=.5):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(in_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=drop).to(torch.device('cuda:0')),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=drop).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(n_heads*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(n_heads*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        pred = self.classify( hg )\n",
    "        return pred\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        return hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor( [[1,2,3,4,5],[5,4,3,2,1]] )\n",
    "b = torch.tensor( [[-1,-2,-3,-4,-5],[-5,-4,-3,-2,-1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    Gs_Fs, labels = map(list, zip(*samples))\n",
    "    graphs = []\n",
    "    for g, f in Gs_Fs:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g)\n",
    "        g_dgl.ndata['f'] = torch.FloatTensor(f.A).to(torch.device('cuda:0'))\n",
    "        g_dgl.to(torch.device('cuda:0'))\n",
    "        graphs.append(g_dgl)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.to(torch.device('cuda:0'))\n",
    "    labels = torch.tensor(labels).to(torch.device('cuda:0'))\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # https://github.com/mbsariyildiz/focal-loss.pytorch\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'best_param_simple_reut.pth'\n",
    "n_epochs = 100\n",
    "patience = 25\n",
    "hidden_dim = 300\n",
    "train_batch_size = 16\n",
    "test_val_batch_size = 256\n",
    "\n",
    "#model = SimpleClassifierGCN(len(graph_builder.vocab), hidden_dim, dataset.nclass, drop=.5).to(torch.device('cuda:0'))\n",
    "#model = ClassifierGAT(len(graph_builder.vocab), hidden_dim, dataset.nclass, n_heads=2, drop=.5).to(torch.device('cuda:0'))\n",
    "model = ClassifierGCN(len(graph_builder.vocab), hidden_dim, dataset.nclass, drop=.3).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = FocalLoss(gamma=2).to(torch.device('cuda:0'))\n",
    "#loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "\n",
    "#optimizer = optim.Adam( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()\n",
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val  = DataLoader(list(zip(Gs_val,  fold.y_val )), batch_size=test_val_batch_size,\n",
    "                              shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter 0, train acc 0.581 train loss 365.28: 100%|██████████| 7923/7923 [00:37<00:00, 209.66it/s]\n",
      "iter 0, val   acc 0.602 ( over: 1.04 ):  66%|██████▋   | 1792/2702 [00:08<00:04, 206.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-42abb33f2af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mangaravite/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mangaravite/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-156-2aa56f58acbf>\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGs_Fs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mg_dgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDGLGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mg_dgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mg_dgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mg_dgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mangaravite/.local/lib/python3.7/site-packages/dgl/graph.py\u001b[0m in \u001b[0;36mfrom_networkx\u001b[0;34m(self, nx_graph, node_attrs, edge_attrs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# original edges (v, u).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnx_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m             \u001b[0mnx_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mangaravite/.local/lib/python3.7/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36mto_directed\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         G.add_edges_from((u, v, deepcopy(data))\n\u001b[0;32m-> 1569\u001b[0;31m                          \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m                          for v, data in nbrs.items())\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mangaravite/.local/lib/python3.7/site-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'WN2898'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mebunch_to_add\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mne\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mangaravite/.local/lib/python3.7/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1568\u001b[0m         G.add_edges_from((u, v, deepcopy(data))\n\u001b[1;32m   1569\u001b[0m                          \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m                          for v, data in nbrs.items())\n\u001b[0m\u001b[1;32m   1571\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "n_iters = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(list(zip(Gs_train, fold.y_train)), batch_size=train_batch_size,\n",
    "                             shuffle=True, collate_fn=collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for i, (bg, label) in enumerate(data_loader):\n",
    "            outputs = model(bg)\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            # Train eval phase\n",
    "            total += label.size(0)\n",
    "            correct += (sampled_Y == label).sum().item()\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            del loss, outputs, bg, probs_Y, sampled_Y\n",
    "            pbar.update( len(label) )\n",
    "            pbar.set_description_str('iter {}, train acc {:.3f} train loss {:.2f}'.format(epoch, (correct/total), epoch_loss / (epoch + 1)))\n",
    "        \n",
    "        score_train = correct/total\n",
    "    with tqdm(total=len(data_loader_val.dataset), smoothing=0.) as pbar:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.\n",
    "        for bg, label in data_loader_val:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(bg)\n",
    "            \n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "\n",
    "            # Validation eval phase\n",
    "            total += label.size(0)\n",
    "            correct += (sampled_Y == label).sum().item()\n",
    "            \n",
    "            #break\n",
    "            \n",
    "            del probs_Y, outputs, bg, sampled_Y\n",
    "            pbar.update( label.size(0) )\n",
    "            score_val = correct/total\n",
    "\n",
    "            pbar.set_description_str('iter {}, val   acc {:.3f} ( over: {:.3} )'.format(epoch, score_val, score_val/score_train))\n",
    "            \n",
    "        #break\n",
    "        pbar.set_description_str('iter {}, val  acc {:.3f} ( over: {:.3}/{} )'.format(epoch, score_val, score_val/score_train, n_iters))\n",
    "        score = correct/total\n",
    "        if best_score is None or score > best_score:\n",
    "            torch.save(model, PATH)\n",
    "            best_score = score\n",
    "            n_iters = 0\n",
    "        else:\n",
    "            n_iters += 1\n",
    "            if n_iters >= patience:\n",
    "                print()\n",
    "                print('BEST val acc {:.3f}'.format(best_score), end='\\r')\n",
    "                break\n",
    "        pbar.set_description_str('iter {}, val   acc {:.3f} ( over: {:.3}/{} )'.format(epoch, score_val, score_val/score_train, n_iters))\n",
    "        epoch_loss /= (epoch + 1)\n",
    "        epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaky_relu\n",
    "\n",
    "#ReLU\n",
    "#BEST val acc 0.702 reut  w=5 | full_weight_prob\n",
    "#BEST val acc 0.622 acm   w=5 | full_weight_prob\n",
    "\n",
    "#BEST val acc 0.732 webkb w=5 | full_weight\n",
    "#BEST val acc 0.700 reut  w=5 | full_weight\n",
    "#BEST val acc 0.754 20ng  w=5 | full_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST val acc 0.623: \n",
    "PATH = 'best_param_simple_acm.pth'\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "hidden_dim = 300\n",
    "train_batch_size = 16\n",
    "test_val_batch_size = 256\n",
    "loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-2, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.613 val loss 0.0 ( 3057/4983. over: 0.674 )\r"
     ]
    }
   ],
   "source": [
    "model = torch.load(PATH)\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0.\n",
    "    model.eval()\n",
    "    for bg, label in data_loader_val:\n",
    "        outputs = model(bg)\n",
    "        probs_Y = torch.softmax(outputs, 1)\n",
    "        sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "        #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "\n",
    "        qtd_docs = label.size(0)\n",
    "        qtd_correct = (sampled_Y == label).sum().item()\n",
    "        total += qtd_docs\n",
    "        correct += qtd_correct\n",
    "\n",
    "        del probs_Y, outputs, bg, sampled_Y\n",
    "        pbar.update( label.size(0) )\n",
    "        score_val = correct/total\n",
    "\n",
    "        print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(Gs_train), '+', len(Gs_val), end=' = ' )\n",
    "Gs_train_val = Gs_train + Gs_val\n",
    "print( len(Gs_train_val) )\n",
    "\n",
    "print( len(fold.y_train), '+', len(fold.y_val), end=' = ' )\n",
    "y_train_val = fold.y_train + fold.y_val\n",
    "print( len(y_train_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train_val_t  = DataLoader(list(zip(Gs_train_val, y_train_val)), batch_size=test_val_batch_size,\n",
    "                              shuffle=False, collate_fn=collate)\n",
    "X_train_val_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm_notebook(total=len(data_loader_train_val_t.dataset), smoothing=0.) as pbar:\n",
    "        for G, label in data_loader_train_val_t:\n",
    "            X_train_val_t = model.transform( G ).cpu().numpy()\n",
    "            X_train_val_all.append( X_train_val_t )\n",
    "            pbar.update( len(label) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_all2 = np.concatenate( X_train_val_all )\n",
    "X_train_val_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'loss': 'squared_hinge', 'C': 1, 'verbose': 0,\n",
    "         'intercept_scaling': 1, 'fit_intercept': True,\n",
    "         'max_iter': 1000, 'penalty': 'l2', 'multi_class': 'ovr',\n",
    "         'random_state': None, 'dual': False,'tol': 0.001,\n",
    "         'class_weight': None}\n",
    "estimator = LinearSVC(**param)\n",
    "tunning = [{'C': 2.0 ** np.arange(-5, 9, 2)}]\n",
    "\n",
    "gs = GridSearchCV(estimator, tunning,\n",
    "                n_jobs=64, refit=False,\n",
    "                cv=5, iid=True,\n",
    "                verbose=2, scoring='f1_micro')\n",
    "\n",
    "gs.fit( X_train_val_all2, y_train_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs_test  = graph_builder.transform(fold.X_test)\n",
    "data_loader_test = DataLoader(list(zip(Gs_test, fold.y_test)), batch_size=test_val_batch_size,\n",
    "                              shuffle=False, collate_fn=collate)\n",
    "X_test_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm_notebook(total=len(data_loader_test.dataset), smoothing=0.) as pbar:\n",
    "        for G, label in data_loader_test:\n",
    "            X_test_t = model.transform( G ).cpu().numpy()\n",
    "            X_test_all.append( X_test_t )\n",
    "            pbar.update( len(label) )\n",
    "X_test_all = np.concatenate( X_test_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm = LinearSVC( **gs.best_params_ )\n",
    "lsvm.fit( X_train_val_all2, y_train_val )\n",
    "\n",
    "y_pred = lsvm.predict( X_test_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred == fold.y_test)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH)\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0.\n",
    "    model.eval()\n",
    "    for bg, label in data_loader_test:\n",
    "        outputs = model(bg)\n",
    "        probs_Y = torch.softmax(outputs, 1)\n",
    "        sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "        #print(probs_Y.shape, sampled_Y.shape, label.shape)\n",
    "\n",
    "        qtd_docs = label.size(0)\n",
    "        qtd_correct = (sampled_Y == label).sum().item()\n",
    "        total += qtd_docs\n",
    "        correct += qtd_correct\n",
    "\n",
    "        del probs_Y, outputs, bg, sampled_Y\n",
    "        pbar.update( label.size(0) )\n",
    "        score_val = correct/total\n",
    "\n",
    "        print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    # https://github.com/mbsariyildiz/focal-loss.pytorch\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "\n",
    "class ClassifierGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=.5, device='cuda:0'):\n",
    "        super(ClassifierGCN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphConv(in_dim, hidden_dim, activation=F.leaky_relu).to(torch.device(device)),\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.leaky_relu).to(torch.device(device))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, 1).to(torch.device(device))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device(device))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(hidden_dim, n_classes).to(torch.device(device))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, h)\n",
    "        pred = self.classify( hg )\n",
    "        return pred\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "        hg = self.norm( h )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, h)\n",
    "        return hg\n",
    "\n",
    "PATH = 'best_param_simple.pth'\n",
    "class TGA(BaseEstimator, TransformerMixin, ClassifierMixin):\n",
    "    def __init__(self, hidden_dim=300,\n",
    "                 drop=.5, n_epochs=100, patience=10, hidden_dim,\n",
    "                 train_batch_size=16, transform_batch_size=256,\n",
    "                 lr=1e-2, weight_decay=1e-4,\n",
    "                 optim_func='adam', loss_func='focal', \n",
    "                 device='cuda:0', verbose=False):\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.drop=drop\n",
    "        self.n_epochs=n_epochs\n",
    "        self.patience=patience\n",
    "        self.train_batch_size=train_batch_size\n",
    "        self.transform_batch_size=transform_batch_size\n",
    "        self.lr=lr\n",
    "        self.weight_decay=weight_decay\n",
    "        self.device=device\n",
    "        \n",
    "        if loss_func.lower() == 'focal':\n",
    "            self.loss_func = FocalLoss().to(torch.device(device))  \n",
    "        elif loss_func.lower() == 'cross_entropy':\n",
    "            self.loss_func = nn.CrossEntropyLoss().to(torch.device(device))\n",
    "        \n",
    "        if optim_func.lower() == 'focal':\n",
    "            self.optimizer = optim.Adam( model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        elif optim_func.lower() == 'rmsprop':\n",
    "            self.optimizer = optim.RMSprop( model.parameters(), lr=self.lr, weight_decay=self.weight_decay )\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \n",
    "        in_dim= #Tamanho Vocab\n",
    "        self.nclass= #Quantidade de Classes\n",
    "        \n",
    "        model.train()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        self.model = ClassifierGCN(in_dim, self.hidden_dim, self.nclass, drop=self.drop).to(torch.device(self.device))\n",
    "        \n",
    "        best_score = None\n",
    "        n_iters = 0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            data_loader = DataLoader(list(zip(Gs_train, y_train)), batch_size=self.train_batch_size,\n",
    "                                     shuffle=True, collate_fn=collate)\n",
    "            epoch_loss = 0\n",
    "            with tqdm_notebook(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "                t0 = time()\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                model.train()\n",
    "                for i, (bg, label) in enumerate(data_loader):\n",
    "                    outputs = model(bg)\n",
    "                    probs_Y = torch.softmax(outputs, 1)\n",
    "                    sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "\n",
    "                    # Train eval phase\n",
    "                    total += label.size(0)\n",
    "                    correct += (sampled_Y == label).sum().item()\n",
    "\n",
    "                    # NN backprop phase\n",
    "                    loss = loss_func(outputs, label)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.detach().item()\n",
    "\n",
    "                    del loss, outputs, bg, probs_Y, sampled_Y\n",
    "                    pbar.update( len(label) )\n",
    "                    print('iter {}, train loss {:.2f} train acc: {:.3f}'.format(epoch, epoch_loss / (epoch + 1), (correct/total)), end='\\r')\n",
    "\n",
    "                score_train = correct/total\n",
    "            with tqdm_notebook(total=len(data_loader_val.dataset), smoothing=0.) as pbar:\n",
    "                model.eval()\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                epoch_loss = 0.\n",
    "                for bg, label in data_loader_val:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(bg)\n",
    "\n",
    "                    probs_Y = torch.softmax(outputs, 1)\n",
    "                    sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "\n",
    "                    # Validation eval phase\n",
    "                    total += label.size(0)\n",
    "                    correct += (sampled_Y == label).sum().item()\n",
    "\n",
    "                    #break\n",
    "\n",
    "                    del probs_Y, outputs, bg, sampled_Y\n",
    "                    pbar.update( label.size(0) )\n",
    "                    score_val = correct/total\n",
    "\n",
    "                    print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3} )'.format(score_val, epoch_loss, correct, total, score_val/score_train), end='\\r')\n",
    "\n",
    "                #break\n",
    "                print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3}/{} )'.format(score_val, epoch_loss, correct, total, score_val/score_train, n_iters), end='\\r')\n",
    "                score = correct/total\n",
    "                if best_score is None or score > best_score:\n",
    "                    torch.save(model, PATH)\n",
    "                    best_score = score\n",
    "                    n_iters = 0\n",
    "                else:\n",
    "                    n_iters += 1\n",
    "                    if n_iters >= patience:\n",
    "                        print()\n",
    "                        print('BEST val acc {:.3f}'.format(best_score), end='\\r')\n",
    "                        break\n",
    "                print('val acc {:.3f} val loss {:.3} ( {}/{}. over: {:.3}/{} )'.format(score_val, epoch_loss, correct, total, score_val/score_train, n_iters), end='\\r')\n",
    "                epoch_loss /= (epoch + 1)\n",
    "                epoch_losses.append(epoch_loss)\n",
    "            print()\n",
    "        \n",
    "        \n",
    "        epoch_losses = []\n",
    "    def transform(self, X):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
