{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import Pool\n",
    "from collections import namedtuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "import networkx as nx\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords as stopwords_by_lang\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.utils import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val'), 15062)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/20ng/')\n",
    "g = dataset.get_fold_instances(10, with_val=True)\n",
    "fold = next(g)\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "\n",
    "def ig(X, y):\n",
    "\n",
    "    def get_t1(fc, c, f):\n",
    "        t = np.log2(fc/(c * f))\n",
    "        t[~np.isfinite(t)] = 0\n",
    "        return np.multiply(fc, t)\n",
    "\n",
    "    def get_t2(fc, c, f):\n",
    "        t = np.log2((1-f-c+fc)/((1-c)*(1-f)))\n",
    "        t[~np.isfinite(t)] = 0\n",
    "        return np.multiply((1-f-c+fc), t)\n",
    "\n",
    "    def get_t3(c, f, class_count, observed, total):\n",
    "        nfc = (class_count - observed)/total\n",
    "        t = np.log2(nfc/(c*(1-f)))\n",
    "        t[~np.isfinite(t)] = 0\n",
    "        return np.multiply(nfc, t)\n",
    "\n",
    "    def get_t4(c, f, feature_count, observed, total):\n",
    "        fnc = (feature_count - observed)/total\n",
    "        t = np.log2(fnc/((1-c)*f))\n",
    "        t[~np.isfinite(t)] = 0\n",
    "        return np.multiply(fnc, t)\n",
    "\n",
    "    X = check_array(X, accept_sparse='csr')\n",
    "    if np.any((X.data if issparse(X) else X) < 0):\n",
    "        raise ValueError(\"Input X must be non-negative.\")\n",
    "\n",
    "    Y = LabelBinarizer().fit_transform(y)\n",
    "    if Y.shape[1] == 1:\n",
    "        Y = np.append(1 - Y, Y, axis=1)\n",
    "\n",
    "    # counts\n",
    "\n",
    "    observed = safe_sparse_dot(Y.T, X)          # n_classes * n_features\n",
    "    total = observed.sum(axis=0).reshape(1, -1).sum()\n",
    "    feature_count = X.sum(axis=0).reshape(1, -1)\n",
    "    class_count = (X.sum(axis=1).reshape(1, -1) * Y).T\n",
    "\n",
    "    # probs\n",
    "\n",
    "    f = feature_count / feature_count.sum()\n",
    "    c = class_count / float(class_count.sum())\n",
    "    fc = observed / total\n",
    "\n",
    "    # the feature score is averaged over classes\n",
    "    scores = (get_t1(fc, c, f) +\n",
    "            get_t2(fc, c, f) +\n",
    "            get_t3(c, f, class_count, observed, total) +\n",
    "            get_t4(c, f, feature_count, observed, total)).mean(axis=0)\n",
    "\n",
    "    scores = np.asarray(scores).reshape(-1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "replace_patterns = [\n",
    "    ('<[^>]*>', ''),                                    # remove HTML tags\n",
    "    ('(\\D)\\d\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D\\D)\\d\\d\\d\\D\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedZipcodePlusFour \\\\2'),\n",
    "    ('(\\D)\\d(\\D)', '\\\\1ParsedOneDigit\\\\2'),\n",
    "    ('(\\D)\\d\\d(\\D)', '\\\\1ParsedTwoDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d(\\D)', '\\\\1ParsedThreeDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d(\\D)', '\\\\1ParsedFourDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedFiveDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedSixDigits\\\\2'),\n",
    "    ('\\d+', 'ParsedDigits')\n",
    "]\n",
    "\n",
    "compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "\n",
    "def generate_preprocessor(replace_patterns):\n",
    "    compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "    def preprocessor(text):\n",
    "        for pattern, replace in compiled_replace_patterns:\n",
    "            text = re.sub(pattern, replace, text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    return preprocessor\n",
    "\n",
    "generated_patters=generate_preprocessor(replace_patterns)\n",
    "\n",
    "def preprocessor(text):\n",
    "    # For each pattern, replace it with the appropriate string\n",
    "    for pattern, replace in compiled_replace_patterns:\n",
    "        text = re.sub(pattern, replace, text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mindf=2, lan='english', stopwordsSet='nltk', model='topk', k=500,\n",
    "                 vocab_max_size=99999999999, verbose=False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "        self.mindf = mindf\n",
    "        self.le = LabelEncoder()\n",
    "        self.verbose = verbose\n",
    "        self.lan = lan\n",
    "        if stopwordsSet == 'nltk':\n",
    "            self.stopwordsSet = stopwords_by_lang.words(lan)\n",
    "        elif stopwordsSet == 'scikit':\n",
    "            self.stopwordsSet = stop_words\n",
    "        else:\n",
    "            self.stopwordsSet = []\n",
    "        self.model =  model\n",
    "        self.k     = k\n",
    "        self.analyzer = TfidfVectorizer(ngram_range=(1,2),stop_words=self.stopwordsSet,\n",
    "                                        max_features=vocab_max_size,\n",
    "                                        preprocessor=preprocessor, min_df=mindf)#.build_analyzer()\n",
    "        self.local_analyzer = self.analyzer.build_analyzer()\n",
    "        self.analyzer.set_params( analyzer=self.local_analyzer )\n",
    "        self.node_mapper      = {}\n",
    "        self.vocab_max_size   = vocab_max_size\n",
    "        \n",
    "    def analyzer_doc(self, doc):\n",
    "        return self.local_analyzer(doc)\n",
    "    def fit(self, X, y):\n",
    "        self.N           = len(X)\n",
    "        y                = self.le.fit_transform( y )\n",
    "        self.n_class     = len(self.le.classes_)\n",
    "        docs_in_tids     = []\n",
    "        self.node_mapper = {}\n",
    "        \n",
    "        with Pool(processes=64) as p:\n",
    "            for docid,doc_in_terms in tqdm(enumerate(p.imap(self.analyzer_doc, X)),\n",
    "                                           total=self.N, disable=not self.verbose):\n",
    "                doc_in_terms = list(set(map( self._filter_fit_, list(doc_in_terms) ))) \n",
    "                docs_in_tids.extend( [ (1,docid,term)\n",
    "                                      for term in doc_in_terms ] )\n",
    "        print(\"Build Matrix\")\n",
    "        data, row_ind, docs_in_terms = list(zip(*docs_in_tids))\n",
    "        self.term_freqs              = Counter(docs_in_terms)\n",
    "        self.term_freqs              = { term: freq for term,freq in self.term_freqs.items() if freq >= self.mindf  }\n",
    "        docs_in_tids                 = [ (d,docid,term) for (d,docid,term) in docs_in_tids if term in self.term_freqs ]\n",
    "        data, row_ind, docs_in_terms = list(zip(*docs_in_tids))\n",
    "        \n",
    "        \n",
    "        col_ind                      = [ self.node_mapper.setdefault(term, len(self.node_mapper))\n",
    "                                        for term in docs_in_terms ]\n",
    "        \n",
    "        X_vec = csr_matrix((data, (row_ind, col_ind)), shape=(self.N, len(self.term_freqs)))\n",
    "        \n",
    "        self.term_array = [ term for (term,term_id) in sorted(self.node_mapper.items(), key=lambda x: x[1]) ]\n",
    "        \n",
    "        print(\"Selecting terms\")\n",
    "        res = list(zip(self.term_array, ig(X_vec, y)))\n",
    "        res = sorted(res, key=lambda x: x[1], reverse=True)[:self.vocab_max_size]\n",
    "        \n",
    "        print(\"Building vocab\")\n",
    "        self.term_freqs  = { term:self.term_freqs[term] for term, weight in res }\n",
    "        self.term_freqs['<BLANK>']  = self.N\n",
    "        self.term_freqs['<UNK>']    = self.N\n",
    "        \n",
    "        self.node_mapper = { term: termid for (termid, (term, weight)) in zip(range(len(res)), res) }\n",
    "        self.node_mapper['<BLANK>'] = len(self.node_mapper)\n",
    "        self.node_mapper['<UNK>']   = len(self.node_mapper)\n",
    "        self.term_array = [ term for (term,term_id) in sorted(self.node_mapper.items(), key=lambda x: x[1]) ]\n",
    "        \n",
    "        \n",
    "        self.fi_ = [ weight for term, weight in res ]\n",
    "        self.fi_.append(0.)\n",
    "        self.fi_.append(0.)\n",
    "        self.fi_ = np.array(self.fi_)\n",
    "        \n",
    "        self.vocab_size = len(self.node_mapper)\n",
    "        return self\n",
    "    def _filter_transform_(self, term):\n",
    "        if term in self.stopwordsSet:\n",
    "            return '<STPW>'\n",
    "        if term not in self.node_mapper:\n",
    "            return '<UNK>'\n",
    "        return term\n",
    "    def _filter_fit_(self, term):\n",
    "        if term in self.stopwordsSet:\n",
    "            return '<STPW>'\n",
    "        return term\n",
    "    def _model_(self, doc):\n",
    "        doc_counter = Counter(doc)\n",
    "        doc = np.array(list(doc_counter.keys()))\n",
    "        if len(doc) > self.k:\n",
    "            weigths = np.array([ self.fi_[t] for t in doc ])\n",
    "            weigths = softmax(weigths)\n",
    "            if self.model == 'topk':\n",
    "                doc = doc[(-weigths).argsort()[:self.k]]\n",
    "            elif self.model == 'sample':\n",
    "                doc = np.random.choice(doc, size=self.k, replace=False, p=weigths)\n",
    "        TFs = np.array([ doc_counter[tid] for tid in doc ])\n",
    "        DFs = np.array([ self.term_freqs[self.term_array[tid]] for tid in doc ])\n",
    "        return doc, TFs, DFs\n",
    "    def transform(self, X, verbose=None):\n",
    "        verbose = verbose if verbose is not None else self.verbose\n",
    "        n = len(X)\n",
    "        terms_ = []\n",
    "        for i,doc_in_terms in tqdm(enumerate(map(self.analyzer_doc, X)), total=n, disable=not verbose):\n",
    "            doc_in_terms = map( self._filter_transform_, doc_in_terms )\n",
    "            #doc_in_terms = filter( lambda x: x != '<STPW>', doc_in_terms )\n",
    "            doc_tids = [ self.node_mapper[tid] for tid in doc_in_terms ]\n",
    "            doc_tids, TFs, DFs = self._model_(doc_tids)\n",
    "            terms_.append( (doc_tids, TFs, DFs) )\n",
    "        doc_tids, TFs, DFs = list(zip(*terms_))\n",
    "        return list(doc_tids), list(TFs), list(DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e16d26d71024a2982f6446609f6fe95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Matrix\n",
      "Selecting terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mangaravite/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log2\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/mangaravite/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(stopwordsSet=[], verbose=True, vocab_max_size=300000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(mindf=2, vocab_max_size=300000, verbose=True, stopwordsSet=None)\n",
    "tokenizer.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tokenizer.le.transform( fold.y_train )\n",
    "y_val   = tokenizer.le.transform( fold.y_val )\n",
    "y_test  = tokenizer.le.transform( fold.y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.clamp(a_n, min=eps)\n",
    "    b_norm = b / torch.clamp(b_n, min=eps)\n",
    "    sim_mt = torch.bmm(a_norm, b_norm.transpose(1, 2))\n",
    "    return torch.cos(sim_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTFIDF_V1(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, maxF=20, drop=.5,\n",
    "                 initrange=.5, negative_slope=99.):\n",
    "        super(AttentionTFIDF_V1, self).__init__()\n",
    "        self.hiddens        = hiddens\n",
    "        self.maxF           = maxF\n",
    "        self.value_emb      = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.query_emb      = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.key_emb        = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.TF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.DF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.fc             = nn.Linear(hiddens, nclass)\n",
    "        self.initrange      = initrange \n",
    "        self.negative_slope = negative_slope\n",
    "        self.drop_          = drop\n",
    "        self.init_weights()\n",
    "    def forward(self, doc_tids, TFs, DFs):\n",
    "        batch_size = doc_tids.size(0)\n",
    "        bx_packed  = doc_tids == 0\n",
    "        pad_mask   = bx_packed.logical_not()\n",
    "        doc_sizes  = pad_mask.sum(dim=1).view(batch_size, 1)\n",
    "        pad_mask   = pad_mask.view(*bx_packed.shape, 1)\n",
    "        pad_mask   = pad_mask.logical_and(pad_mask.transpose(1, 2))\n",
    "        \n",
    "        TFs     = torch.clamp( TFs, max=self.maxF-1 )\n",
    "        h_TFs   = self.TF_emb( TFs )\n",
    "        h_TFs   = F.dropout( h_TFs, p=self.drop_, training=self.training )\n",
    "        \n",
    "        DFs     = torch.clamp( DFs, max=self.maxF-1 )\n",
    "        h_DFs   = self.DF_emb( DFs )\n",
    "        h_DFs   = F.dropout( h_DFs, p=self.drop_, training=self.training )\n",
    "        \n",
    "        h_query = self.query_emb( doc_tids )\n",
    "        h_query = h_query + h_TFs + h_DFs\n",
    "        h_query = torch.tanh( h_query )\n",
    "        h_query = F.dropout( h_query, p=self.drop_, training=self.training )\n",
    "        \n",
    "        h_key = self.key_emb( doc_tids )\n",
    "        h_key = h_key + h_TFs + h_DFs\n",
    "        h_key = torch.tanh( h_key )\n",
    "        h_key = F.dropout( h_key, p=self.drop_, training=self.training )\n",
    "        \n",
    "        #co_weights  = torch.bmm( h_key, h_query.transpose( 1, 2 ) )\n",
    "        co_weights  = sim_matrix( h_key, h_query )\n",
    "        #co_weights = torch.tanh( co_weights )\n",
    "        #co_weights  = co_weights / torch.pow(1.+co_weights, 2.)\n",
    "        \n",
    "        co_weights[pad_mask.logical_not()] = 0. # Set the 3D-pad mask values to\n",
    "        #co_weights = torch.tanh(co_weights)\n",
    "        co_weights = torch.relu(co_weights)\n",
    "        \n",
    "        #co_weights  = F.leaky_relu( co_weights, negative_slope=self.negative_slope)\n",
    "        #co_weights[pad_mask.logical_not()] = float('-inf') # Set the 3D-pad mask values to -inf (=0 in sigmoid)\n",
    "        #co_weights = torch.sigmoid(co_weights)\n",
    "        \n",
    "        weights = co_weights.sum(axis=2) / doc_sizes\n",
    "        weights[bx_packed] = float('-inf') # Set the 2D-pad mask values to -inf  (=0 in softmax)\n",
    "        \n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        weights = torch.where(torch.isnan(weights), torch.zeros_like(weights), weights)\n",
    "        weights = weights.view( *weights.shape, 1 )\n",
    "        \n",
    "        h_value = self.value_emb( doc_tids )\n",
    "        h_value = h_value + h_TFs + h_DFs\n",
    "        h_value = F.dropout( h_value, p=self.drop_, training=self.training )\n",
    "        \n",
    "        docs_h = h_value * weights\n",
    "        docs_h = docs_h.sum(axis=1)\n",
    "        docs_h = F.dropout( docs_h, p=self.drop_, training=self.training )\n",
    "        docs_h = self.fc(docs_h)\n",
    "        return docs_h, weights, co_weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        self.TF_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.DF_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.query_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.key_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.value_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.fc.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \"\"\"\n",
    "        \n",
    "        nn.init.xavier_normal_(self.TF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.DF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.query_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.key_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.value_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformTFIDF(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, maxF=20, nhead=2, drop=.5,\n",
    "                 initrange=.5, negative_slope=99.):\n",
    "        super(TransformTFIDF, self).__init__()\n",
    "        \n",
    "        self.hiddens        = hiddens\n",
    "        self.maxF           = maxF\n",
    "        self.vocab_size     = vocab_size\n",
    "        \n",
    "        self.TF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.DF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.term_emb       = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.trns_terms     = nn.Transformer(d_model = hiddens, nhead=nhead)\n",
    "        \n",
    "        self.fc             = nn.Linear(hiddens, nclass)\n",
    "        \n",
    "        self.drop_          = drop\n",
    "        \n",
    "        self.init_weights()\n",
    "    def forward(self, doc_tids, TFs, DFs):\n",
    "        \n",
    "        doc_tids = torch.clamp( doc_tids, max=self.vocab_size-1 )\n",
    "        terms_h  = self.term_emb(doc_tids)\n",
    "        \n",
    "        TFs      = torch.clamp( TFs, max=self.maxF-1 )\n",
    "        TFs_h    = self.TF_emb( TFs )\n",
    "        \n",
    "        DFs      = torch.clamp( DFs, max=self.maxF-1 )\n",
    "        DFs_h    = self.DF_emb(DFs)\n",
    "        \n",
    "        hidden_terms = terms_h + TFs_h + DFs_h\n",
    "        hidden_terms   = self.trns_terms(hidden_terms, hidden_terms)\n",
    "        term_imp = self.fc( hidden_terms )\n",
    "        term_imp = F.relu( term_imp )\n",
    "        docs_h   = term_imp.sum(axis=1)\n",
    "        \n",
    "        return docs_h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        pass\n",
    "        \"\"\"\n",
    "        self.TF_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.DF_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.query_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.key_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.value_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.fc.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.TF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.DF_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.query_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.key_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.value_emb.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 1000\n",
    "#max_epochs = 30\n",
    "max_epochs = 10\n",
    "drop=0.8\n",
    "max_drop=.5 # default .85\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = 64\n",
    "k = 512 # default 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train(param):\n",
    "    X, y = zip(*param)\n",
    "    doc_tids, TFs, DFs = tokenizer.transform(X, verbose=False)\n",
    "    doc_tids = pad_sequence(list(map(torch.LongTensor, doc_tids)), batch_first=True, padding_value=0)\n",
    "    \n",
    "    TFs = pad_sequence(list(map(torch.tensor, TFs)), batch_first=True, padding_value=0)\n",
    "    #TFs = torch.LongTensor(TFs)\n",
    "    TFs = torch.LongTensor(torch.log2(TFs+1).round().long())\n",
    "    \n",
    "    DFs = pad_sequence(list(map(torch.tensor, DFs)), batch_first=True, padding_value=0)\n",
    "    DFs = torch.LongTensor(torch.log2(DFs+1).round().long())\n",
    "    \n",
    "    return doc_tids, TFs, DFs, torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f012d689d38>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SimpleClassifier(tokenizer.vocab_size, 300, tokenizer.n_class, dropout=drop).to( device )\n",
    "#ab = AttentionTFIDF_V1(tokenizer.vocab_size, 300, tokenizer.n_class,\n",
    "#                       initrange=0.3, drop=drop).to( device )\n",
    "ab = TransformTFIDF(tokenizer.vocab_size, 300, tokenizer.n_class).to( device )\n",
    "#ab = AttentionBag(tokenizer.vocab_size, 300, tokenizer.n_class, drop=drop).to( device )\n",
    "#ab = NotTooSimpleClassifier(tokenizer.vocab_size, 300, tokenizer.n_class, dropout1=drop, dropout2=drop).to( device )\n",
    "tokenizer.k = k\n",
    "optimizer = optim.AdamW( ab.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "loss_func_cel = nn.CrossEntropyLoss().to( device )\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.95,\n",
    "#                                                       patience=10, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.95,\n",
    "                                                       patience=3, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=.98, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mangaravite/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb71beed6de24f6c953cf76c0e946866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9a22ff76f4486fa2d59f9bc1367425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/16954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'trns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6f830e9c973e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mpred_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mab\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdoc_tids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDFs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mpred_docs\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mloss_func_cel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9e0b9d4026b2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, doc_tids, TFs, DFs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mhidden_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterms_h\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTFs_h\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDFs_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mhidden_terms\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtrns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mterm_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhidden_terms\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mterm_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mterm_imp\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trns' is not defined"
     ]
    }
   ],
   "source": [
    "best = 99999.\n",
    "counter = 1\n",
    "loss_val = 1.\n",
    "eps = .9\n",
    "dl_val = DataLoader(list(zip(fold.X_val, y_val)), batch_size=batch_size,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=num_workers)\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    dl_train = DataLoader(list(zip(fold.X_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_train, num_workers=num_workers)\n",
    "    loss_train  = 0.\n",
    "    with tqdm(total=len(y_train)+len(y_val), smoothing=0., desc=f\"Epoch {e+1}\") as pbar:\n",
    "        total = 0\n",
    "        correct  = 0\n",
    "        ab.train()\n",
    "        tokenizer.model = 'sample'\n",
    "        tokenizer.k = k\n",
    "        for i, (doc_tids, TFs, DFs, y) in enumerate(dl_train):\n",
    "            doc_tids = doc_tids.to( device )\n",
    "            TFs      = TFs.to( device )\n",
    "            DFs      = DFs.to( device )\n",
    "            y        = y.to( device )\n",
    "            \n",
    "            pred_docs,_,_ = ab( doc_tids, TFs, DFs )\n",
    "            pred_docs     = torch.softmax(pred_docs, dim=1)\n",
    "            loss          = loss_func_cel(pred_docs, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            total      += len(y)\n",
    "            y_pred      = pred_docs.argmax(axis=1)\n",
    "            correct    += (y_pred == y).sum().item()\n",
    "            #ab.drop_ =  np.power((correct/total),loss_val)\n",
    "            #ab.drop_ =  np.power((correct/total),4)\n",
    "            ab.drop_ =  (correct/total)*max_drop\n",
    "            \n",
    "            toprint  = f\"Train loss: {loss_train/(i+1):.5}/{loss.item():.5} \"\n",
    "            toprint += f'Drop: {ab.drop_:.5} '\n",
    "            toprint += f'ACC: {correct/total:.5} '\n",
    "            \n",
    "            print(toprint, end=f\"{' '*100}\\r\")\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            del doc_tids, TFs\n",
    "            del DFs, y, pred_docs\n",
    "            del loss, y_pred\n",
    "        loss_train = loss_train/(i+1)\n",
    "        print()\n",
    "        #print(ab.drop_)\n",
    "        total = 0\n",
    "        correct  = 0\n",
    "        ab.eval()\n",
    "        tokenizer.model = 'topk'\n",
    "        tokenizer.k = 512\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.\n",
    "            for i, (doc_tids, TFs, DFs, y) in enumerate(dl_val):\n",
    "                doc_tids = doc_tids.to( device )\n",
    "                TFs      = TFs.to( device )\n",
    "                DFs      = DFs.to( device )\n",
    "                y        = y.to( device )\n",
    "\n",
    "                pred_docs,_,_ = ab( doc_tids, TFs, DFs )\n",
    "                pred_docs     = torch.softmax(pred_docs, dim=1)\n",
    "                loss          = loss_func_cel(pred_docs, y)\n",
    "\n",
    "                loss_val   += loss.item()\n",
    "                total      += len(y)\n",
    "                y_pred      = pred_docs.argmax(axis=1)\n",
    "                correct    += (y_pred == y).sum().item()\n",
    "                \n",
    "                print(f'Val loss: {loss_val/(i+1):.5} ACC: {correct/total:.5}', end=f\"{' '*100}\\r\")\n",
    "                pbar.update( len(y) )\n",
    "                \n",
    "                del doc_tids, TFs, DFs, y\n",
    "                del pred_docs, loss\n",
    "            print()\n",
    "            loss_val   = (loss_val/(i+1))\n",
    "            scheduler.step(loss_val)\n",
    "\n",
    "            if best-loss_val > 0.0001 :\n",
    "                best = loss_val\n",
    "                counter = 1\n",
    "                print(f'New Best Val loss: {best:.5}', end=f\"{' '*100}\\n\")\n",
    "                best_model = copy.deepcopy(ab).to('cpu')\n",
    "            elif counter > max_epochs:\n",
    "                print(f'Best Val loss: {best:.5}', end=f\"{' '*100}\\n\")\n",
    "                break\n",
    "            else:\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método:\n",
    "#    -> Amostragem dos termos (1,2-gramas)\n",
    "#    -> Dropout Forte e Dinâmico\n",
    "#    -> Embedding de Frequência e Raridade dos termos\n",
    "# Aproveitamento das Disciplinas do Mestrado\n",
    "# Projeto de Tese (Prorrogação?)\n",
    "# Resultados / Experimentação / Escrita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_test = 'cpu'\n",
    "print(\"Copying\")\n",
    "ab = copy.deepcopy(best_model).to(device_test)\n",
    "print(\"Eval\")\n",
    "ab.eval()\n",
    "print(\"DataLoader\")\n",
    "loss_total = 0\n",
    "correct_t = 0\n",
    "total_t = 0\n",
    "dl_test = DataLoader(list(zip(fold.X_test, y_test)), batch_size=16,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=12)\n",
    "tokenizer.k = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting\")\n",
    "for i, (docs_tids_t, TFs_t, DFs_t, y_t) in enumerate(dl_test):\n",
    "    docs_tids_t = docs_tids_t.to( device_test )\n",
    "    TFs_t       = TFs_t.to( device_test )\n",
    "    DFs_t       = DFs_t.to( device_test )\n",
    "    y_t         = y_t.to( device_test )\n",
    "\n",
    "    pred_docs_t,weigths,coweights = ab( docs_tids_t, TFs_t, DFs_t )\n",
    "    sofmax_docs_t = torch.softmax(pred_docs_t, dim=1)\n",
    "\n",
    "    y_pred_t    = sofmax_docs_t.argmax(axis=1)\n",
    "    correct_t  += (y_pred_t == y_t).sum().item()\n",
    "    total_t    += len(y_t)\n",
    "    loss_total += loss_func_cel(sofmax_docs_t, y_t)\n",
    "\n",
    "    print(f'Test loss: {loss_total.item()/(i+1):.5} ACC: {correct_t/total_t:.5}', end=f\"{' '*100}\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
