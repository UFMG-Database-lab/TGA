{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import Dataset, GraphsizePretrained\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "webkb = Dataset('/home/Documentos/datasets/classification/datasets/acm/')\n",
    "\n",
    "dataset = webkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = next(dataset.get_fold_instances(10))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:25, 15708.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 680 ms, total: 25.7 s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt')\n",
    "#Gs_train = graph_builder.fit_transform(fold.X_train)\n",
    "#Gs_val   = graph_builder.transform(fold.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10627/10627 [00:06<00:00, 1706.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt',\n",
       "          stopwords='remove', verbose=None, w=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164318, 22962)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 239),\n",
       " (1, 77),\n",
       " (2, 454),\n",
       " (3, 2040),\n",
       " (4, 1129),\n",
       " (5, 626),\n",
       " (6, 2396),\n",
       " (7, 1077),\n",
       " (8, 1374),\n",
       " (9, 58),\n",
       " (10, 490),\n",
       " (11, 527),\n",
       " (12, 1469),\n",
       " (13, 1061),\n",
       " (14, 212),\n",
       " (15, 2121),\n",
       " (16, 5903),\n",
       " (17, 5511),\n",
       " (18, 3000),\n",
       " (19, 651),\n",
       " (20, 272),\n",
       " (21, 339),\n",
       " (22, 801),\n",
       " (23, 490),\n",
       " (24, 4236),\n",
       " (25, 616),\n",
       " (26, 2894),\n",
       " (27, 1716),\n",
       " (28, 762),\n",
       " (29, 1622),\n",
       " (30, 1784),\n",
       " (31, 6477),\n",
       " (32, 678),\n",
       " (33, 2005),\n",
       " (34, 402),\n",
       " (35, 108),\n",
       " (36, 399),\n",
       " (37, 621),\n",
       " (38, 113),\n",
       " (39, 10572),\n",
       " (40, 4550),\n",
       " (41, 9420),\n",
       " (42, 292),\n",
       " (43, 3248),\n",
       " (44, 2754),\n",
       " (45, 42),\n",
       " (46, 302),\n",
       " (47, 907),\n",
       " (48, 1138),\n",
       " (49, 494),\n",
       " (50, 170),\n",
       " (51, 610),\n",
       " (52, 5991),\n",
       " (53, 140),\n",
       " (54, 772),\n",
       " (55, 1598),\n",
       " (56, 498),\n",
       " (57, 1034),\n",
       " (58, 640),\n",
       " (59, 1353),\n",
       " (60, 1672),\n",
       " (61, 355),\n",
       " (62, 901),\n",
       " (63, 3315),\n",
       " (64, 513),\n",
       " (65, 372),\n",
       " (66, 379),\n",
       " (67, 2356),\n",
       " (68, 869),\n",
       " (69, 1307),\n",
       " (70, 460),\n",
       " (71, 3545),\n",
       " (72, 1663),\n",
       " (73, 1378),\n",
       " (74, 66),\n",
       " (75, 956),\n",
       " (76, 3284),\n",
       " (77, 1867),\n",
       " (78, 479),\n",
       " (79, 768),\n",
       " (80, 239),\n",
       " (81, 889),\n",
       " (82, 94),\n",
       " (83, 2373),\n",
       " (84, 1166),\n",
       " (85, 2113),\n",
       " (86, 132),\n",
       " (87, 2402),\n",
       " (88, 278),\n",
       " (89, 2070)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 n_heads=8, n_convs=2, drop=.5, first_hidden='emb', attn_drop=.5,\n",
    "                 encoders={'term','label'}, device='cuda:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        h = G.ndata[self.first_hidden].float()\n",
    "        for (k, mask) in kwargs.items():\n",
    "            if k in self.encoders:\n",
    "                if mask is not None:\n",
    "                    h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                else:\n",
    "                    h = self.encoders[k]( h )\n",
    "        \n",
    "        for l, conv in enumerate(self.layers):\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "            h = self.down_proj[l]( h )\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_class,\n",
    "                  n_heads=8, drop=.5, attn_drop=.5,\n",
    "                  device='cuda:0'):\n",
    "        super(TGA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.device = torch.device(device)\n",
    "        self.gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                                     encoders={'label'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        self.gat_local  = GenericGAT(hidden_dim, hidden_dim, \n",
    "                                     encoders={'term'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     first_hidden='emb',\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "\n",
    "        self.lin = nn.Linear( 2*hidden_dim, 1).to(self.device)\n",
    "        # Depois tentar alguma ativação (ReLU, por exemplo, pode \"desativar\" alguns termos no softmax)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear( 2*hidden_dim, self.n_class).to(self.device)\n",
    "        #self.fc2 = nn.Linear( hidden_dim, self.n_class).to(self.device)\n",
    "        #self.fc3 = nn.Linear( hidden_dim, self.n_class).to(self.device)\n",
    "    def forward(self, G, gs):\n",
    "        h_global           = self.gat_global( G, label=G.ndata['label'].nonzero().flatten() )\n",
    "        #gs.ndata['weight'] = h_global[ gs.ndata['idx'] ] # Tentar concatenando\n",
    "        h                  = h_global[ gs.ndata['idx'] ]\n",
    "        h_local            = self.gat_local(gs, term=None)\n",
    "        h_local            = torch.cat((h, h_local), 1)\n",
    "        h_docs             = self.pooling( gs, h_local )\n",
    "        return self.fc1( h_docs )\n",
    "# torch.Size([3652, 300]) torch.Size([3652, 300]) torch.Size([128, 300])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=300\n",
    "hidden_dim=300\n",
    "n_heads=8\n",
    "drop=0.3\n",
    "attn_drop=0.5\n",
    "batch_size=32\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat_global): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gat_local): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (term): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lin): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (pooling): GlobalAttentionPooling(\n",
       "    (gate_nn): Linear(in_features=600, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=600, out_features=90, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TGA( in_dim, hidden_dim, graph_builder.n_class,\n",
    "            n_heads=n_heads, drop=drop, attn_drop=attn_drop )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl = []\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl.append( g_dgl )\n",
    "        \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, dgl.batch(Gs_dgl), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e2f93742284945ae081c0ab8087838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10627.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.73 GiB (GPU 0; 3.95 GiB total capacity; 1.89 GiB already allocated; 1.10 GiB free; 133.78 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2a0018b7b254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/dgl/backend/pytorch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_out)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mgrad_lhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reduce_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_lhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_data_nd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mgrad_rhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs_data_nd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeat_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             K.backward_rhs_binary_op_reduce(\n\u001b[1;32m    357\u001b[0m                 \u001b[0mreducer\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreducer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.73 GiB (GPU 0; 3.95 GiB total capacity; 1.89 GiB already allocated; 1.10 GiB free; 133.78 MiB cached)"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=2)\n",
    "    with tqdm(total=len(fold.y_train)) as pbar:\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for G, gs, y in data_loader:\n",
    "            G = G.to( torch.device('cuda:0') )\n",
    "            gs = gs.to( torch.device('cuda:0') )\n",
    "            y = y.to( torch.device('cuda:0') )\n",
    "            outputs = model( G, gs )\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            total += y.size(0)\n",
    "            correct += (sampled_Y == y).sum().item()\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {epoch} Acc train: {correct/total:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf sdfg sdfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigG.ndata['label'].nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.unbatch(Gs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs.ndata['idx'].max(),Gs.ndata['idx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigG.ndata['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigG.ndata['emb'][Gs.ndata['idx']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                        encoders={'term','label'}, \n",
    "                        n_heads=n_heads, drop=drop,\n",
    "                        attn_drop=attn_drop, device=device)\n",
    "gat_local  = GenericGAT(in_dim, hidden_dim, \n",
    "                        encoders={'term'}, \n",
    "                        n_heads=n_heads, drop=drop,\n",
    "                        attn_drop=attn_drop, device=device)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_Y = torch.softmax(outputs, 1)\n",
    "sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "\n",
    "total += labels.size(0)\n",
    "correct += (sampled_Y == labels).sum().item()\n",
    "\n",
    "# NN backprop phase\n",
    "loss = loss_func(outputs, labels)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "epoch_loss += loss.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(zip(fold.X_train, fold.y_train), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=graph_builder.collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        #  g_dgl, labels, node_idx, docs_idx, global_terms_idx, range(nclass)\n",
    "        for i, (bg, labels, node_idx, docs_idx, global_terms_idx, terms_idx, labels_idx) in enumerate(data_loader):\n",
    "            # model(G, docs_idx, terms_idx, global_terms_idx, labels_idx)\n",
    "            outputs, L_hiddens, GT_hiddens, doc_dists = model(bg, docs_idx, terms_idx, global_terms_idx, labels_idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, graphsize=None,\n",
    "                 n_heads=8, drop=.5, attn_drop=.5, device='cuda:0'):\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        if graphsize is not None:\n",
    "            self.graphsize = graphsize\n",
    "        \n",
    "        self.gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                                     encoders={'term','label'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        \n",
    "        self.gat_local = GenericGAT(in_dim, hidden_dim,\n",
    "                                     encoders={'term'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Linear( hidden_dim, 1).to(self.device)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "        \n",
    "    def forward(self, G, gs):\n",
    "        H = self.gat_global(G, term=self.graphsize.global_term_ids.values(), label=self.graphsize.label_ids)\n",
    "        h = self.gat_local(gs, term=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train):\n",
    "    G = graphisize.big_graph( X_train, y_train )\n",
    "    for e in epochs:\n",
    "        G = gat_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform( fold.y_train )\n",
    "y_val   = le.transform( fold.y_val )\n",
    "\n",
    "graph_builder.embbeding = { t:v for (t,v) in graph_builder.embeddings_dict.items() }\n",
    "graph_builder.label_ids = [ y for y in le.classes_ ]\n",
    "for y in graph_builder.label_ids:\n",
    "    hotenc = np.zeros(300)\n",
    "    hotenc[y] = 1\n",
    "    graph_builder.embbeding[y] = hotenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_graph(self, X, y):\n",
    "    docs = list(map(self.analyzer.build_analyzer(), self.progress_bar(X)))\n",
    "    edges_to_add = set()\n",
    "    self.node_mapper = { y: y for y in label_ids }\n",
    "    for (doc,y) in zip( docs, y_train ):\n",
    "\n",
    "        doc_in_terms = set(filter( lambda x: x in self.embeddings_dict, doc))\n",
    "        terms_by_id = list(map(lambda x: self.node_mapper.setdefault(x, len(self.node_mapper)), doc_in_terms))\n",
    "\n",
    "        list_of_edges = list(map( lambda x: (y, x), terms_by_id ))\n",
    "        list(map(edges_to_add.add, list_of_edges))\n",
    "        \n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from( [ (idx, {'emb': self.embbeding[t], 'term': t} ) for (t,idx) in self.node_mapper.items() ] )\n",
    "    g.add_edges_from( edges_to_add )\n",
    "    return g\n",
    "    #g_dgl = dgl.DGLGraph()\n",
    "    #g_dgl.from_networkx(g, node_attrs=['emb'] )\n",
    "    #g_dgl = g_dgl.to(torch.device('cuda:0'))\n",
    "    #return g_dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get_big_graph(graph_builder, fold.X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GenericGAT(hidden_dim, hidden_dim, \n",
    "        n_heads=4, drop=0.01, attn_drop=0.01, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = gat(g_dgl)\n",
    "h = h.view(h.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[label_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h, gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerCollate(object):\n",
    "    def __init__(self, label_vectors, terms_vectors, nclass, device='cuda:0'):\n",
    "        self.terms_vectors = terms_vectors\n",
    "        self.label_vectors = label_vectors\n",
    "        self.nclass = nclass\n",
    "        self.device = torch.device(device)\n",
    "    def collate(self, samples):\n",
    "        Gs_Fs, labels = map(list, zip(*samples))\n",
    "        big_graph = nx.Graph()\n",
    "        node_idx = { }\n",
    "        for y in range(self.nclass):\n",
    "            node_idx.setdefault( ('L', y), len(node_idx) )\n",
    "\n",
    "        big_graph.add_nodes_from( [ (node_idx[('L',y)], {'idx': node_idx[('L',y)], 'emb': self.label_vectors[y]}) for y in range(self.nclass)] )\n",
    "\n",
    "        docs_idx = []\n",
    "        terms_idx = []\n",
    "        global_terms_idx = set()\n",
    "\n",
    "        for i,(g,y) in enumerate(list(zip(Gs_Fs, y_train))):\n",
    "            # Get words idxs and add in the big graph\n",
    "            nodes = [ ( node_idx.setdefault((i,w), len(node_idx) ), {'idx': node_idx[(i,w)], 'emb': att['emb']}) for w,att in g.nodes(data=True) ]\n",
    "            big_graph.add_nodes_from( nodes )\n",
    "\n",
    "            # Build document words idxs\n",
    "            if len(nodes) > 0:\n",
    "                nodes,_ = list(zip(*nodes))\n",
    "                terms_idx.extend( list(nodes) )\n",
    "            docs_idx.append( list(nodes) )\n",
    "\n",
    "            # Add terms -> terms edges in the big graph (Local terms co-occurs)\n",
    "            w_edges = [ (node_idx.setdefault((i,s), len(node_idx) ), node_idx.setdefault((i,t), len(node_idx) )) for (s,t) in g.edges ]\n",
    "            big_graph.add_edges_from( w_edges )\n",
    "\n",
    "            # Get global terms nodes format=[(id_global_term,{ key_att: value_att })]\n",
    "            filtered_nodes = []\n",
    "            for (w,att) in g.nodes(data=True):\n",
    "                id_w = node_idx.setdefault(('gt', w), len(node_idx) )\n",
    "                if id_w not in big_graph:\n",
    "                    #filtered_nodes.append( ( id_w,{'idx': id_w, 'emb': self.terms_vectors[att['word']]} ) )\n",
    "                    filtered_nodes.append( ( id_w,{'idx': id_w, 'emb': att['emb']} ) )\n",
    "            #filtered_nodes = [ ( node_idx.setdefault(('gt', w), len(node_idx) ),{'idx': node_idx[('gt', w)], 'emb': self.terms_vectors['emb']} ) for (w,att) in g.nodes(data=True) if node_idx.setdefault(('gt',w), len(node_idx) ) not in big_graph ]\n",
    "            big_graph.add_nodes_from( filtered_nodes )\n",
    "\n",
    "            # Add labels -> terms edges in the big graph\n",
    "            big_graph.add_edges_from( [(node_idx[('L', y)], node_idx[('gt', w)]) for w in g.nodes] )\n",
    "\n",
    "            # Add words -> terms edges in the big graph\n",
    "            big_graph.add_edges_from( [(node_idx[(i, w)], node_idx[('gt', w)]) for w in g.nodes] )\n",
    "\n",
    "            # Add global terms idx\n",
    "            if len(filtered_nodes) > 0:\n",
    "                filtered_nodes,_ = list(zip(*filtered_nodes))\n",
    "                global_terms_idx = global_terms_idx.union( set(filtered_nodes) )\n",
    "\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(big_graph, node_attrs=['idx','emb'] )\n",
    "\n",
    "        #g_dgl.ndata['emb'] = torch.FloatTensor(g_dgl.ndata['emb'])\n",
    "        g_dgl.to(torch.device('cuda:0'))\n",
    "\n",
    "        labels = torch.tensor(labels).to(self.device)\n",
    "        \n",
    "        del big_graph, nodes\n",
    "\n",
    "        return g_dgl, labels, { v:k for (k,v) in node_idx.items() }, list(docs_idx), list(global_terms_idx), list(terms_idx), range(self.nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes,\n",
    "                 n_heads=8, drop=.5, attn_drop=.5,\n",
    "                 last=None, dist_func='diff',\n",
    "                 device='cuda:0'):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.encoder_global_term = nn.Linear(in_dim, hidden_dim).to(self.device)\n",
    "        self.encoder_label       = nn.Linear(in_dim, hidden_dim).to(self.device)\n",
    "        self.encoder_term        = nn.Linear(in_dim, hidden_dim).to(self.device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device),\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device)\n",
    "        ])\n",
    "        \n",
    "        self.pool_labels_old = nn.Linear(2*hidden_dim, hidden_dim).to(self.device)\n",
    "        self.pool_global_terms_old = nn.Linear(2*hidden_dim, hidden_dim).to(self.device)\n",
    "        \n",
    "\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device),\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        self.last = last\n",
    "        \n",
    "        self._doc_weights_ = self._doc_weights_att_\n",
    "        \n",
    "        self.lin = nn.Linear( hidden_dim, 1).to(self.device)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "        \n",
    "        if dist_func.lower() == 'cat':\n",
    "            self._dist_func_ = self.dist_cat_func\n",
    "            self.att_w = nn.Linear( 2*hidden_dim, 1).to(self.device)\n",
    "        else:\n",
    "            self.att_w = nn.Linear( hidden_dim, 1).to(self.device)\n",
    "            if dist_func.lower() == 'diff':\n",
    "                self._dist_func_ = self.dist_diff_func\n",
    "            elif dist_func.lower() == 'mult':\n",
    "                self._dist_func_ = self.dist_mult_func\n",
    "            elif dist_func.lower() == 'pool':\n",
    "                self._doc_weights_ = self._doc_weights_pool_att_\n",
    "                self.att_w = nn.Linear( 2*hidden_dim, 1).to(self.device)\n",
    "                self._dist_func_ = self.dist_mult_func\n",
    "                \n",
    "    \n",
    "    def forward(self, G, docs_idx, terms_idx, global_terms_idx, labels_idx):\n",
    "        \n",
    "        self.n_class = len(labels_idx)\n",
    "        self.n_docs_batch = len(docs_idx)\n",
    "  \n",
    "        h = G.ndata['emb'].float()\n",
    "        h[global_terms_idx] = self.encoder_global_term( h[global_terms_idx] )\n",
    "        h[labels_idx]       = self.encoder_label( h[labels_idx] )\n",
    "        h[terms_idx]        = self.encoder_term( h[terms_idx] )\n",
    "        \n",
    "        \n",
    "        old_labels_hiddens       = h[labels_idx]\n",
    "        old_global_terms_hiddens = h[global_terms_idx]\n",
    "        \n",
    "        G.ndata['emb'] = h\n",
    "        \n",
    "        for l, conv in enumerate(self.layers):\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "            h = self.down_proj[l]( h )\n",
    "        \n",
    "        Gs = list(map(G.subgraph, docs_idx))\n",
    "        batch = dgl.batch( Gs )\n",
    "        \n",
    "        docs_hiddens = self.pooling( batch, h[terms_idx] )\n",
    "        labels_hiddens = h[labels_idx]\n",
    "        global_terms_idx_hiddens = h[global_terms_idx]\n",
    "        \n",
    "        concated_labels = torch.cat( (old_labels_hiddens, labels_hiddens), 1 )\n",
    "        label_hiddens   = self.pool_labels_old( concated_labels )\n",
    "        \n",
    "        concated_global_terms    = torch.cat( (old_global_terms_hiddens, global_terms_idx_hiddens), 1 )\n",
    "        global_terms_idx_hiddens = self.pool_global_terms_old( concated_global_terms )\n",
    "        \n",
    "        weights, doc_dists = self._doc_weights_( docs_hiddens, labels_hiddens )\n",
    "        \n",
    "        if self.last is not None:\n",
    "            weights = self.last(weights)\n",
    "            \n",
    "        del docs_hiddens, batch, Gs, old_labels_hiddens\n",
    "        \n",
    "        return weights, labels_hiddens, global_terms_idx_hiddens, doc_dists\n",
    "    def _doc_weights_att_(self, docs_hiddens, labels_hiddens):\n",
    "        \n",
    "        result = []\n",
    "        for d in range(self.n_docs_batch):\n",
    "            doc_vec = docs_hiddens[d]\n",
    "            for c in range(self.n_class):\n",
    "                class_vec = labels_hiddens[c]\n",
    "                \n",
    "                dist = self._dist_func_( doc_vec, class_vec )\n",
    "                result.append(dist)\n",
    "                \n",
    "        doc_dists = torch.stack( result )\n",
    "        unstacked_weights = self.att_w( doc_dists )\n",
    "        weights = torch.reshape( unstacked_weights, (self.n_docs_batch, self.n_class) )\n",
    "        \n",
    "        del result, unstacked_weights\n",
    "        \n",
    "        return weights, doc_dists\n",
    "    \n",
    "    def _doc_weights_pool_att_(self, docs_hiddens, labels_hiddens):\n",
    "        \n",
    "        result = []\n",
    "        labels_hiddens = labels_hiddens.flatten()\n",
    "        for d in range(self.n_docs_batch):\n",
    "            doc_vec = docs_hiddens[d]\n",
    "            for c in range(self.n_class):\n",
    "                class_vec = labels_hiddens[c]\n",
    "                dist      = self._dist_func_( doc_vec, class_vec )\n",
    "                dist      = torch.cat((dist, labels_hiddens))\n",
    "            result.append(dist)\n",
    "            dist = self._dist_func_( doc_vec, class_vec )\n",
    "                \n",
    "        doc_dists = torch.stack( result )\n",
    "        weights = self.att_w( doc_dists )\n",
    "        \n",
    "        return weights, doc_dists\n",
    "        \n",
    "    def dist_diff_func(self, vec1, vec2):\n",
    "        return (vec1-vec2)\n",
    "        \n",
    "    def dist_mult_func(self, vec1, vec2):\n",
    "        return (vec1*vec2)\n",
    "        \n",
    "    def dist_cat_func(self, vec1, vec2):\n",
    "        return torch.cat((vec1, vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model:\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierGAT(graph_builder.ndim, hidden_dim, dataset.nclass,\n",
    "                      n_heads=n_heads, drop=.0, attn_drop=.0, dist_func='cat').to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_score = None\n",
    "n_iters = 0\n",
    "n_epochs = 3\n",
    "\n",
    "\n",
    "\n",
    "sampler = SamplerCollate(\n",
    "    label_vectors = { k: np.random.uniform( size=graph_builder.ndim ) for k in le.classes_ },\n",
    "    terms_vectors = graph_builder.embeddings_dict.copy(),\n",
    "    nclass = len(le.classes_)\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(list(zip(Gs_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=sampler.collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        #  g_dgl, labels, node_idx, docs_idx, global_terms_idx, range(nclass)\n",
    "        for i, (bg, labels, node_idx, docs_idx, global_terms_idx, terms_idx, labels_idx) in enumerate(data_loader):\n",
    "            # model(G, docs_idx, terms_idx, global_terms_idx, labels_idx)\n",
    "            outputs, L_hiddens, GT_hiddens, doc_dists = model(bg, docs_idx, terms_idx, global_terms_idx, labels_idx)\n",
    "            \n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (sampled_Y == labels).sum().item()\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            L_hiddens = L_hiddens.detach().cpu().numpy()\n",
    "            for l in sampler.label_vectors.keys():\n",
    "                sampler.label_vectors[l] = L_hiddens[l]\n",
    "                \n",
    "            terms = [ graph_builder.vocab_idx[node_idx[gt_id][1]] for gt_id in global_terms_idx ]\n",
    "            GT_hiddens = GT_hiddens.detach().cpu().numpy()\n",
    "            for i,t in enumerate(terms):\n",
    "                sampler.terms_vectors[t] = GT_hiddens[i]\n",
    "            \n",
    "            pbar.update( len(labels) )\n",
    "            pbar.set_description_str(f'iter {epoch} Acc train: {correct/total:.3}')\n",
    "            \n",
    "            del bg, labels, node_idx, docs_idx, global_terms_idx, terms_idx, labels_idx\n",
    "            del outputs, L_hiddens, GT_hiddens, doc_dists\n",
    "            del probs_Y, sampled_Y\n",
    "            del loss, terms\n",
    "            #break\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laxg srgs grdaels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_hidden_flatten = L_hiddens.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bg, labels, node_idx, docs_idx, global_terms_idx, terms_idx, labels_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
