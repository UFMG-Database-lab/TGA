{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from TGA.utils import Dataset, GraphsizePretrained\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:26, 15341.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 708 ms, total: 26.4 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test'), 11977)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/Documentos/datasets/classification/datasets/reut/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=False))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11977/11977 [00:06<00:00, 1794.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.36 s, sys: 124 ms, total: 8.48 s\n",
      "Wall time: 8.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(encoding=None,\n",
       "                    pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt',\n",
       "                    verbose=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174619, 24001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 275),\n",
       " (1, 77),\n",
       " (2, 525),\n",
       " (3, 2248),\n",
       " (4, 1302),\n",
       " (5, 650),\n",
       " (6, 2638),\n",
       " (7, 1165),\n",
       " (8, 1483),\n",
       " (9, 58),\n",
       " (10, 490),\n",
       " (11, 580),\n",
       " (12, 1537),\n",
       " (13, 1113),\n",
       " (14, 219),\n",
       " (15, 2257),\n",
       " (16, 6159),\n",
       " (17, 5771),\n",
       " (18, 3211),\n",
       " (19, 651),\n",
       " (20, 272),\n",
       " (21, 339),\n",
       " (22, 853),\n",
       " (23, 604),\n",
       " (24, 4536),\n",
       " (25, 794),\n",
       " (26, 3059),\n",
       " (27, 1841),\n",
       " (28, 835),\n",
       " (29, 1766),\n",
       " (30, 1953),\n",
       " (31, 6886),\n",
       " (32, 721),\n",
       " (33, 2124),\n",
       " (34, 433),\n",
       " (35, 108),\n",
       " (36, 431),\n",
       " (37, 719),\n",
       " (38, 113),\n",
       " (39, 11350),\n",
       " (40, 4884),\n",
       " (41, 9921),\n",
       " (42, 294),\n",
       " (43, 3537),\n",
       " (44, 2962),\n",
       " (45, 42),\n",
       " (46, 335),\n",
       " (47, 947),\n",
       " (48, 1233),\n",
       " (49, 514),\n",
       " (50, 188),\n",
       " (51, 641),\n",
       " (52, 6302),\n",
       " (53, 140),\n",
       " (54, 972),\n",
       " (55, 1785),\n",
       " (56, 498),\n",
       " (57, 1056),\n",
       " (58, 695),\n",
       " (59, 1442),\n",
       " (60, 1751),\n",
       " (61, 376),\n",
       " (62, 906),\n",
       " (63, 3476),\n",
       " (64, 553),\n",
       " (65, 372),\n",
       " (66, 455),\n",
       " (67, 2464),\n",
       " (68, 905),\n",
       " (69, 1357),\n",
       " (70, 545),\n",
       " (71, 3748),\n",
       " (72, 1700),\n",
       " (73, 1508),\n",
       " (74, 66),\n",
       " (75, 1001),\n",
       " (76, 3434),\n",
       " (77, 1909),\n",
       " (78, 479),\n",
       " (79, 877),\n",
       " (80, 260),\n",
       " (81, 904),\n",
       " (82, 94),\n",
       " (83, 2609),\n",
       " (84, 1192),\n",
       " (85, 2229),\n",
       " (86, 132),\n",
       " (87, 2542),\n",
       " (88, 278),\n",
       " (89, 2142)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 drop=.5, n_heads=8, attn_drop=.5,\n",
    "                 activation=F.leaky_relu, n_convs=2,\n",
    "                 first_hidden='emb', encoders={'term','label'},\n",
    "                 device='cpu:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        #self.norm = nn.BatchNorm1d(hidden_dim).to(self.device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=activation,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        self.norm_projs = [\n",
    "            nn.BatchNorm1d(hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        with G.local_scope():\n",
    "            h = G.ndata[self.first_hidden].float()\n",
    "            for (k, mask) in kwargs.items():\n",
    "                if k in self.encoders:\n",
    "                    if mask is not None:\n",
    "                        h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                    else:\n",
    "                        h = self.encoders[k]( h )\n",
    "\n",
    "            for l, conv in enumerate(self.layers):\n",
    "                h = conv(G, h)\n",
    "                h = h.view(h.shape[0], -1)\n",
    "                h = self.down_proj[l]( h )\n",
    "                h = self.norm_projs[l]( h )\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, n_heads=16, drop=.5, attn_drop=.5, device='cuda:0'):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device(device))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device)),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(n_heads*hidden_dim + hidden_dim, 1).to(torch.device(device))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device(device))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim + hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear( n_heads*hidden_dim + hidden_dim, n_classes).to(torch.device(device))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['emb'].float()\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((h,he), 1)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl_list = []\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        if len(g) > 0:\n",
    "            g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl_list.append( g_dgl )\n",
    "    \n",
    "    Gs_dgl = dgl.batch(Gs_dgl_list)\n",
    "    \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    #subgraph = graph_builder.g.subgraph(idx_terms)\n",
    "    #big_graph_dgl.from_networkx(subgraph, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, Gs_dgl, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(torch.nn.Module):\n",
    "    def __init__(self, input_l, hidden_l, nclass, n_heads=1,\n",
    "                drop=0.5, attn_drop=0.5, loss=None, n_convs=1,activation=None,\n",
    "                 device='cuda:0'):\n",
    "        \n",
    "        super(TGA, self).__init__()\n",
    "        \n",
    "        self.gat_global = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop, n_convs=n_convs,\n",
    "                 activation=activation, device='cuda:0' ).to(device)\n",
    "        \n",
    "        \n",
    "        self.gat_local = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop, n_convs=n_convs, encoders={'terms'},\n",
    "                 activation=activation, device='cuda:0' ).to(device)\n",
    "        \n",
    "        #self.norm_label = nn.BatchNorm1d(hidden_l).to(device)\n",
    "        #self.norm_docs = nn.BatchNorm1d(hidden_l).to(device)\n",
    "\n",
    "        self.gate = nn.Linear( hidden_l, 1 ).to(device)\n",
    "        self.feat = nn.Linear( hidden_l, hidden_l ).to(device)\n",
    "        self.gap  = GlobalAttentionPooling(self.gate, feat_nn=self.feat).to(device)\n",
    "        \n",
    "        #self.nclass  = nclass\n",
    "        #self.fc1     = nn.Linear( hidden_l, hidden_l ).to(device)\n",
    "        #self.fc2     = nn.Linear(  hidden_l//2, self.nclass ).to(device)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.fc_global = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear( hidden_l, hidden_l )\n",
    "        )\n",
    "        \n",
    "        self.fc_local = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear( hidden_l, hidden_l )\n",
    "        )\n",
    "        \n",
    "        self.fc_local_classifier = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear( hidden_l, nclass )\n",
    "        )\n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, G, gs, y, label_idx=None):\n",
    "        if label_idx is None:\n",
    "            label_idx = G.ndata['label'].nonzero().flatten()\n",
    "            \n",
    "        terms_idx = range(len(label_idx),len(graph_builder.g))\n",
    "        \n",
    "        h_global  = self.gat_global(G, label=label_idx, term=terms_idx)\n",
    "\n",
    "        h_labels  = h_global[label_idx]\n",
    "        #h_labels  = self.norm_label(h_labels)\n",
    "        h_labels  = self.fc_global(h_labels)\n",
    "\n",
    "        gs.ndata['emb'] = h_global[gs.ndata['idx'].reshape(-1)]\n",
    "        h_local         = self.gat_local(gs, terms=None)\n",
    "        h_docs          = self.gap( gs, h_local )\n",
    "        h_docs          = self.fc_local(h_docs)\n",
    "        #h_docs          = self.norm_docs(h_docs)\n",
    "        pred_docs       = self.fc_local_classifier(h_docs)\n",
    "        \n",
    "        #h_docs_pred = self.fc1(h_docs)\n",
    "        #h_docs_pred = self.fc2(h_docs_pred)\n",
    "        #h_docs_pred = nn.softmax(h_docs_pred, 1)\n",
    "        \n",
    "        return h_docs, pred_docs, h_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 300\n",
    "input_l = 300\n",
    "n_heads = 4\n",
    "drop=0.3\n",
    "batch_size=32\n",
    "attn_drop=0.3\n",
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat_global): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (term): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gat_local): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (terms): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gate): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (feat): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (gap): GlobalAttentionPooling(\n",
       "    (gate_nn): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (feat_nn): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_global): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_local): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_local_classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=90, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tga = TGA(input_l, hidden_l, nclass=graph_builder.n_class,\n",
    "          activation=None,\n",
    "          n_heads=n_heads, drop=drop, attn_drop=attn_drop, n_convs=2).to(device)\n",
    "tga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy\n",
    "class NpairLoss(nn.Module):\n",
    "    \"\"\"the multi-class n-pair loss\"\"\"\n",
    "    def __init__(self, l2_reg=0.02):\n",
    "        super(NpairLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, anchor, target, positive=None):\n",
    "        batch_size = anchor.size(0)\n",
    "        target = target.view(target.size(0), 1)\n",
    "\n",
    "        target = (target == torch.transpose(target, 0, 1)).float()\n",
    "        target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
    "\n",
    "        if positive is not None:\n",
    "            logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
    "        else:\n",
    "            logit = torch.matmul(anchor, torch.transpose(anchor, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size\n",
    "        \n",
    "        loss_ce = cross_entropy(logit, target)\n",
    "\n",
    "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
    "        return loss\n",
    "class SelfDistLoss(nn.Module):\n",
    "    def __init__(self, l2_reg=0.02):\n",
    "        super(SelfDistLoss, swelf).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "    def forward(self, hiddens):\n",
    "        L = torch.matmul(hiddens, hiddens.T)\n",
    "        L = F.sigmoid(L)\n",
    "        L_mapper = (L >= L.diag()).float()\n",
    "        L_mapper = L_mapper / L_mapper.sum(axis=1)\n",
    "        \n",
    "        return (L_mapper * L).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.AdamW( tga.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "\n",
    "loss_func_npl = NpairLoss(l2_reg=5e-4)\n",
    "loss_func_cel = nn.CrossEntropyLoss()\n",
    "\n",
    "#RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import set_start_method\n",
    "try:\n",
    "    set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5a4cdf49e840269c12e49af22a9315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6649f97570f4495589d8d9e8131246be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11977.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 74, 32, 32, 49, 32, 49, 49, 32, 32, 32, 74, 32, 32, 49, 15, 32, 32,\n",
      "        32, 49, 74, 32, 49, 49, 32, 25, 49, 49, 32, 32, 32, 32, 32, 32, 49, 35,\n",
      "        32, 49, 49, 32, 32, 74, 32, 32, 32, 74, 32, 32, 32, 49, 32, 49, 32, 49,\n",
      "        32, 32, 56, 32, 49, 32, 32, 32, 49, 49, 32, 49, 32, 67, 32, 32, 49, 49,\n",
      "        49, 32, 74, 49, 32, 32, 32, 49, 49, 32, 82, 32, 32, 32, 49, 32, 74, 32],\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "tensor([74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "        74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "        74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "        74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "        74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74],\n",
      "tensor([86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 41, 86, 86, 86, 45, 86, 86, 86, 86, 50, 86, 86, 53,\n",
      "        54, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 86, 83, 86, 54, 86, 86, 86, 86],\n",
      "tensor([50,  1, 50, 50, 41, 49, 41,  7, 41,  9, 49, 49, 50, 41, 14, 86, 41, 41,\n",
      "        41, 50, 50, 41, 41, 49, 41, 49, 41, 86, 41, 41, 30, 41, 50, 41, 50, 35,\n",
      "        41, 50, 50, 41, 41, 41, 49, 43, 41, 45, 50, 41, 50, 49, 50, 49, 41, 53,\n",
      "        50, 50, 56, 41, 50, 59, 50, 41, 50, 41, 49, 50, 41, 49, 50, 50, 50, 41,\n",
      "        41, 49, 74, 41, 41, 41, 41, 41, 50, 49, 50, 41, 41, 86, 86, 50, 53, 41],\n",
      "tensor([ 0,  1, 41, 41, 41, 74, 41, 41, 41, 41, 50, 74, 41, 74, 41, 41, 41, 41,\n",
      "        41, 19, 41, 66, 22, 74, 41, 25, 41, 41, 41, 41, 42, 41, 68, 41, 41, 74,\n",
      "        68, 41, 38, 41, 41, 41, 42, 43, 41, 45, 74, 41, 74, 74, 50, 68, 41, 74,\n",
      "        41, 41, 42, 41, 41, 41, 41, 41, 41, 41, 66, 66, 66, 41, 68, 41, 41, 41,\n",
      "        41, 41, 74, 41, 41, 41, 41, 68, 80, 41, 74, 41, 41, 41, 41, 41, 74, 41],\n",
      "tensor([50, 74, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 14, 50, 50, 41,\n",
      "        50, 50, 20, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 41, 50, 50, 50, 50,\n",
      "        50, 50, 50, 41, 41, 41, 74, 50, 50, 45, 46, 50, 50, 50, 50, 50, 50, 74,\n",
      "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 64, 50, 50, 50, 50, 50, 70, 50,\n",
      "        50, 50, 74, 50, 50, 50, 74, 79, 50, 50, 50, 50, 50, 50, 86, 50, 50, 50],\n",
      "tensor([45, 45, 45, 45, 45, 45, 45, 45, 45,  9, 45, 45, 45, 45, 45, 45, 45, 45,\n",
      "        45, 19, 45, 65, 45, 45, 45, 45, 45, 45, 45, 45, 45, 41, 45, 45, 45, 45,\n",
      "        45, 45, 86, 41, 45, 41, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
      "        45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 65, 45, 45, 45, 45, 45, 45,\n",
      "        45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 41, 86, 45, 45, 45],\n",
      "tensor([45, 45, 45, 79, 45, 79, 45, 45, 79,  9, 79, 79, 79, 45, 14, 79, 41, 41,\n",
      "        79, 45, 79, 79, 45, 45, 79, 79, 79, 45, 79, 79, 45, 41, 45, 33, 45, 79,\n",
      "        45, 45, 45, 41, 40, 41, 79, 45, 45, 45, 45, 45, 48, 49, 45, 45, 45, 45,\n",
      "        45, 79, 79, 45, 79, 45, 45, 86, 45, 45, 45, 65, 66, 45, 68, 79, 70, 45,\n",
      "        45, 79, 74, 45, 79, 79, 79, 79, 45, 45, 82, 41, 79, 41, 86, 45, 45, 79],\n",
      "tensor([29,  1, 68,  3, 29, 20, 22, 68, 68, 22, 68, 68, 22, 13, 14, 22, 22, 22,\n",
      "        68, 19, 20, 20, 22, 22, 22, 68, 22, 68, 20, 29, 22, 41, 68, 68, 68, 35,\n",
      "        68, 68, 20, 41, 22, 41, 68, 22, 22, 22, 46, 22, 22, 22, 50, 13, 22, 20,\n",
      "        68, 68, 66, 68, 58, 22, 22, 68, 68, 22, 64, 20, 66, 22, 68, 29, 70, 22,\n",
      "        68, 68, 22, 22, 22, 66, 68, 22, 80, 68, 68, 22, 68, 22, 22, 22, 20, 68],\n",
      "tensor([82, 45, 82, 82, 45, 82, 82, 82, 82, 45, 82, 82, 45, 45, 82, 45, 45, 80,\n",
      "        45, 82, 82, 21, 45, 45, 45, 82, 45, 45, 65, 82, 82, 45, 45, 82, 82, 82,\n",
      "        82, 82, 82, 41, 80, 41, 82, 45, 45, 45, 82, 45, 45, 45, 45, 82, 45, 45,\n",
      "        45, 45, 82, 82, 82, 45, 45, 82, 45, 45, 82, 65, 82, 82, 82, 82, 82, 45,\n",
      "        82, 45, 45, 82, 45, 82, 82, 82, 80, 82, 82, 45, 82, 45, 45, 82, 82, 45],\n",
      "tensor([42, 42, 42, 42, 88, 42, 65, 65, 42, 68, 42, 42, 42, 42, 65, 42, 68, 88,\n",
      "        42, 42, 20, 21, 42, 42, 42, 42, 42, 42, 42, 65, 88, 88, 42, 42, 42, 42,\n",
      "        42, 65, 65, 41, 68, 41, 42, 88, 42, 42, 65, 42, 88, 42, 42, 42, 42, 42,\n",
      "        42, 42, 42, 42, 42, 88, 42, 42, 88, 88, 42, 65, 42, 42, 42, 42, 42, 42,\n",
      "        42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 88, 42, 88, 88, 42, 88, 42],\n",
      "tensor([42, 88, 42, 58, 58, 42, 21, 58, 58, 88, 58, 42, 42, 42, 42, 58, 88, 21,\n",
      "        58, 58, 42, 42, 42, 42, 88, 42, 58, 42, 58, 58, 58, 88, 42, 58, 58, 58,\n",
      "        42, 42, 42, 41, 88, 41, 42, 42, 42, 64, 42, 42, 88, 58, 53, 42, 42, 42,\n",
      "        42, 58, 42, 58, 58, 58, 58, 42, 42, 58, 64, 42, 42, 42, 42, 42, 42, 42,\n",
      "        42, 42, 53, 21, 21, 58, 42, 42, 42, 42, 42, 88, 42, 21, 42, 42, 42, 42],\n",
      "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 53, 65, 65, 65, 35,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 53,\n",
      "        65, 65, 65, 65, 58, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 82, 65, 65, 65, 65, 65, 65, 65],\n",
      "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65,  9, 65, 65, 65, 65, 65, 65, 21, 65,\n",
      "        65, 65, 20, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,  0, 65, 65, 65, 65,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 45, 65, 65, 65, 65, 65, 65, 21, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 20,\n",
      "        65, 65, 45, 65, 65, 65, 65, 65, 65, 65, 65,  0, 65, 21, 65, 65, 65, 65],\n",
      "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65],\n",
      "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65,  9, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65],\n",
      "tensor([65,  1, 65, 65, 65, 65, 65, 65, 65, 74, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 74, 65, 65, 65, 65,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 45, 65, 65, 53, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 74, 65, 65, 65, 65, 65, 65, 65, 65, 53, 65, 65, 65, 65, 65, 65],\n",
      "tensor([20,  1, 88, 20, 20,  5, 20, 20, 20,  9, 20, 20, 20, 20, 20, 20, 88, 20,\n",
      "        20, 20, 20, 20, 88, 20, 20, 20, 20, 20, 20, 20, 20, 88, 20, 20, 20, 35,\n",
      "        20, 20, 88, 41, 88, 41, 42, 20, 20, 74, 88, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20, 20, 88, 20, 20, 20, 20, 66, 20, 68, 20, 20, 20,\n",
      "        20, 20, 74, 20, 20, 20, 20, 20, 20, 88, 20, 88, 20, 88, 86, 20, 88, 20],\n",
      "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 45, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65],\n",
      "tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 74, 20, 11, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 38, 41, 20, 41, 20, 20, 20, 45, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 74, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 74, 74, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 74, 20, 20, 20, 20],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([37, 45, 37, 37, 37, 37, 37, 37, 37,  9, 37, 65, 84, 37, 37, 37, 84, 37,\n",
      "        37, 37, 20, 37, 37, 23, 37, 20, 37, 37, 37, 37, 37, 84, 37, 37, 37, 65,\n",
      "        37, 37, 38, 41, 37, 41, 37, 37, 84, 45, 37, 37, 84, 37, 37, 37, 37, 53,\n",
      "        37, 37, 65, 84, 37, 37, 37, 37, 37, 37, 64, 65, 37, 37, 37, 37, 37, 37,\n",
      "        37, 37, 74, 37, 37, 37, 37, 37, 80, 37, 37, 84, 84, 37, 45, 37, 37, 84],\n",
      "tensor([20,  1, 53, 20, 20, 53, 53, 20, 20,  9, 20, 20, 20, 13, 14, 20, 53, 53,\n",
      "        53, 20, 20, 20, 53, 20, 53, 25, 20, 27, 53, 53, 53, 20, 53, 53, 53, 53,\n",
      "        42, 37, 74, 41, 20, 41, 42, 53, 53, 45, 20, 53, 53, 53, 20, 20, 53, 53,\n",
      "        20, 53, 53, 53, 20, 53, 20, 20, 20, 53, 53, 65, 53, 53, 20, 53, 53, 53,\n",
      "        53, 20, 74, 53, 53, 53, 20, 53, 53, 53, 20, 53, 20, 53, 74, 20, 53, 53],\n",
      "tensor([20,  1, 65, 11, 64, 11, 11, 65, 20,  9, 65, 11, 20, 11, 14, 11, 20, 65,\n",
      "        64, 11, 20, 65, 20, 11, 20, 14, 65, 65, 14, 65, 14, 20, 20, 20, 64, 35,\n",
      "        20, 37, 38, 41, 64, 41, 20, 65, 20, 45, 14, 20, 20, 65, 14, 11, 14, 20,\n",
      "        64, 14, 56, 20, 65, 20, 20, 20, 11, 11, 64, 65, 11, 65, 11, 65, 64, 11,\n",
      "        20, 20,  1, 20, 20, 11, 20, 64, 65, 20, 20, 20, 65, 20, 20, 11, 65, 65],\n",
      "tensor([64, 64, 64, 64, 64, 64, 64, 64, 64,  9, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 34, 64,\n",
      "        64, 64, 64, 41, 64, 41, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64],\n",
      "tensor([65,  1, 11, 65, 11, 65, 65, 65, 65, 41, 65, 11, 11, 65, 65, 65, 11, 11,\n",
      "        11, 11, 65, 65, 11, 65, 11, 65, 65, 65, 65, 65, 65, 11, 11, 65, 65, 65,\n",
      "        65, 65, 11, 41, 65, 41, 65, 11, 11, 65, 23, 65, 23, 65, 11, 65, 11, 65,\n",
      "        65, 65, 65, 11, 65, 65, 11, 65, 65, 65, 65, 65, 11, 65, 65, 65, 65, 65,\n",
      "        11, 65, 74, 65, 65, 11, 65, 65, 65, 65, 65, 11, 65, 71, 65, 11, 65, 65],\n",
      "tensor([65, 45, 65, 70, 65, 65, 65, 65, 65, 41, 65, 65, 70, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 21, 70, 70, 70, 25, 65, 65, 65, 65, 65, 70, 65, 65, 65, 65,\n",
      "        65, 65, 65, 41, 65, 41, 65, 65, 65, 45, 70, 70, 45, 65, 70, 65, 65, 70,\n",
      "        65, 70, 65, 70, 65, 70, 65, 70, 65, 65, 64, 65, 65, 65, 65, 65, 70, 65,\n",
      "        65, 70, 45, 70, 70, 65, 70, 65, 65, 70, 70, 70, 65, 67, 70, 65, 65, 70],\n",
      "tensor([20,  1, 20, 20, 20, 20, 20, 20, 20, 41, 20, 51, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 51, 20, 20, 20, 20, 20, 51, 20, 20, 51, 20, 20, 20, 51, 51,\n",
      "        20, 20, 20, 41, 20, 41, 20, 20, 20, 45, 20, 20, 20, 20, 20, 51, 20, 53,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 51, 20, 51, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 51, 20, 20, 20, 20, 20],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-47921ee1f1df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "best = None\n",
    "nepochs = 25\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=4)\n",
    "    total_loss = 0.\n",
    "    with tqdm(total=len(fold.y_train), smoothing=0.) as pbar:\n",
    "        total = 1\n",
    "        correct_class = 0\n",
    "        correct_repre = 0\n",
    "        correct_both  = 0\n",
    "        tga.train()\n",
    "        for i, (G, gs, y) in enumerate(data_loader):\n",
    "            G = G.to( device )\n",
    "            gs = gs.to( device )\n",
    "            y = y.to( device )\n",
    "            \n",
    "            h_docs, pred_docs, h_labels = tga( G, gs, y )\n",
    "            #h_docs, pred_docs = tga( G, gs, y )\n",
    "            \n",
    "            \n",
    "            pred_docs = F.softmax(pred_docs)\n",
    "            pred_docs2 = F.softmax(torch.matmul(h_docs, h_labels.T))\n",
    "            pred_docs3 = pred_docs+pred_docs2\n",
    "            \n",
    "            loss1 = loss_func_npl( h_docs, y, positive=h_labels[y] )\n",
    "            loss2 = loss_func_cel(pred_docs, y)\n",
    "            \n",
    "            loss = loss1 + loss2\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total      += len(y)\n",
    "            \n",
    "            y_pred = pred_docs.argmax(axis=1)\n",
    "            correct_repre    += (y_pred == y).sum()\n",
    "            \n",
    "            y_pred = pred_docs2.argmax(axis=1)\n",
    "            correct_class    += (y_pred == y).sum()\n",
    "            \n",
    "            y_pred = pred_docs3.argmax(axis=1)\n",
    "            correct_both     += (y_pred == y).sum()\n",
    "            \n",
    "            to_print   = f'Acc Clss: {(1.*correct_class/total).item():.3} '\n",
    "            to_print  += f'Repr: {(1.*correct_repre/total).item():.3}'\n",
    "            to_print  += f'Both: {(1.*correct_both/total).item():.3}'\n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {e} Loss: {total_loss/(i+1):.4} ({to_print})')\n",
    "            \n",
    "            #break\n",
    "            if best is None or best > (total_loss/(i+1)):\n",
    "                hiddens_labels = h_labels\n",
    "                hiddens_docs = h_docs\n",
    "                best = total_loss/(i+1)\n",
    "            #del loss, h_labels, G, gs, loss1, loss2, pred_docs, h_docs\n",
    "            print(nn.Softmax()(torch.matmul(h_labels, h_labels.T)).argmax(axis=1), end='\\r')\n",
    "            del loss, G, gs, loss1, pred_docs, h_docs\n",
    "            #break\n",
    "    del data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter 23 Loss: 3.792 (Acc Clss: 0.765 Repr: 0.829): 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65,  1, 65, 65, 65, 65, 65, 65, 65, 41, 65, 65, 65, 13, 65, 65, 65, 65,\n",
       "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,  0, 65, 65, 65, 65,\n",
       "        65, 65,  0, 41, 65, 41, 65, 65, 65, 65,  0, 70, 41, 65, 65, 65, 65, 65,\n",
       "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65,\n",
       "        65, 65,  1, 65, 65, 65, 62, 65, 65, 65, 65, 62, 65, 20, 65, 65, 65, 65],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(torch.matmul(h_labels, h_labels.T)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(856.7173, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1735, device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(L >= L.diag()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_mapper.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(L_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
