{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from TGA.utils import Dataset, GraphsizePretrained\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:26, 14856.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 759 ms, total: 27.3 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test'), 7376)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/Documentos/datasets/classification/datasets/webkb/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=False))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7376/7376 [00:06<00:00, 1109.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 s, sys: 68.5 ms, total: 8.41 s\n",
      "Wall time: 8.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(encoding=None,\n",
       "                    pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt',\n",
       "                    verbose=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91187, 20770)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 12418),\n",
       " (1, 19960),\n",
       " (2, 9382),\n",
       " (3, 3918),\n",
       " (4, 9014),\n",
       " (5, 12838),\n",
       " (6, 2901)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 drop=.5, n_heads=8, attn_drop=.5,\n",
    "                 activation=F.leaky_relu, n_convs=2,\n",
    "                 first_hidden='emb', encoders={'term','label'},\n",
    "                 device='cpu:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        #self.norm = nn.BatchNorm1d(hidden_dim).to(self.device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=activation,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        self.norm_projs = [\n",
    "            nn.BatchNorm1d(hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        with G.local_scope():\n",
    "            h = G.ndata[self.first_hidden].float()\n",
    "            for (k, mask) in kwargs.items():\n",
    "                if k in self.encoders:\n",
    "                    if mask is not None:\n",
    "                        h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                    else:\n",
    "                        h = self.encoders[k]( h )\n",
    "\n",
    "            for l, conv in enumerate(self.layers):\n",
    "                h = conv(G, h)\n",
    "                h = h.view(h.shape[0], -1)\n",
    "                h = self.down_proj[l]( h )\n",
    "                h = self.norm_projs[l]( h )\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, n_heads=16, drop=.5, attn_drop=.5, device='cuda:0'):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device(device))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device)),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(n_heads*hidden_dim + hidden_dim, 1).to(torch.device(device))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device(device))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim + hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear( n_heads*hidden_dim + hidden_dim, n_classes).to(torch.device(device))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['emb'].float()\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((h,he), 1)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl_list = []\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        if len(g) > 0:\n",
    "            g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl_list.append( g_dgl )\n",
    "    \n",
    "    Gs_dgl = dgl.batch(Gs_dgl_list)\n",
    "    \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    #subgraph = graph_builder.g.subgraph(idx_terms)\n",
    "    #big_graph_dgl.from_networkx(subgraph, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, Gs_dgl, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(torch.nn.Module):\n",
    "    def __init__(self, input_l, hidden_l, nclass, n_heads=1,\n",
    "                drop=0.5, attn_drop=0.5, loss=None, n_convs=1,activation=None,\n",
    "                 device='cuda:0'):\n",
    "        \n",
    "        super(TGA, self).__init__()\n",
    "        \n",
    "        self.gat_global = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop, n_convs=n_convs,\n",
    "                 activation=activation, device='cuda:0' ).to(device)\n",
    "        \n",
    "        \n",
    "        self.gat_local = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop, n_convs=n_convs, encoders={'terms'},\n",
    "                 activation=activation, device='cuda:0' ).to(device)\n",
    "        \n",
    "        #self.norm_label = nn.BatchNorm1d(hidden_l).to(device)\n",
    "        #self.norm_docs = nn.BatchNorm1d(hidden_l).to(device)\n",
    "\n",
    "        self.gate = nn.Linear( hidden_l, 1 ).to(device)\n",
    "        self.feat = nn.Linear( hidden_l, hidden_l ).to(device)\n",
    "        self.gap  = GlobalAttentionPooling(self.gate, feat_nn=self.feat).to(device)\n",
    "        \n",
    "        #self.nclass  = nclass\n",
    "        #self.fc1     = nn.Linear( hidden_l, hidden_l ).to(device)\n",
    "        #self.fc2     = nn.Linear(  hidden_l//2, self.nclass ).to(device)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.fc_global = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear( hidden_l, hidden_l )\n",
    "        )\n",
    "        \n",
    "        self.fc_local = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear( hidden_l, hidden_l )\n",
    "        )\n",
    "        \n",
    "        self.fc_local_classifier = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear( hidden_l, nclass )\n",
    "        )\n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, G, gs, y, label_idx=None):\n",
    "        if label_idx is None:\n",
    "            label_idx = G.ndata['label'].nonzero().flatten()\n",
    "            \n",
    "        terms_idx = range(len(label_idx),len(graph_builder.g))\n",
    "        \n",
    "        h_global  = self.gat_global(G, label=label_idx, term=terms_idx)\n",
    "\n",
    "        h_labels  = h_global[label_idx]\n",
    "        #h_labels  = self.norm_label(h_labels)\n",
    "        h_labels  = self.fc_global(h_labels)\n",
    "\n",
    "        gs.ndata['emb'] = h_global[gs.ndata['idx'].reshape(-1)]\n",
    "        h_local         = self.gat_local(gs, terms=None)\n",
    "        h_docs          = self.gap( gs, h_local )\n",
    "        h_docs          = self.fc_local(h_docs)\n",
    "        #h_docs          = self.norm_docs(h_docs)\n",
    "        pred_docs       = self.fc_local_classifier(h_docs)\n",
    "        \n",
    "        #h_docs_pred = self.fc1(h_docs)\n",
    "        #h_docs_pred = self.fc2(h_docs_pred)\n",
    "        #h_docs_pred = nn.softmax(h_docs_pred, 1)\n",
    "        \n",
    "        return h_docs, pred_docs, h_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 300\n",
    "input_l = 300\n",
    "n_heads = 4\n",
    "drop=0.3\n",
    "batch_size=32\n",
    "attn_drop=0.3\n",
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat_global): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (term): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gat_local): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (terms): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=1200, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gate): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (feat): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (gap): GlobalAttentionPooling(\n",
       "    (gate_nn): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (feat_nn): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_global): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_local): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_local_classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tga = TGA(input_l, hidden_l, nclass=graph_builder.n_class,\n",
    "          activation=None,\n",
    "          n_heads=n_heads, drop=drop, attn_drop=attn_drop, n_convs=2).to(device)\n",
    "tga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy\n",
    "class NpairLoss(nn.Module):\n",
    "    \"\"\"the multi-class n-pair loss\"\"\"\n",
    "    def __init__(self, l2_reg=0.02):\n",
    "        super(NpairLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, anchor, target, positive=None):\n",
    "        batch_size = anchor.size(0)\n",
    "        target = target.view(target.size(0), 1)\n",
    "\n",
    "        target = (target == torch.transpose(target, 0, 1)).float()\n",
    "        target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
    "\n",
    "        if positive is not None:\n",
    "            logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
    "        else:\n",
    "            logit = torch.matmul(anchor, torch.transpose(anchor, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size\n",
    "        \n",
    "        loss_ce = cross_entropy(logit, target)\n",
    "\n",
    "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
    "        return loss\n",
    "class SelfDistLoss(nn.Module):\n",
    "    def __init__(self, l2_reg=0.02, eps = 0.00003):\n",
    "        super(SelfDistLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.eps = 0.00003\n",
    "    def forward(self, hiddens):\n",
    "        L = torch.matmul(hiddens, hiddens.T)\n",
    "        L = F.sigmoid(L)\n",
    "        L_mapper = (L >= L.diag()).float()\n",
    "        ind = torch.eye(L_mapper.size(0),L_mapper.size(1)).to(L_mapper.device)\n",
    "        L_mapper *= (1-ind)\n",
    "        L_mapper = F.normalize(L_mapper)\n",
    "        values = (L_mapper * L).sum(axis=1)\n",
    "        svalue = max((values > 0.).sum(), self.eps)\n",
    "        return values.sum()/svalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.AdamW( tga.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "\n",
    "loss_func_npl = NpairLoss(l2_reg=5e-4)\n",
    "loss_func_cel = nn.CrossEntropyLoss()\n",
    "loss_func_spc = SelfDistLoss()\n",
    "\n",
    "#RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import set_start_method\n",
    "try:\n",
    "    set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b524f9ff69fb4f00ae0f98e1131c7c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70c3e89d524495a9e16ae46edf0633d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7376.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L)oss: 4.8/5.45 NP-L: 2.86/3.21 CE-L: 1.36/1.62 Sp-L: 0.585/0.623 Acc Cls: 0.405 Repr: 0.553 Both: 0.553                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bc2e68e6554c878d700bff4d180ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7376.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L)oss: 3.76/4.71 NP-L: 2.21/2.97 CE-L: 1.55/1.51 Sp-L: 0.0/0.224 Acc Cls: 0.56 Repr: 0.653 Both: 0.654                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c774f7e9d4794ac2bfd799c98f89a024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7376.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L)oss: 3.59/4.41 NP-L: 1.99/2.81 CE-L: 1.6/1.45 Sp-L: 0.0/0.156 Acc Cls: 0.622 Repr: 0.717 Both: 0.718                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025cec7abb16463d9519e98381cd2985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7376.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L)oss: 3.96/4.28 NP-L: 2.48/2.68 CE-L: 1.48/1.41 Sp-L: 0.0/0.192 Acc Cls: 0.695 Repr: 0.748 Both: 0.75                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f842c36ea0434f1b95a1d41742dad99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7376.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L)oss: 3.37/4.19 NP-L: 2.19/2.6 CE-L: 1.18/1.4 Sp-L: 0.0/0.188 Acc Cls: 0.73 Repr: 0.763 Both: 0.763                     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best = None\n",
    "nepochs = 5\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=4)\n",
    "    total_loss  = 0.\n",
    "    total_loss1 = 0.\n",
    "    total_loss2 = 0.\n",
    "    total_loss3 = 0.\n",
    "    with tqdm(total=len(fold.y_train), smoothing=0.) as pbar:\n",
    "        total = 1\n",
    "        correct_class = 0\n",
    "        correct_repre = 0\n",
    "        correct_both  = 0\n",
    "        tga.train()\n",
    "        for i, (G, gs, y) in enumerate(data_loader):\n",
    "            G = G.to( device )\n",
    "            gs = gs.to( device )\n",
    "            y = y.to( device )\n",
    "            \n",
    "            h_docs, pred_docs, h_labels = tga( G, gs, y )\n",
    "            #h_docs, pred_docs = tga( G, gs, y )\n",
    "            \n",
    "            \n",
    "            pred_docs = F.softmax(pred_docs)\n",
    "            pred_docs2 = F.softmax(torch.matmul(h_docs, h_labels.T))\n",
    "            pred_docs3 = pred_docs+pred_docs2\n",
    "            \n",
    "            loss1 = loss_func_npl( h_docs, y, positive=h_labels[y] )\n",
    "            loss2 = loss_func_cel(pred_docs, y)\n",
    "            loss3 = loss_func_spc(h_labels)\n",
    "            \n",
    "            loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total      += len(y)\n",
    "            \n",
    "            y_pred            = pred_docs.argmax(axis=1)\n",
    "            correct_repre    += (y_pred == y).sum()\n",
    "            total_loss1      += loss1.item()\n",
    "            \n",
    "            y_pred = pred_docs2.argmax(axis=1)\n",
    "            correct_class    += (y_pred == y).sum()\n",
    "            total_loss2      += loss2.item()\n",
    "            \n",
    "            y_pred = pred_docs3.argmax(axis=1)\n",
    "            correct_both     += (y_pred == y).sum()\n",
    "            total_loss3      += loss3.item()\n",
    "            \n",
    "            to_print   = f'(L)oss: {loss.item():.3}/{total_loss /(i+1):.3} '\n",
    "            to_print  += f'NP-L: {loss1.item():.3}/{total_loss1/(i+1):.3} '\n",
    "            to_print  += f'CE-L: {loss2.item():.3}/{total_loss2/(i+1):.3} '\n",
    "            to_print  += f'Sp-L: {loss3.item():.3}/{total_loss3/(i+1):.3} '\n",
    "            to_print  += f'Acc Cls: {(1.*correct_class/total).item():.3} '\n",
    "            to_print  += f'Repr: {(1.*correct_repre/total).item():.3} '\n",
    "            to_print  += f'Both: {(1.*correct_both/total).item():.3}'\n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {e} Acc: {(1.*correct_both/total).item():.3}')\n",
    "            \n",
    "            #break\n",
    "            if best is None or best > (total_loss/(i+1)):\n",
    "                hiddens_labels = h_labels\n",
    "                hiddens_docs = h_docs\n",
    "                best = total_loss/(i+1)\n",
    "            #del loss, h_labels, G, gs, loss1, loss2, pred_docs, h_docs\n",
    "            print(to_print, end='                \\r')\n",
    "            del loss, G, gs, loss1, pred_docs, h_docs\n",
    "            #break\n",
    "    del data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webkb \n",
    "# acm   (L)oss:  3.94 L-NPair: 2.14 L-Cross: 1.76 L-Space: 0.0352 Acc Clss: 0.764 Repr: 0.779 Both: 0.777\n",
    "# reut  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens = h_labels\n",
    "\n",
    "\n",
    "L = torch.matmul(hiddens, hiddens.T)\n",
    "L = F.sigmoid(L)\n",
    "L_mapper = (L >= L.diag()).float()\n",
    "L_mapper *= (1-torch.eye(L_mapper.size(0),L_mapper.size(1)).to(torch.device('cuda:0')))\n",
    "L_mapper = F.normalize(L_mapper)\n",
    "#L_mapper = L_mapper / L_mapper.sum(axis=1, keepdims=True)\n",
    "L_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = (L_mapper * L).sum(axis=1)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svalue = max((values > 0.).sum(), 0.)\n",
    "svalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svalues = (values > 0.).sum()\n",
    "values = values.sum()/svalues\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-b454cdb962b9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-b454cdb962b9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    svalue = torch.max(, 0.002)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "svalue = torch.max(, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(values > 0.).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
