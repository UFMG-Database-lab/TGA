{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import Dataset, GraphsizePretrained\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:22, 17778.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 596 ms, total: 22.8 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=1, verbose=True,\n",
    "                   pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test'), 22402)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/acm/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=False))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22402/22402 [00:05<00:00, 4111.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.76 s, sys: 87.7 ms, total: 6.85 s\n",
      "Wall time: 6.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt',\n",
       "                    verbose=None, w=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133170, 36302)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3058),\n",
       " (1, 8738),\n",
       " (2, 10723),\n",
       " (3, 16623),\n",
       " (4, 1199),\n",
       " (5, 6160),\n",
       " (6, 5394),\n",
       " (7, 15351),\n",
       " (8, 13961),\n",
       " (9, 3823),\n",
       " (10, 11860)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 drop=.5, n_heads=8, attn_drop=.5,\n",
    "                 activation=F.leaky_relu, n_convs=2,\n",
    "                 first_hidden='emb', encoders={'term','label'},\n",
    "                 device='cpu:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=activation,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        with G.local_scope():\n",
    "            h = G.ndata[self.first_hidden].float()\n",
    "            for (k, mask) in kwargs.items():\n",
    "                if k in self.encoders:\n",
    "                    if mask is not None:\n",
    "                        h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                    else:\n",
    "                        h = self.encoders[k]( h )\n",
    "\n",
    "            for l, conv in enumerate(self.layers):\n",
    "                h = conv(G, h)\n",
    "                h = h.view(h.shape[0], -1)\n",
    "                h = self.down_proj[l]( h )\n",
    "                #h = F.relu( h )\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nx.Graph()\n",
    "g.add_node(1, weight=1)\n",
    "g.nodes[1]['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl_list = []\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl_list.append( g_dgl )\n",
    "    \n",
    "    Gs_dgl = dgl.batch(Gs_dgl_list)\n",
    "    \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    #subgraph = graph_builder.g.subgraph(idx_terms)\n",
    "    #big_graph_dgl.from_networkx(subgraph, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, Gs_dgl, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineDistanceLoss(torch.nn.Module):\n",
    "    def __init__(self, reduction='mean', alpha=-1.):\n",
    "        super(CosineDistanceLoss, self).__init__()\n",
    "\n",
    "        self.loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, X, Y=None, y_idxs=None):\n",
    "        nsmpl, ndims = X.shape\n",
    "        A = []\n",
    "        B = []\n",
    "        target = []\n",
    "        if Y is not None and y_idxs is not None:\n",
    "            for (x,y_idx) in zip(X, y_idxs):\n",
    "                for i,y in enumerate(Y):\n",
    "                    A.append( x )\n",
    "                    B.append( y )\n",
    "                    target.append( 1 if i == y_idx.item() else -1 )\n",
    "            \n",
    "        else:\n",
    "            for i in range(nsmpl):\n",
    "                for j in range(i+1, nsmpl):\n",
    "                    A.append( X[i] )\n",
    "                    B.append( self.alpha*X[j] )\n",
    "                    target.append( 1 )\n",
    "\n",
    "        A=torch.cat(A).reshape( len(target), ndims ).to(X.device)\n",
    "        B=torch.cat(B).reshape( len(target), ndims ).to(X.device)\n",
    "        target=torch.Tensor(target).to(X.device)\n",
    "            \n",
    "        \n",
    "        return self.loss(A, B, target=target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(torch.nn.Module):\n",
    "    def __init__(self, input_l, hidden_l, n_heads=1,\n",
    "                drop=0.5, attn_drop=0.5,\n",
    "                 device='cuda:0'):\n",
    "        \n",
    "        super(TGA, self).__init__()\n",
    "        self.gat = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop,\n",
    "                 activation=None, device='cuda:0' ).to(device)\n",
    "        self.norm = nn.BatchNorm1d(hidden_l).to(device)\n",
    "\n",
    "        self.gate = nn.Linear( hidden_l, 1 ).to(device)\n",
    "        self.feat = nn.Linear( hidden_l, hidden_l ).to(device)\n",
    "        self.gap = GlobalAttentionPooling(self.gate, feat_nn=self.feat).to(device)\n",
    "\n",
    "    def forward(self, G, gs, y, label_idx=None):\n",
    "        if label_idx is None:\n",
    "            label_idx = G.ndata['label'].nonzero().flatten()\n",
    "\n",
    "        h_global = self.gat(G, label=label_idx, term=range(len(label_idx),len(graph_builder.g)))\n",
    "        h_global = self.norm(h_global)\n",
    "\n",
    "        labels_hiddens = h_global[label_idx]\n",
    "\n",
    "        h_local = self.gap( gs, h_global[gs.ndata['idx'].reshape(-1)] )\n",
    "        \n",
    "        return h_local, h_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 300\n",
    "input_l = 300\n",
    "n_heads = 2\n",
    "drop=0.5\n",
    "batch_size=32\n",
    "attn_drop=0.5\n",
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (term): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=600, bias=False)\n",
       "        (feat_drop): Dropout(p=0.5, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.5, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=600, bias=False)\n",
       "        (feat_drop): Dropout(p=0.5, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.5, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gate): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (feat): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (gap): GlobalAttentionPooling(\n",
       "    (gate_nn): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (feat_nn): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tga = TGA(input_l, hidden_l, n_heads=n_heads, drop=drop, attn_drop=attn_drop).to(device)\n",
    "tga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_global = CosineDistanceLoss(reduction='mean', alpha=-0.5).to(device)\n",
    "loss_func_local  = CosineDistanceLoss(reduction='mean').to(device)\n",
    "\n",
    "optimizer = optim.AdamW( tga.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "\n",
    "\n",
    "#RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22807c19c0047d596cc1d9238d6b816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa82a9e35d548a18702b8b160169647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22402.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local loss: 0.12(0.103), global loss: -0.0413(-0.0486), loss: 0.079(0.0541)                 \r"
     ]
    }
   ],
   "source": [
    "\n",
    "best = None\n",
    "nepochs = 10\n",
    "lr = 2.*5e-2\n",
    "best_loss = None\n",
    "best_loss_local = None\n",
    "best_loss_global = None\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=2)\n",
    "    with tqdm(total=len(fold.y_train)) as pbar:\n",
    "        total = 1\n",
    "        correct = 1\n",
    "        tga.train()\n",
    "        for G, gs, y in data_loader:\n",
    "            G = G.to( device )\n",
    "            gs = gs.to( device )\n",
    "            y = y.to( device )\n",
    "            \n",
    "            h_local, h_global = tga( G, gs, y )\n",
    "            \n",
    "            label_idx = G.ndata['label'].nonzero().flatten()\n",
    "            labels_hiddens = h_global[label_idx]\n",
    "            \n",
    "            loss_global = loss_func_global(labels_hiddens)\n",
    "            \n",
    "            loss_local = loss_func_local(h_local, labels_hiddens, y)\n",
    "            \n",
    "            \n",
    "            loss = loss_global + loss_local\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "    \n",
    "            if best_loss is None or loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_loss_local = loss_local.item()\n",
    "                best_loss_global = loss_global.item()\n",
    "                best_h_global = h_global.cpu()\n",
    "            \n",
    "            \n",
    "            to_print = f'local loss: {loss_local:.3}({best_loss_local:.3}), '\n",
    "            to_print += f'global loss: {loss_global-1.:.3}({best_loss_global-1.:.3}), '\n",
    "            to_print += f'loss: {loss-1.:.3}({best_loss-1.:.3})             '\n",
    "            \n",
    "            print( to_print , end='\\r')\n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {e} Best Loss: {best_loss-1.:.3}')\n",
    "            \n",
    "            del loss, loss_local, loss_global, labels_hiddens, label_idx, G, gs, y, h_local, h_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bcf6b979dea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh_global\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'h_global' is not defined"
     ]
    }
   ],
   "source": [
    "best_h_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.ndata['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gat.train()\n",
    "best = None\n",
    "label_idx = big_graph_dgl.ndata['label'].nonzero().flatten()\n",
    "nepochs = 200\n",
    "lr = 2.*5e-2\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    if e % 50 == 1:\n",
    "        gat = best_model\n",
    "        optimizer = optim.AdamW( gat.parameters(), lr=lr/2, weight_decay=5e-3)\n",
    "    h = gat(big_graph_dgl, label=label_idx, term=range(len(label_idx),len(graph_builder.g)))\n",
    "    h = norm(h)\n",
    "    labels_hiddens = h[label_idx]\n",
    "    \n",
    "    loss = loss_func(labels_hiddens)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if best is None or loss.item() < best:\n",
    "        best = loss.item()\n",
    "        best_model = gat\n",
    "        h_best = h.detach().cpu()\n",
    "    \n",
    "    print(f\"epoch: {e}, loss: {loss.item()-1:.6} min_loss: {best-1:.6}\")\n",
    "    del labels_hiddens, loss, h\n",
    "    if (best-1.) < -0.8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_best[label_idx] # Embedding dos labels (Distâncias maximizadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # embedding dos termos novos\n",
    "h_best[len(label_idx):] # o mapeamento é (o inverso de) node_mapper[big_graph.ndata['idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_best[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_graph_dgl.ndata['emb'][100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdfgsrvsdtvhsfhvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variância média DAS hidden dimensions dos labels &\n",
    "#     -> O quanto as hidden estão variando entre as labels\n",
    "#        Quanto maior esse valor, mais \"diferente\" são as representações dos LABELS\n",
    "# Variância média ENTRE A MÉDIA DAS hidden dimensions dos labels\n",
    "#     -> O quanto as hiddens estão variando entre si\n",
    "#        Quanto maior esse valor, mais \"diferente\" são as representações das DIMENSÕES\n",
    "#        Ou seja, VAR(AVG(h1), AVG(h2), ..., AVG(hn)) é alto\n",
    "h_best[label_idx].std(axis=0).mean(), h_best[label_idx].mean(axis=0).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( h_best[label_idx].mean(axis=0).detach().cpu().numpy(), bins=nbins )\n",
    "plt.title( 'Labels\\n[VAR(h1), VAR(h2), ..., VAR(hn)]' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(verbose=10, metric='cosine')\n",
    "X = tsne.fit_transform(h_best.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = X.T\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(x[len(label_idx):],y[len(label_idx):], linewidths= 0.0, s=25, alpha=0.025)\n",
    "plt.scatter(x[label_idx.cpu().numpy()],y[label_idx.cpu().numpy()], marker='x')\n",
    "plt.xlim( (x.min()-5, x.max()+5) )\n",
    "plt.ylim( (y.min()-5, y.max()+5) )\n",
    "plt.title(f'{dataset.dname} - heads={n_heads}')\n",
    "plt.savefig(f'{dataset.dname}-heads={n_heads}_diff.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(verbose=10)\n",
    "X2 = tsne.fit_transform(big_graph_dgl.ndata['emb'][len(label_idx):].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = X2.T\n",
    "plt.scatter(x, y, linewidths= 0.0, alpha=0.01)\n",
    "plt.xlim( (x.min()-5, x.max()+5) )\n",
    "plt.ylim( (y.min()-5, y.max()+5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[label_idx.cpu().numpy()],y[label_idx.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( h_best[len(label_idx):].std(axis=0).detach().cpu().numpy(), bins=nbins )\n",
    "plt.title( 'Terms\\n[VAR(h1), VAR(h2), ..., VAR(hn)]' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( h_best[label_idx].mean(axis=0).detach().cpu().numpy(), bins=nbins )\n",
    "plt.title( 'Labels\\n[AVG(h1), AVG(h2), ..., AVG(hn)]' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( h_best[len(label_idx):].mean(axis=0).detach().cpu().numpy(), bins=nbins )\n",
    "plt.title( 'Terms\\n[AVG(h1), AVG(h2), ..., AVG(hn)]' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = big_graph_dgl.ndata['label'].nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = gat(big_graph_dgl)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hiddens = h[label_idx]\n",
    "labels_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "B = []\n",
    "y = []\n",
    "for i in range(labels_hiddens.size()[0]):\n",
    "    for j in range(labels_hiddens.size()[0]):\n",
    "        if i != j:\n",
    "            A.append( labels_hiddens[i] )\n",
    "            B.append( -1.*labels_hiddens[j] )\n",
    "            y.append( 1 )\n",
    "            \n",
    "B=torch.cat(B).reshape( len(y), 300 )\n",
    "A=torch.cat(A).reshape( len(y), 300 )\n",
    "y=torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(B, A, target=y), loss_func(A, B, target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_class,\n",
    "                  n_heads=8, drop=.5, attn_drop=.5,\n",
    "                  device='cuda:0'):\n",
    "        super(TGA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.device = torch.device(device)\n",
    "        self.gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                                     encoders={'label'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        \n",
    "        self.gat_local  = GenericGAT(hidden_dim, hidden_dim, \n",
    "                                     encoders={'term'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     first_hidden='emb',\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "\n",
    "        self.lin = nn.Linear( hidden_dim, 1).to(self.device)\n",
    "        # Depois tentar alguma ativação (ReLU, por exemplo, pode \"desativar\" alguns termos no softmax)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear( hidden_dim, hidden_dim//2).to(self.device)\n",
    "        self.fc2 = nn.Linear( hidden_dim//2, hidden_dim//4).to(self.device)\n",
    "        self.fc3 = nn.Linear( hidden_dim//4, self.n_class).to(self.device)\n",
    "    def forward(self, G, gs):\n",
    "        #h_global           = self.gat_global( G, label=G.ndata['label'].nonzero().flatten() )\n",
    "        #gs.ndata['weight'] = h_global[ gs.ndata['idx'] ] # Tentar concatenando\n",
    "        h_local            = self.gat_local(gs, term=None)\n",
    "        #h_local            = torch.cat((h_local, h_global[ gs.ndata['idx'] ]), 1)\n",
    "        h_local            = self.pooling( gs, h_local )\n",
    "        h_local            = self.fc1( h_local )\n",
    "        h_local            = self.fc2( h_local )\n",
    "        h_local            = self.fc3( h_local )\n",
    "        return h_local\n",
    "# torch.Size([3652, 300]) torch.Size([3652, 300]) torch.Size([128, 300])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=300\n",
    "hidden_dim=2\n",
    "n_heads=8\n",
    "drop=0.3\n",
    "attn_drop=0.5\n",
    "batch_size=128\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TGA( in_dim, hidden_dim, graph_builder.n_class,\n",
    "            n_heads=n_heads, drop=drop, attn_drop=attn_drop )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_nodes_from( [ (0, {'idx': 0}), (1, {'idx': 1}), (2, {'idx': 2}) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.get_node_attributes(g,'idx').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hiddens = torch.eye( 11 )\n",
    "labels_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
