{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import repeat\n",
    "\n",
    "from collections import namedtuple\n",
    "from os import path, remove\n",
    "import io\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "replace_patterns = [\n",
    "    ('<[^>]*>', ''),                                    # remove HTML tags\n",
    "    ('(\\D)\\d\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D\\D)\\d\\d\\d\\D\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedZipcodePlusFour \\\\2'),\n",
    "    ('(\\D)\\d(\\D)', '\\\\1ParsedOneDigit\\\\2'),\n",
    "    ('(\\D)\\d\\d(\\D)', '\\\\1ParsedTwoDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d(\\D)', '\\\\1ParsedThreeDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d(\\D)', '\\\\1ParsedFourDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedFiveDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedSixDigits\\\\2'),\n",
    "    ('\\d+', 'ParsedDigits')\n",
    "]\n",
    "\n",
    "compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "\n",
    "def generate_preprocessor(replace_patterns):\n",
    "    compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "    def preprocessor(text):\n",
    "        # For each pattern, replace it with the appropriate string\n",
    "        for pattern, replace in compiled_replace_patterns:\n",
    "            text = re.sub(pattern, replace, text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    return preprocessor\n",
    "\n",
    "generated_patters=generate_preprocessor(replace_patterns)\n",
    "\n",
    "def preprocessor(text):\n",
    "    # For each pattern, replace it with the appropriate string\n",
    "    for pattern, replace in compiled_replace_patterns:\n",
    "        text = re.sub(pattern, replace, text)\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphsizePretrained(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, w=2, pretrained_vec='glove.6B.100d', verbose=False):\n",
    "        self.w = w\n",
    "        self.pretrained_vec = pretrained_vec\n",
    "        self.embeddings_dict = {}\n",
    "        \n",
    "        if not verbose:\n",
    "            self.progress_bar = lambda x: x\n",
    "        else:\n",
    "            from tqdm import tqdm\n",
    "            self.progress_bar = tqdm\n",
    "            \n",
    "        with open(self.pretrained_vec, 'r') as f:\n",
    "            for line in self.progress_bar(f):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                self.ndim = len(vector)\n",
    "                self.embeddings_dict[word] = vector\n",
    "        self.vocab = { word: i for (i,word) in enumerate( self.embeddings_dict.keys() ) }\n",
    "        \n",
    "        self.analyzer = TfidfVectorizer(preprocessor=preprocessor)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.N = len(X)\n",
    "        return self\n",
    "   \n",
    "    def transform(self, text):\n",
    "        docs = list(map(self.analyzer.build_analyzer(), self.progress_bar(text)))\n",
    "        result = list(map(self._build_graph_, self.progress_bar(docs)))\n",
    "        return result\n",
    "    \n",
    "    def _build_graph_(self, doc):\n",
    "        terms    = list(filter( lambda x: x in self.embeddings_dict, doc))\n",
    "        sorted_terms = sorted(list(set(terms)))\n",
    "\n",
    "        cooccur_count = Counter()\n",
    "        for i,idt in enumerate(terms):\n",
    "            terms_to_add = terms[ max(i-self.w, 0):i ]\n",
    "            terms_to_add = list(zip(terms_to_add, repeat(idt)))\n",
    "            terms_to_add = list(map(sorted,terms_to_add))\n",
    "            terms_to_add = list(map(tuple,terms_to_add))\n",
    "            cooccur_count.update( terms_to_add )\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from( sorted_terms )\n",
    "        w_edges = [ (s,t,w) for ((s,t),w) in cooccur_count.items() ]\n",
    "        G.add_weighted_edges_from( w_edges, weight='freq' )\n",
    "        \n",
    "        return G, np.array([ self.embeddings_dict[term] for term in sorted_terms ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from time import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "webkb = Dataset('/home/mangaravite/Documents/datasets/topics/webkb/')\n",
    "reut  = Dataset('/home/mangaravite/Documents/datasets/topics/reut/')\n",
    "acm   = Dataset('/home/mangaravite/Documents/datasets/topics/acm/')\n",
    "_20ng   = Dataset('/home/mangaravite/Documents/datasets/topics/20ng/')\n",
    "\n",
    "dataset = acm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = next(dataset.get_fold_instances(10))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:20, 19409.48it/s]\n",
      "100%|██████████| 19907/19907 [00:05<00:00, 3673.38it/s]\n",
      "100%|██████████| 19907/19907 [00:12<00:00, 1546.86it/s]\n",
      "100%|██████████| 2495/2495 [00:00<00:00, 3902.91it/s]\n",
      "100%|██████████| 2495/2495 [00:01<00:00, 1818.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.1 s, sys: 1.58 s, total: 41.7 s\n",
      "Wall time: 41 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=5, verbose=True, pretrained_vec='/home/mangaravite/Documents/pretrained_vectors/glove.6B.300d.txt')\n",
    "Gs_train = graph_builder.fit_transform(fold.X_train)\n",
    "Gs_val   = graph_builder.transform(fold.X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=.5):\n",
    "        super(ClassifierGCN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(2*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( 2*hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear(2*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((h,he), 1)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred\n",
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, n_heads=16, drop=.5, attn_drop=.5, device='cuda:0'):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device(device))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device)),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(n_heads*hidden_dim + hidden_dim, 1).to(torch.device(device))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device(device))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim + hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear( n_heads*hidden_dim + hidden_dim, n_classes).to(torch.device(device))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((h,he), 1)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    Gs_Fs, labels = map(list, zip(*samples))\n",
    "    graphs = []\n",
    "    for g, f in Gs_Fs:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g)\n",
    "        g_dgl.ndata['f'] = torch.FloatTensor(f).to(torch.device('cuda:0'))\n",
    "        g_dgl.to(torch.device('cuda:0'))\n",
    "        graphs.append(g_dgl)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.to(torch.device('cuda:0'))\n",
    "    labels = torch.tensor(labels).to(torch.device('cuda:0'))\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    # https://github.com/mbsariyildiz/focal-loss.pytorch\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'best_param_pretrained_wekb.pth'\n",
    "n_heads=4\n",
    "n_epochs = 100\n",
    "patience = 25\n",
    "hidden_dim = 300\n",
    "train_batch_size = 32\n",
    "test_val_batch_size = 256\n",
    "\n",
    "#model = SimpleClassifierGCN(len(graph_builder.vocab), hidden_dim, dataset.nclass, drop=.5).to(torch.device('cuda:0'))\n",
    "model = ClassifierGAT(graph_builder.ndim, hidden_dim, dataset.nclass, n_heads=n_heads, drop=.5, attn_drop=.3).to(torch.device('cuda:0'))\n",
    "#model = ClassifierGCN(graph_builder.ndim, hidden_dim, dataset.nclass, drop=.5).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_func = FocalLoss().to(torch.device('cuda:0'))\n",
    "loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()\n",
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val  = DataLoader(list(zip(Gs_val,  fold.y_val )), batch_size=test_val_batch_size,\n",
    "                              shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter 0, train acc 0.584 train loss 840.94: 100%|██████████| 19907/19907 [01:05<00:00, 305.10it/s]\n",
      "iter 0, val   acc 0.657 ( over: 1.12/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 380.70it/s]\n",
      "iter 1, train acc 0.654 train loss 343.60: 100%|██████████| 19907/19907 [01:05<00:00, 304.25it/s]\n",
      "iter 1, val   acc 0.675 ( over: 1.03/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 383.00it/s]\n",
      "iter 2, train acc 0.674 train loss 214.53: 100%|██████████| 19907/19907 [01:05<00:00, 306.00it/s]\n",
      "iter 2, val   acc 0.678 ( over: 1.01/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 385.53it/s]\n",
      "iter 3, train acc 0.688 train loss 153.24: 100%|██████████| 19907/19907 [01:04<00:00, 307.96it/s]\n",
      "iter 3, val   acc 0.688 ( over: 1.0/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 360.45it/s]\n",
      "iter 4, train acc 0.697 train loss 120.40: 100%|██████████| 19907/19907 [01:04<00:00, 307.21it/s]\n",
      "iter 4, val   acc 0.693 ( over: 0.995/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 361.85it/s]\n",
      "iter 5, train acc 0.696 train loss 98.20: 100%|██████████| 19907/19907 [01:04<00:00, 307.73it/s]\n",
      "iter 5, val   acc 0.695 ( over: 0.998/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 361.28it/s]\n",
      "iter 6, train acc 0.701 train loss 83.33: 100%|██████████| 19907/19907 [01:04<00:00, 309.43it/s]\n",
      "iter 6, val   acc 0.684 ( over: 0.975/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 367.28it/s]\n",
      "iter 7, train acc 0.707 train loss 72.36: 100%|██████████| 19907/19907 [01:04<00:00, 309.90it/s]\n",
      "iter 7, val   acc 0.696 ( over: 0.985/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 359.53it/s]\n",
      "iter 8, train acc 0.707 train loss 64.14: 100%|██████████| 19907/19907 [01:04<00:00, 308.50it/s]\n",
      "iter 8, val   acc 0.687 ( over: 0.971/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 365.05it/s]\n",
      "iter 9, train acc 0.706 train loss 57.75: 100%|██████████| 19907/19907 [01:04<00:00, 309.90it/s]\n",
      "iter 9, val   acc 0.701 ( over: 0.993/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 387.22it/s]\n",
      "iter 10, train acc 0.707 train loss 51.93: 100%|██████████| 19907/19907 [01:04<00:00, 309.97it/s]\n",
      "iter 10, val   acc 0.695 ( over: 0.983/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 391.43it/s]\n",
      "iter 11, train acc 0.710 train loss 47.25: 100%|██████████| 19907/19907 [01:05<00:00, 305.98it/s]\n",
      "iter 11, val   acc 0.693 ( over: 0.977/2 ): 100%|██████████| 2495/2495 [00:06<00:00, 387.32it/s]\n",
      "iter 12, train acc 0.713 train loss 43.48: 100%|██████████| 19907/19907 [01:05<00:00, 303.63it/s]\n",
      "iter 12, val   acc 0.704 ( over: 0.988/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 384.10it/s]\n",
      "iter 13, train acc 0.714 train loss 40.13: 100%|██████████| 19907/19907 [01:05<00:00, 304.01it/s]\n",
      "iter 13, val   acc 0.712 ( over: 0.997/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 385.87it/s]\n",
      "iter 14, train acc 0.715 train loss 37.62: 100%|██████████| 19907/19907 [01:04<00:00, 306.63it/s]\n",
      "iter 14, val   acc 0.697 ( over: 0.975/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 391.55it/s]\n",
      "iter 15, train acc 0.715 train loss 34.98: 100%|██████████| 19907/19907 [01:04<00:00, 308.21it/s]\n",
      "iter 15, val   acc 0.712 ( over: 0.996/2 ): 100%|██████████| 2495/2495 [00:06<00:00, 390.57it/s]\n",
      "iter 16, train acc 0.717 train loss 32.97: 100%|██████████| 19907/19907 [01:04<00:00, 309.90it/s]\n",
      "iter 16, val   acc 0.705 ( over: 0.984/3 ): 100%|██████████| 2495/2495 [00:06<00:00, 364.34it/s]\n",
      "iter 17, train acc 0.714 train loss 30.95: 100%|██████████| 19907/19907 [01:04<00:00, 309.41it/s]\n",
      "iter 17, val   acc 0.702 ( over: 0.983/4 ): 100%|██████████| 2495/2495 [00:06<00:00, 364.71it/s]\n",
      "iter 18, train acc 0.714 train loss 29.42: 100%|██████████| 19907/19907 [01:04<00:00, 306.90it/s]\n",
      "iter 18, val   acc 0.705 ( over: 0.987/5 ): 100%|██████████| 2495/2495 [00:06<00:00, 365.36it/s]\n",
      "iter 19, train acc 0.718 train loss 27.67: 100%|██████████| 19907/19907 [01:05<00:00, 305.50it/s]\n",
      "iter 19, val   acc 0.709 ( over: 0.987/6 ): 100%|██████████| 2495/2495 [00:06<00:00, 359.05it/s]\n",
      "iter 20, train acc 0.719 train loss 26.34: 100%|██████████| 19907/19907 [01:04<00:00, 307.20it/s]\n",
      "iter 20, val   acc 0.701 ( over: 0.975/7 ): 100%|██████████| 2495/2495 [00:06<00:00, 361.65it/s]\n",
      "iter 21, train acc 0.723 train loss 25.04: 100%|██████████| 19907/19907 [01:04<00:00, 306.68it/s]\n",
      "iter 21, val   acc 0.691 ( over: 0.957/8 ): 100%|██████████| 2495/2495 [00:06<00:00, 362.50it/s]\n",
      "iter 22, train acc 0.720 train loss 23.93: 100%|██████████| 19907/19907 [01:04<00:00, 307.53it/s]\n",
      "iter 22, val   acc 0.708 ( over: 0.983/9 ): 100%|██████████| 2495/2495 [00:06<00:00, 365.34it/s]\n",
      "iter 23, train acc 0.720 train loss 22.95: 100%|██████████| 19907/19907 [01:04<00:00, 307.80it/s]\n",
      "iter 23, val   acc 0.700 ( over: 0.972/10 ): 100%|██████████| 2495/2495 [00:06<00:00, 386.69it/s]\n",
      "iter 24, train acc 0.722 train loss 22.04: 100%|██████████| 19907/19907 [01:05<00:00, 304.46it/s]\n",
      "iter 24, val   acc 0.711 ( over: 0.985/11 ): 100%|██████████| 2495/2495 [00:06<00:00, 386.08it/s]\n",
      "iter 25, train acc 0.724 train loss 21.11: 100%|██████████| 19907/19907 [01:05<00:00, 305.85it/s]\n",
      "iter 25, val   acc 0.707 ( over: 0.977/12 ): 100%|██████████| 2495/2495 [00:06<00:00, 389.48it/s]\n",
      "iter 26, train acc 0.722 train loss 20.15: 100%|██████████| 19907/19907 [01:05<00:00, 304.95it/s]\n",
      "iter 26, val   acc 0.715 ( over: 0.99/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 384.38it/s]\n",
      "iter 27, train acc 0.723 train loss 19.45: 100%|██████████| 19907/19907 [01:05<00:00, 305.68it/s]\n",
      "iter 27, val   acc 0.708 ( over: 0.98/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 389.03it/s]\n",
      "iter 28, train acc 0.720 train loss 18.81: 100%|██████████| 19907/19907 [01:05<00:00, 305.98it/s]\n",
      "iter 28, val   acc 0.713 ( over: 0.99/2 ): 100%|██████████| 2495/2495 [00:06<00:00, 389.44it/s]\n",
      "iter 29, train acc 0.719 train loss 18.26: 100%|██████████| 19907/19907 [01:04<00:00, 308.85it/s]\n",
      "iter 29, val   acc 0.717 ( over: 0.998/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 362.57it/s]\n",
      "iter 30, train acc 0.723 train loss 17.55: 100%|██████████| 19907/19907 [01:04<00:00, 309.49it/s]\n",
      "iter 30, val   acc 0.705 ( over: 0.974/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 367.86it/s]\n",
      "iter 31, train acc 0.725 train loss 16.99: 100%|██████████| 19907/19907 [01:04<00:00, 306.32it/s]\n",
      "iter 31, val   acc 0.707 ( over: 0.975/2 ): 100%|██████████| 2495/2495 [00:06<00:00, 366.64it/s]\n",
      "iter 32, train acc 0.722 train loss 16.53: 100%|██████████| 19907/19907 [01:04<00:00, 308.84it/s]\n",
      "iter 32, val   acc 0.706 ( over: 0.978/3 ): 100%|██████████| 2495/2495 [00:06<00:00, 363.16it/s]\n",
      "iter 33, train acc 0.724 train loss 16.01: 100%|██████████| 19907/19907 [01:04<00:00, 309.20it/s]\n",
      "iter 33, val   acc 0.715 ( over: 0.988/4 ): 100%|██████████| 2495/2495 [00:06<00:00, 362.77it/s]\n",
      "iter 34, train acc 0.725 train loss 15.45: 100%|██████████| 19907/19907 [01:04<00:00, 310.75it/s]\n",
      "iter 34, val   acc 0.717 ( over: 0.989/5 ): 100%|██████████| 2495/2495 [00:06<00:00, 389.35it/s]\n",
      "iter 35, train acc 0.729 train loss 14.95: 100%|██████████| 19907/19907 [01:05<00:00, 303.64it/s]\n",
      "iter 35, val   acc 0.711 ( over: 0.976/6 ): 100%|██████████| 2495/2495 [00:06<00:00, 386.81it/s]\n",
      "iter 36, train acc 0.725 train loss 14.52: 100%|██████████| 19907/19907 [01:05<00:00, 303.88it/s]\n",
      "iter 36, val   acc 0.708 ( over: 0.976/7 ): 100%|██████████| 2495/2495 [00:06<00:00, 388.43it/s]\n",
      "iter 37, train acc 0.727 train loss 14.20: 100%|██████████| 19907/19907 [01:05<00:00, 304.74it/s]\n",
      "iter 37, val   acc 0.705 ( over: 0.97/8 ): 100%|██████████| 2495/2495 [00:06<00:00, 382.18it/s]\n",
      "iter 38, train acc 0.725 train loss 13.88: 100%|██████████| 19907/19907 [01:05<00:00, 304.71it/s]\n",
      "iter 38, val   acc 0.712 ( over: 0.981/9 ): 100%|██████████| 2495/2495 [00:06<00:00, 390.40it/s]\n",
      "iter 39, train acc 0.726 train loss 13.42: 100%|██████████| 19907/19907 [01:05<00:00, 305.54it/s]\n",
      "iter 39, val   acc 0.711 ( over: 0.981/10 ): 100%|██████████| 2495/2495 [00:06<00:00, 385.25it/s]\n",
      "iter 40, train acc 0.724 train loss 13.15: 100%|██████████| 19907/19907 [01:04<00:00, 306.43it/s]\n",
      "iter 40, val   acc 0.709 ( over: 0.979/11 ): 100%|██████████| 2495/2495 [00:06<00:00, 363.22it/s]\n",
      "iter 41, train acc 0.729 train loss 12.75: 100%|██████████| 19907/19907 [01:04<00:00, 307.34it/s]\n",
      "iter 41, val   acc 0.706 ( over: 0.968/12 ): 100%|██████████| 2495/2495 [00:06<00:00, 364.67it/s]\n",
      "iter 42, train acc 0.722 train loss 12.44: 100%|██████████| 19907/19907 [01:04<00:00, 308.04it/s]\n",
      "iter 42, val   acc 0.706 ( over: 0.978/13 ): 100%|██████████| 2495/2495 [00:06<00:00, 363.96it/s]\n",
      "iter 43, train acc 0.732 train loss 12.08: 100%|██████████| 19907/19907 [01:04<00:00, 309.54it/s]\n",
      "iter 43, val   acc 0.715 ( over: 0.976/14 ): 100%|██████████| 2495/2495 [00:06<00:00, 364.74it/s]\n",
      "iter 44, train acc 0.728 train loss 11.86: 100%|██████████| 19907/19907 [01:04<00:00, 307.25it/s]\n",
      "iter 44, val   acc 0.712 ( over: 0.978/15 ): 100%|██████████| 2495/2495 [00:06<00:00, 387.59it/s]\n",
      "iter 45, train acc 0.728 train loss 11.66: 100%|██████████| 19907/19907 [01:04<00:00, 308.49it/s]\n",
      "iter 45, val   acc 0.714 ( over: 0.981/16 ): 100%|██████████| 2495/2495 [00:06<00:00, 388.78it/s]\n",
      "iter 46, train acc 0.731 train loss 11.25: 100%|██████████| 19907/19907 [01:04<00:00, 306.55it/s]\n",
      "iter 46, val   acc 0.714 ( over: 0.976/17 ): 100%|██████████| 2495/2495 [00:06<00:00, 388.56it/s]\n",
      "iter 47, train acc 0.727 train loss 11.15: 100%|██████████| 19907/19907 [01:05<00:00, 303.06it/s]\n",
      "iter 47, val   acc 0.719 ( over: 0.99/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 378.88it/s]\n",
      "iter 48, train acc 0.731 train loss 10.88: 100%|██████████| 19907/19907 [01:05<00:00, 302.20it/s]\n",
      "iter 48, val   acc 0.717 ( over: 0.982/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 386.92it/s]\n",
      "iter 49, train acc 0.728 train loss 10.66: 100%|██████████| 19907/19907 [01:05<00:00, 305.31it/s]\n",
      "iter 49, val   acc 0.721 ( over: 0.991/0 ): 100%|██████████| 2495/2495 [00:06<00:00, 384.66it/s]\n",
      "iter 50, train acc 0.730 train loss 10.38: 100%|██████████| 19907/19907 [01:05<00:00, 305.82it/s]\n",
      "iter 50, val   acc 0.711 ( over: 0.973/1 ): 100%|██████████| 2495/2495 [00:06<00:00, 386.72it/s]\n",
      "iter 51, train acc 0.728 train loss 10.29: 100%|██████████| 19907/19907 [01:04<00:00, 306.72it/s]\n",
      "iter 51, val   acc 0.701 ( over: 0.962/2 ): 100%|██████████| 2495/2495 [00:06<00:00, 359.64it/s]\n",
      "iter 52, train acc 0.727 train loss 10.04: 100%|██████████| 19907/19907 [01:04<00:00, 308.15it/s]\n",
      "iter 52, val   acc 0.716 ( over: 0.985/3 ): 100%|██████████| 2495/2495 [00:06<00:00, 359.55it/s]\n",
      "iter 53, train acc 0.726 train loss 9.87: 100%|██████████| 19907/19907 [01:05<00:00, 305.52it/s]\n",
      "iter 53, val   acc 0.707 ( over: 0.973/4 ): 100%|██████████| 2495/2495 [00:06<00:00, 358.33it/s]\n",
      "iter 54, train acc 0.729 train loss 9.68: 100%|██████████| 19907/19907 [01:05<00:00, 306.12it/s]\n",
      "iter 54, val   acc 0.710 ( over: 0.974/5 ): 100%|██████████| 2495/2495 [00:06<00:00, 360.17it/s]\n",
      "iter 55, train acc 0.730 train loss 9.48: 100%|██████████| 19907/19907 [01:04<00:00, 306.81it/s]\n",
      "iter 55, val   acc 0.708 ( over: 0.97/6 ): 100%|██████████| 2495/2495 [00:06<00:00, 364.08it/s]\n",
      "iter 56, train acc 0.732 train loss 9.26: 100%|██████████| 19907/19907 [01:04<00:00, 308.95it/s]\n",
      "iter 56, val   acc 0.709 ( over: 0.969/7 ): 100%|██████████| 2495/2495 [00:06<00:00, 389.41it/s]\n",
      "iter 57, train acc 0.729 train loss 9.13: 100%|██████████| 19907/19907 [01:04<00:00, 306.36it/s]\n",
      "iter 57, val   acc 0.713 ( over: 0.978/8 ): 100%|██████████| 2495/2495 [00:06<00:00, 379.78it/s]\n",
      "iter 58, train acc 0.729 train loss 8.96: 100%|██████████| 19907/19907 [01:05<00:00, 303.53it/s]\n",
      "iter 58, val   acc 0.704 ( over: 0.965/9 ): 100%|██████████| 2495/2495 [00:06<00:00, 385.48it/s]\n",
      "iter 59, train acc 0.732 train loss 8.81: 100%|██████████| 19907/19907 [01:06<00:00, 301.34it/s]\n",
      "iter 59, val   acc 0.711 ( over: 0.971/10 ): 100%|██████████| 2495/2495 [00:06<00:00, 385.12it/s]\n",
      "iter 60, train acc 0.733 train loss 8.68: 100%|██████████| 19907/19907 [01:05<00:00, 303.49it/s]\n",
      "iter 60, val   acc 0.709 ( over: 0.968/11 ): 100%|██████████| 2495/2495 [00:06<00:00, 386.52it/s]\n",
      "iter 61, train acc 0.731 train loss 8.57: 100%|██████████| 19907/19907 [01:04<00:00, 308.00it/s]\n",
      "iter 61, val   acc 0.711 ( over: 0.973/12 ): 100%|██████████| 2495/2495 [00:06<00:00, 358.42it/s]\n",
      "iter 62, train acc 0.728 train loss 8.43: 100%|██████████| 19907/19907 [01:04<00:00, 306.94it/s]\n",
      "iter 62, val   acc 0.719 ( over: 0.988/13 ): 100%|██████████| 2495/2495 [00:06<00:00, 360.13it/s]\n",
      "iter 63, train acc 0.733 train loss 8.19: 100%|██████████| 19907/19907 [01:04<00:00, 308.00it/s]\n",
      "iter 63, val   acc 0.709 ( over: 0.967/14 ): 100%|██████████| 2495/2495 [00:06<00:00, 361.45it/s]\n",
      "iter 64, train acc 0.730 train loss 8.11: 100%|██████████| 19907/19907 [01:04<00:00, 308.31it/s]\n",
      "iter 64, val   acc 0.713 ( over: 0.976/15 ): 100%|██████████| 2495/2495 [00:06<00:00, 363.95it/s]\n",
      "iter 65, train acc 0.733 train loss 7.99: 100%|██████████| 19907/19907 [01:04<00:00, 308.25it/s]\n",
      "iter 65, val   acc 0.718 ( over: 0.979/16 ): 100%|██████████| 2495/2495 [00:06<00:00, 363.91it/s]\n",
      "iter 66, train acc 0.728 train loss 7.86: 100%|██████████| 19907/19907 [01:04<00:00, 308.17it/s]\n",
      "iter 66, val   acc 0.710 ( over: 0.975/17 ): 100%|██████████| 2495/2495 [00:06<00:00, 362.54it/s]\n",
      "iter 67, train acc 0.731 train loss 7.75: 100%|██████████| 19907/19907 [01:04<00:00, 308.66it/s]\n",
      "iter 67, val   acc 0.714 ( over: 0.977/18 ): 100%|██████████| 2495/2495 [00:06<00:00, 392.39it/s]\n",
      "iter 68, train acc 0.736 train loss 7.59: 100%|██████████| 19907/19907 [01:05<00:00, 305.25it/s]\n",
      "iter 68, val   acc 0.716 ( over: 0.973/19 ): 100%|██████████| 2495/2495 [00:06<00:00, 387.73it/s]\n",
      "iter 69, train acc 0.732 train loss 7.50: 100%|██████████| 19907/19907 [01:05<00:00, 305.19it/s]\n",
      "iter 69, val   acc 0.712 ( over: 0.973/20 ): 100%|██████████| 2495/2495 [00:06<00:00, 389.10it/s]\n",
      "iter 70, train acc 0.733 train loss 7.42: 100%|██████████| 19907/19907 [01:05<00:00, 305.12it/s]\n",
      "iter 70, val   acc 0.717 ( over: 0.979/21 ): 100%|██████████| 2495/2495 [00:06<00:00, 388.04it/s]\n",
      "iter 71, train acc 0.734 train loss 7.29: 100%|██████████| 19907/19907 [01:05<00:00, 305.60it/s]\n",
      "iter 71, val   acc 0.715 ( over: 0.975/22 ): 100%|██████████| 2495/2495 [00:06<00:00, 387.69it/s]\n",
      "iter 72, train acc 0.738 train loss 7.16: 100%|██████████| 19907/19907 [01:04<00:00, 306.90it/s]\n",
      "iter 72, val   acc 0.713 ( over: 0.967/23 ): 100%|██████████| 2495/2495 [00:06<00:00, 390.42it/s]\n",
      "iter 73, train acc 0.733 train loss 7.05: 100%|██████████| 19907/19907 [01:05<00:00, 303.98it/s]\n",
      "iter 73, val   acc 0.709 ( over: 0.968/24 ): 100%|██████████| 2495/2495 [00:06<00:00, 387.66it/s]\n",
      "iter 74, train acc 0.732 train loss 7.01: 100%|██████████| 19907/19907 [01:04<00:00, 306.41it/s]\n",
      "iter 74, val  acc 0.713 ( over: 0.974/24 ): 100%|██████████| 2495/2495 [00:06<00:00, 359.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST val acc 0.721\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "n_iters = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(list(zip(Gs_train, fold.y_train)), batch_size=train_batch_size,\n",
    "                             shuffle=True, collate_fn=collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for i, (bg, label) in enumerate(data_loader):\n",
    "            outputs = model(bg)\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            # Train eval phase\n",
    "            total += label.size(0)\n",
    "            correct += (sampled_Y == label).sum().item()\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            del loss, outputs, bg, probs_Y, sampled_Y\n",
    "            pbar.update( len(label) )\n",
    "            pbar.set_description_str('iter {}, train acc {:.3f} train loss {:.2f}'.format(epoch, (correct/total), epoch_loss / (epoch + 1)))\n",
    "        \n",
    "        score_train = correct/total\n",
    "    with tqdm(total=len(data_loader_val.dataset), smoothing=0.) as pbar:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.\n",
    "        for bg, label in data_loader_val:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(bg)\n",
    "            \n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "\n",
    "            # Validation eval phase\n",
    "            total += label.size(0)\n",
    "            correct += (sampled_Y == label).sum().item()\n",
    "            \n",
    "            #break\n",
    "            \n",
    "            del probs_Y, outputs, bg, sampled_Y\n",
    "            pbar.update( label.size(0) )\n",
    "            score_val = correct/total\n",
    "\n",
    "            pbar.set_description_str('iter {}, val   acc {:.3f} ( over: {:.3} )'.format(epoch, score_val, score_val/score_train))\n",
    "            \n",
    "        #break\n",
    "        pbar.set_description_str('iter {}, val  acc {:.3f} ( over: {:.3}/{} )'.format(epoch, score_val, score_val/score_train, n_iters))\n",
    "        score = correct/total\n",
    "        if best_score is None or score > best_score:\n",
    "            torch.save(model, PATH)\n",
    "            best_score = score\n",
    "            n_iters = 0\n",
    "        else:\n",
    "            n_iters += 1\n",
    "            if n_iters >= patience:\n",
    "                print()\n",
    "                print('BEST val acc {:.3f}'.format(best_score), end='\\r')\n",
    "                break\n",
    "        pbar.set_description_str('iter {}, val   acc {:.3f} ( over: {:.3}/{} )'.format(epoch, score_val, score_val/score_train, n_iters))\n",
    "        epoch_loss /= (epoch + 1)\n",
    "        epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### acm ######################################\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.721 10folds\n",
    "#       CrossEntropyLoss, Adam\n",
    "#       n_heads=4, drop=.5, attn_drop=.3\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-3,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = _\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 32\n",
    "#       test_val_batch_size = 256\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.708 iter=20 10folds\n",
    "#       CrossEntropyLoss, Adam\n",
    "#       n_heads=8, drop=.3, attn_drop=.2\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-3,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = _\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 16\n",
    "#       test_val_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### reut ######################################\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.753(/0.747) iter=24 10folds (aparente underfitting) \n",
    "#       CrossEntropyLoss, Adam\n",
    "#       n_heads=4, drop=.5, attn_drop=.3\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-3,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = _\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 16\n",
    "#       test_val_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### webkb ######################################\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.820 iter=15 10folds\n",
    "#       CrossEntropyLoss, Adam\n",
    "#       n_heads=4, drop=.5, attn_drop=.3\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-3,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = 25\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 16\n",
    "#       test_val_batch_size = 256\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.815 10folds (aparente underfitting)\n",
    "#       CrossEntropyLoss, Adam\n",
    "#       n_heads=4, drop=.5, attn_drop=.3\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-3,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = 12\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 16\n",
    "#       test_val_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### 20ng ######################################\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.764 iter=27 10folds\n",
    "#       FocalLoss, Adam\n",
    "#       n_heads=16, drop=.2, attn_drop=.1\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-4,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = _\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 16\n",
    "#       test_val_batch_size = 256\n",
    "###################____________________ GAT ____________________###################\n",
    "# BEST val acc 0.767 iter=31 10folds\n",
    "#       CrossEntropyLoss, Adam\n",
    "#       n_heads=8, drop=.2, attn_drop=.1\n",
    "#       lr=1e-3, w=5\n",
    "#       weight_decay=1e-4,\n",
    "#       PATH = 'best_param_pretrained_wekb.pth'\n",
    "#       n_epochs = 100\n",
    "#       patience = _\n",
    "#       hidden_dim = 300\n",
    "#       train_batch_size = 16\n",
    "#       test_val_batch_size = 256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
