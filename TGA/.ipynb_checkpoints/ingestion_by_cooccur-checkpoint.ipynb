{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "import scipy.stats as st\n",
    "import io\n",
    "from glob import glob\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from os import path\n",
    "import torch\n",
    "from multiprocessing import Pool\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from collections import namedtuple \n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# show zumbies nvidia executions: sudo fuser -v /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_patterns = [\n",
    "    ('<[^>]*>', ''),                                    # remove HTML tags\n",
    "    ('(\\D)\\d\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D\\D)\\d\\d\\d\\D\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedZipcodePlusFour \\\\2'),\n",
    "    ('(\\D)\\d(\\D)', '\\\\1ParsedOneDigit\\\\2'),\n",
    "    ('(\\D)\\d\\d(\\D)', '\\\\1ParsedTwoDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d(\\D)', '\\\\1ParsedThreeDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d(\\D)', '\\\\1ParsedFourDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedFiveDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedSixDigits\\\\2'),\n",
    "    ('\\d+', 'ParsedDigits')\n",
    "]\n",
    "\n",
    "compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "\n",
    "def generate_preprocessor(replace_patterns):\n",
    "    compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "    def preprocessor(text):\n",
    "        # For each pattern, replace it with the appropriate string\n",
    "        for pattern, replace in compiled_replace_patterns:\n",
    "            text = re.sub(pattern, replace, text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    return preprocessor\n",
    "\n",
    "generated_patters=generate_preprocessor(replace_patterns)\n",
    "\n",
    "def preprocessor(text):\n",
    "    # For each pattern, replace it with the appropriate string\n",
    "    for pattern, replace in compiled_replace_patterns:\n",
    "        text = re.sub(pattern, replace, text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphsize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lang='english', w=2, min_df=2, max_feat=999999999, stem=True, analyzer=None, verbose=False):\n",
    "        self.lang = lang\n",
    "        self.w = w\n",
    "        self.min_df = min_df\n",
    "        self.max_feat = max_feat\n",
    "        if not verbose:\n",
    "            self.progress_bar = lambda x: x\n",
    "        else:\n",
    "            from tqdm import tqdm\n",
    "            self.progress_bar = tqdm\n",
    "        \n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "        self._stem_ = lambda x: x\n",
    "        if stem:\n",
    "            from nltk.stem.snowball import SnowballStemmer\n",
    "            self._stem_ = SnowballStemmer(lang).stem\n",
    "            \n",
    "        self.analyzer = analyzer\n",
    "        if self.analyzer is None:\n",
    "            self.analyzer = TfidfVectorizer(preprocessor=preprocessor)\n",
    "        \n",
    "        self.vocab = dict()\n",
    "        self.df = Counter()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.N = len(X)\n",
    "        list(map(self._build_df_, self.progress_bar(X)))\n",
    "        self._filter_()\n",
    "        self._build_vocab_()\n",
    "        return self\n",
    "        \n",
    "    def _build_df_(self, text):\n",
    "        terms = list( filter( lambda x: x not in self.stopwords,\n",
    "                             self.analyzer.build_analyzer()(text) ))\n",
    "        terms = list( map( self._stem_, terms ))\n",
    "        self.df.update( set(terms) )\n",
    "    \n",
    "    def _filter_(self):\n",
    "        self.df = self.df.most_common(self.max_feat)\n",
    "        self.df = dict( list(filter( lambda x: x[1] >= self.min_df, self.df)) )\n",
    "    \n",
    "    def _build_vocab_(self):\n",
    "        self.id2term = sorted(list(self.df.keys()))\n",
    "        self.vocab = dict( [ (k,i) for (i,k) in enumerate(self.id2term) ] )\n",
    "        \n",
    "        self.vocab['<UNK>'] = len(self.id2term)\n",
    "        self.id2term.append( '<UNK>' )\n",
    "    \n",
    "    \n",
    "    def transform(self, text):\n",
    "        docs = list(map(self.analyzer.build_analyzer(), self.progress_bar(text)))\n",
    "        result = list(map(self._build_graph_, self.progress_bar(docs)))\n",
    "        result = list(map(self._build_features_, self.progress_bar(result)))\n",
    "        \n",
    "        return result\n",
    "    def _build_features_(self, param):\n",
    "        G, tfidf, pg = param\n",
    "        if len(G) == 0:\n",
    "            G.add_node( self.vocab['<UNK>'] )\n",
    "            return G, sp.csr_matrix(np.random.normal(size=len(self.vocab)))\n",
    "        \n",
    "        sorted_nodes = sorted( G.nodes )\n",
    "        full_weight = np.array([tfidf])*np.array([pg]).T    \n",
    "        \n",
    "        # Using the Adj matrix\n",
    "        A = nx.to_scipy_sparse_matrix( G, weight='freq', nodelist=sorted_nodes )\n",
    "        A = A.multiply(full_weight)\n",
    "        row,col = A.nonzero()\n",
    "        col2 = [ sorted_nodes[c] for c in col ]\n",
    "        nA = sp.csr_matrix( (A.data, (row, col2)), shape=(A.shape[0], len(self.vocab)) )\n",
    "        \n",
    "\n",
    "        return G, nA\n",
    "    \n",
    "    def _build_graph_(self, doc):\n",
    "        terms = list(filter( lambda x: x in self.vocab, doc))\n",
    "        terms = list(map( lambda x: self.vocab[x], terms ))\n",
    "        sorted_terms = sorted(list(set(terms)))\n",
    "        \n",
    "        tf = Counter(terms)\n",
    "        tfidf = dict( [ (k, v*np.log2((self.N+1)/self.df[self.id2term[k]])) for (k,v) in tf.items() ] )\n",
    "\n",
    "        cooccur_count = Counter()\n",
    "        for i,idt in enumerate(terms):\n",
    "            terms_to_add = terms[ max(i-self.w, 0):i ]\n",
    "            terms_to_add = list(zip(terms_to_add, repeat(idt)))\n",
    "            terms_to_add = list(map(sorted,terms_to_add))\n",
    "            terms_to_add = list(map(tuple,terms_to_add))\n",
    "            cooccur_count.update( terms_to_add )\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from( [ (k,{'tfidf': tfidf[k], 'tf': tf[k] }) for k in set(terms) ] )\n",
    "        w_edges = [ (s,t,w) for ((s,t),w) in cooccur_count.items() ]\n",
    "        G.add_weighted_edges_from( w_edges, weight='freq' )\n",
    "        \n",
    "        \n",
    "        tfidf = [ tfidf[term] for term in sorted_terms ]\n",
    "        tf = [ tf[term] for term in sorted_terms ]\n",
    "        #Add self-loops\n",
    "        G.add_weighted_edges_from( zip(sorted_terms, sorted_terms, tf), weight='freq' )\n",
    "        pg = nx.pagerank( G )\n",
    "        pg = [ pg[term] for term in sorted_terms ]\n",
    "        \n",
    "        return G, tfidf, pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, dataset_path, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dname = path.basename(path.abspath(dataset_path))\n",
    "        self._load_dataset_()\n",
    "        self._identify_splits_()\n",
    "        self.nclass = len(set(self.y))\n",
    "        self.split = {}\n",
    "    @property\n",
    "    def ndocs(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_lines(filename):\n",
    "        with io.open(filename, newline='\\n') as filin:\n",
    "            return filin.readlines()\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_array(X, idxs):\n",
    "        return [ X[idx] for idx in idxs ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_splits_(folddir):\n",
    "        splits = []\n",
    "        with open(folddir, encoding='utf8', errors='ignore') as fileout:\n",
    "            for line in fileout.readlines():\n",
    "                fold = []\n",
    "                for idx_part in line.split(';'):\n",
    "                    index = list(map(int, idx_part.split()))\n",
    "                    fold.append( index )\n",
    "                splits.append( tuple(fold) )\n",
    "        return splits\n",
    "    \n",
    "    def get_split(self, nfold, force_create=True, save=True, with_val=True):\n",
    "        nfold = str(nfold)\n",
    "        if nfold in self.split:\n",
    "            folds = self.split[nfold]\n",
    "            if not with_val: # extends train_index with val_index\n",
    "                folds = self._split_without_val_(folds)\n",
    "            return folds\n",
    "        \n",
    "        if nfold not in self.available_splits:\n",
    "            if not force_create:\n",
    "                raise Exception(f\"[ERROR] The {nfold}-fold split doen't exists. Use force_create=True to create.\")\n",
    "            folds = self._create_splits_( int(nfold) )\n",
    "            self.available_splits.add( nfold )\n",
    "        else:\n",
    "            split_file = path.join(self.dataset_path, 'splits', f'split_{nfold}.csv')\n",
    "            folds = Dataset._load_splits_( split_file )\n",
    "            \n",
    "        if any([len(f[0]) == 3 for f in folds]):\n",
    "            print(f\"[WARNING] The {nfold}-split doen't have validation. Creating...\", end=' ')\n",
    "            folds = self._create_val_(folds)\n",
    "            print(\"Done!\")\n",
    "            \n",
    "        self.split[nfold] = folds\n",
    "        \n",
    "        if save:\n",
    "            self.save_split(nfold)\n",
    "        \n",
    "        if not with_val: # extends train_index with val_index\n",
    "            folds = self._split_without_val_(folds)\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    def get_fold_instances(self, nfold, force_create=True, save=True, with_val=True):\n",
    "        splits = self.get_split( nfold, force_create=force_create, save=save, with_val=with_val)\n",
    "        for s in splits:\n",
    "            yield self._get_fold_instance_(s)\n",
    "    \n",
    "    def save_split(self, split, force_create=True):\n",
    "        splits = self.get_split( split, force_create=force_create )\n",
    "        split_file = path.join(self.dataset_path, 'splits', f'split_{split}.csv')\n",
    "        with open(split_file, 'w', encoding='utf8', errors='ignore') as fileout:\n",
    "            for train_index, val_index, test_index in splits:\n",
    "                train_str = ' '.join(list(map(str, train_index)))\n",
    "                val_str   = ' '.join(list(map(str, val_index)))\n",
    "                test_str  = ' '.join(list(map(str, test_index)))\n",
    "                line = train_str + ';' + val_str + ';' + test_str + '\\n'\n",
    "                fileout.write(line)\n",
    "    \n",
    "    def _create_splits_(self, k):\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=self.random_state)\n",
    "        kf  = list(skf.split(self.texts, self.y))\n",
    "        return kf\n",
    "                \n",
    "    def _split_without_val_(self, splits):\n",
    "        nfolds = []\n",
    "        for train_index, val_index, test_index in splits:\n",
    "            train_index.extend(val_index)\n",
    "            nfolds.append( (train_index, test_index) )\n",
    "        return nfolds\n",
    "    \n",
    "    def _get_fold_instance_(self, s):\n",
    "        if len(s) == 2:\n",
    "            train_idx, test_idx = s\n",
    "            train = ( Dataset.get_array( self.texts, train_idx ), Dataset.get_array( self.y, train_idx ) )\n",
    "            test = ( Dataset.get_array( self.texts, test_idx ), Dataset.get_array( self.y, test_idx ) )\n",
    "            return train, test\n",
    "        elif len(s) == 3:\n",
    "            train_idx, val_idx, test_idx = s\n",
    "            train = ( Dataset.get_array( self.texts, train_idx ), Dataset.get_array( self.y, train_idx ) )\n",
    "            val = ( Dataset.get_array( self.texts, val_idx ), Dataset.get_array( self.y, val_idx ) )\n",
    "            test = ( Dataset.get_array( self.texts, test_idx ), Dataset.get_array( self.y, test_idx ) )\n",
    "            return train, val, test\n",
    "    \n",
    "    def _create_val_(self, split):\n",
    "        aux_split = []\n",
    "        print(len(split[0]))\n",
    "        for (train_ids, test_ids) in split:\n",
    "            train_idx_atual, val_idx_atual = train_test_split(train_ids,\n",
    "                                            test_size=len(test_ids),\n",
    "                                            stratify=Dataset.get_array(self.y, train_ids))\n",
    "            aux_split.append( (train_idx_atual, val_idx_atual, test_ids) )\n",
    "        return aux_split\n",
    "\n",
    "    def _load_dataset_(self):\n",
    "        self.texts = Dataset.read_lines(path.join(self.dataset_path, 'texts.txt'))\n",
    "        self.y = Dataset.read_lines(path.join(self.dataset_path, 'score.txt'))\n",
    "        self.y = list(map(int, self.y))\n",
    "        \n",
    "    def _identify_splits_(self):\n",
    "        splits_files = glob( path.join(self.dataset_path, 'splits', 'split_*.csv') )\n",
    "        self.available_splits = set(map(lambda x: path.basename(x).replace('split_', '').replace('.csv', ''), splits_files ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "import gzip\n",
    "import io\n",
    "import pickle\n",
    "from os import path, mkdir\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "            np.int16, np.int32, np.int64, np.uint8,\n",
    "            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
    "            np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def save_vectorize(vectorize, namefile):\n",
    "\tvecfile = open(namefile, 'wb')\n",
    "\tpickle.dump(vectorize, vecfile)\n",
    "\tvecfile.close()\n",
    "\n",
    "def create_splits(X, y, k, seed):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    return list(kf.split(X, y))\n",
    "\n",
    "def load_json(path_json_file):\n",
    "    if path_json_file is None:\n",
    "        return []\n",
    "    with open(path_json_file) as file_in:\n",
    "        json_obj = json.load(file_in)\n",
    "    return json_obj\n",
    "\n",
    "def save_json(path_json_file, data):\n",
    "    with open(path_json_file, 'w') as file_out:\n",
    "        json.dump(data, file_out, cls=NumpyEncoder)\n",
    "        \n",
    "def is_jsonable(x):\n",
    "    try:\n",
    "        json.dumps(x, cls=NumpyEncoder)\n",
    "        return True\n",
    "    except (TypeError, OverflowError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_path(path_to_create):\n",
    "    path_to_create = path.abspath(path_to_create)\n",
    "    paths = path_to_create.split(path.sep)\n",
    "    complete_path = '/'\n",
    "    for p in paths[1:]:\n",
    "        complete_path = path.join(complete_path, p)\n",
    "        if not path.exists(complete_path):\n",
    "            mkdir( complete_path )\n",
    "\n",
    "def read_texts(filename):\n",
    "    with io.open(filename, newline='\\n') as filin:\n",
    "        return list(map(str.rstrip, filin.readlines()))\n",
    "\n",
    "def read_dataset(pathname):\n",
    "    texts = read_texts(path.join(pathname, 'texts.txt'))\n",
    "    scores = read_texts(path.join(pathname, 'score.txt'))\n",
    "    return texts,scores\n",
    "\n",
    "def get_array(X, idxs):\n",
    "    return [ X[idx] for idx in idxs ]\n",
    "    \n",
    "def save_splits_ids(splits, folddir):\n",
    "    with open(folddir, 'w', encoding='utf8', errors='ignore') as fileout:\n",
    "        for train_index, test_index in splits:\n",
    "            line = ' '.join(list(map(str, train_index))) + ';' + ' '.join(list(map(str, test_index))) + '\\n'\n",
    "            fileout.write(line)\n",
    "\n",
    "def load_splits_ids(folddir, with_val=False):\n",
    "    splits = []\n",
    "    with open(folddir, encoding='utf8', errors='ignore') as fileout:\n",
    "        for line in fileout.readlines():\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                train_index, test_index = parts\n",
    "                train_index = list(map(int, train_index.split()))\n",
    "                test_index = list(map(int, test_index.split()))\n",
    "                splits.append( (train_index, test_index) )\n",
    "            elif len(parts) == 3:\n",
    "                train_index, val_index, test_index = parts\n",
    "                test_index = list(map(int, test_index.split()))\n",
    "                val_index = list(map(int, val_index.split()))\n",
    "                train_index = list(map(int, train_index.split()))\n",
    "                if not with_val:\n",
    "                    train_index.extend(val_index)\n",
    "                    val_index = []\n",
    "                splits.append( (train_index, val_index, test_index) )\n",
    "            else:\n",
    "                raise Exception(\"\")\n",
    "    return splits\n",
    "\n",
    "def dump_svmlight_file_gz(X,y,filename):\n",
    "    with gzip.open(filename, 'w') as filout:\n",
    "        dump_svmlight_file(X, y, filout, zero_based=False)\n",
    "\n",
    "def load_svmlight_fold(filename_train, filename_test):\n",
    "    X_train, y_train = load_svmlight_file(filename_train, zero_based=False)\n",
    "    X_test, y_test = load_svmlight_file(filename_test, n_features=X_train.shape[1], zero_based=False)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7376 823\n"
     ]
    }
   ],
   "source": [
    "texts,scores = read_dataset('/home/mangaravite/Documents/datasets/topics/webkb/')\n",
    "n_class = len(set(scores))\n",
    "splits = load_splits_ids('/home/mangaravite/Documents/datasets/topics/webkb/split_10.csv')\n",
    "train_idx_zero, test_idx_zero = splits[0]\n",
    "\n",
    "#train_idx_zero, val_idx_zero = train_test_split(train_idx_zero,\n",
    "#                                                test_size=len(test_idx_zero),\n",
    "#                                                stratify=get_array(scores, train_idx_zero))\n",
    "\n",
    "X_train, y_train = get_array(texts, train_idx_zero), get_array(scores, train_idx_zero)\n",
    "#X_val, y_val = get_array(texts, val_idx_zero), get_array(scores, val_idx_zero)\n",
    "X_test, y_test = get_array(texts, test_idx_zero), get_array(scores, test_idx_zero)\n",
    "\n",
    "print(len(y_train), len(y_test))\n",
    "#print(y_train[0], y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 370.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1571.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 489.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 997.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 2, 'back': 2, 'web': 2, 'world': 2, 'mail': 2, 'page': 2, 'img': 2, 'work': 2} {'back': 0, 'img': 1, 'mail': 2, 'page': 3, 'time': 4, 'web': 5, 'work': 6, 'world': 7, '<UNK>': 8}\n",
      "[(0, {'tfidf': 0.5849625007211562, 'tf': 1}), (1, {'tfidf': 0.5849625007211562, 'tf': 1}), (2, {'tfidf': 0.5849625007211562, 'tf': 1}), (3, {'tfidf': 1.7548875021634687, 'tf': 3}), (4, {'tfidf': 1.1699250014423124, 'tf': 2}), (5, {'tfidf': 1.1699250014423124, 'tf': 2}), (7, {'tfidf': 1.7548875021634687, 'tf': 3})] [(0, 1, {'freq': 1}), (0, 3, {'freq': 1}), (0, 7, {'freq': 1}), (0, 4, {'freq': 1}), (0, 0, {'freq': 1}), (1, 7, {'freq': 1}), (1, 3, {'freq': 1}), (1, 1, {'freq': 1}), (2, 5, {'freq': 1}), (2, 3, {'freq': 1}), (2, 2, {'freq': 1}), (3, 7, {'freq': 2}), (3, 4, {'freq': 1}), (3, 5, {'freq': 3}), (3, 3, {'freq': 3}), (4, 7, {'freq': 3}), (4, 4, {'freq': 2}), (4, 5, {'freq': 1}), (5, 7, {'freq': 1}), (5, 5, {'freq': 2}), (7, 7, {'freq': 3})]\n",
      "[[0.0826648  0.0826648  0.         0.24799441 0.16532961 0.\n",
      "  0.         0.24799441 0.        ]\n",
      " [0.06865156 0.06865156 0.         0.20595469 0.         0.\n",
      "  0.         0.20595469 0.        ]\n",
      " [0.         0.         0.05667983 0.17003949 0.         0.11335966\n",
      "  0.         0.         0.        ]\n",
      " [0.11307872 0.11307872 0.11307872 1.0177085  0.22615744 0.67847233\n",
      "  0.         0.67847233 0.        ]\n",
      " [0.0824309  0.         0.         0.24729269 0.32972358 0.16486179\n",
      "  0.         0.74187806 0.        ]\n",
      " [0.         0.         0.08443743 0.75993684 0.16887485 0.33774971\n",
      "  0.         0.25331228 0.        ]\n",
      " [0.09701926 0.09701926 0.         0.58211554 0.58211554 0.19403851\n",
      "  0.         0.8731733  0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graph_builder = Graphsize(verbose=True)\n",
    "Gs = graph_builder.fit_transform( X_train[:2] )\n",
    "print(graph_builder.df, graph_builder.vocab)\n",
    "print(Gs[0][0].nodes(data=True), Gs[0][0].edges(data=True))\n",
    "print(Gs[0][1].A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7376/7376 [00:24<00:00, 301.22it/s]\n",
      "100%|██████████| 7376/7376 [00:07<00:00, 1009.53it/s]\n",
      " 39%|███▉      | 2873/7376 [00:43<01:40, 44.64it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_feats = 250000\n",
    "graph_builder = Graphsize(w=5, verbose=True, max_feat=max_feats)\n",
    "Gs_train = graph_builder.fit_transform(X_train)\n",
    "Gs_test  = graph_builder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_heads, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            #GraphConv(in_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0')),\n",
    "            #GraphConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))])\n",
    "            GATConv(in_dim, hidden_dim, num_heads=n_heads, activation=F.relu,\n",
    "                    feat_drop=0.5, attn_drop=0.5).to(torch.device('cuda:0')),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.relu,\n",
    "                    feat_drop=0.5, attn_drop=0.5).to(torch.device('cuda:0'))\n",
    "        ])\n",
    "        self.classify = nn.Linear(n_heads*hidden_dim, n_classes).to(torch.device('cuda:0'))\n",
    "        #self.lin = nn.Linear(hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "        self.lin = nn.Linear(n_heads*hidden_dim, 1).to(torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['f']\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        w = self.lin(h)\n",
    "        G.ndata['h'] = h\n",
    "        G.ndata['w'] = w\n",
    "        hg = dgl.mean_nodes(G, 'h', weight='w')\n",
    "        return self.classify(hg)\n",
    "    def transform(self, G):\n",
    "        h = G.ndata['f']\n",
    "        print(h.shape)\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            print(h.shape)\n",
    "        w = self.lin(h)\n",
    "        G.ndata['h'] = h\n",
    "        G.ndata['w'] = w\n",
    "        hg = dgl.mean_nodes(G, 'h', weight='w')\n",
    "        return hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    Gs_Fs, labels = map(list, zip(*samples))\n",
    "    graphs = []\n",
    "    with Pool(processes=4) as pool:\n",
    "        for g, f in Gs_Fs:\n",
    "            g_dgl = dgl.DGLGraph()\n",
    "            g_dgl.from_networkx(g)\n",
    "            g_dgl.ndata['f'] = torch.FloatTensor(f.A).to(torch.device('cuda:0'))\n",
    "            g_dgl.to(torch.device('cuda:0'))\n",
    "            graphs.append(g_dgl)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.to(torch.device('cuda:0'))\n",
    "    labels = torch.tensor(labels).to(torch.device('cuda:0'))\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(len(graph_builder.vocab), 300, 16, n_class).to(torch.device('cuda:0'))\n",
    "loss_func = nn.CrossEntropyLoss().to(torch.device('cuda:0'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "model.train()\n",
    "torch.cuda.synchronize()\n",
    "epoch_losses = []\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = DataLoader(list(zip(Gs_test, y_test)), batch_size=128, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    data_loader = DataLoader(list(zip(Gs_train, y_train)), batch_size=16, shuffle=True, collate_fn=collate)\n",
    "    epoch_loss = 0\n",
    "    with tqdm_notebook(total=len(data_loader.dataset), smoothing=0.) as pbar:\n",
    "        t0 = time()\n",
    "        for bg, label in data_loader:\n",
    "            outputs = model(bg)\n",
    "            loss = loss_func(outputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            del loss, outputs, bg\n",
    "            pbar.update( len(label) )\n",
    "            print('iter {}, loss {:.4f} ({:.2f})'.format(epoch, epoch_loss / (epoch + 1), (time()-t0)), end='\\r')\n",
    "    with tqdm_notebook(total=len(data_loader_test.dataset), smoothing=0.) as pbar:\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for bg, label in data_loader_test:\n",
    "                outputs = model(bg)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted == label).sum().item()\n",
    "                del predicted, outputs, bg\n",
    "                pbar.update( label.size(0) )\n",
    "                print('acc test {:.4f}'.format(correct/total), end='\\r')\n",
    "        epoch_loss /= (epoch + 1)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(graph_builder.vocab)\n",
    "class_mtx = [ sp.lil_matrix((n,n)) for _ in range(n_class) ]  \n",
    "for (G,feats), y in tqdm_notebook(zip(Gs_train, y_train), total=len(y_train)):\n",
    "    print(len(G))\n",
    "    nodes = sorted(G.nodes)\n",
    "    A = nx.to_scipy_sparse_matrix(G, nodelist=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 300\n",
    "num_heads = 4\n",
    "gcn1 = GraphConv(len(graph_builder.vocab), hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "#gcn2 = GATConv(hidden_dim, hidden_dim, activation=F.relu).to(torch.device('cuda:0'))\n",
    "gat = GATConv(hidden_dim, hidden_dim, num_heads, feat_drop=0.5, attn_drop=0.5, activation=F.relu).to(torch.device('cuda:0'))\n",
    "lin = nn.Linear(num_heads*hidden_dim, 1).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gcn1, gcn2, lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for G, l in tqdm(data_loader):\n",
    "    h = G.ndata['f']\n",
    "    h = gcn1(G,h)\n",
    "    h = gat(G,h)\n",
    "    h = h.view(h.shape[0], -1)\n",
    "    w = lin(h)\n",
    "    G.ndata['h'] = h\n",
    "    G.ndata['w'] = w\n",
    "    hg = dgl.mean_nodes(G, 'h', weight='w')\n",
    "    #del h, G, w\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h.shape)\n",
    "h.view(h.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\t\t\tdel obj\n",
    "\t\t\t\t\tgc.collect()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\t\t\t\tdel obj\n",
    "\t\t\t\t\tgc.collect()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)\n",
    "dump_tensors()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_tensors(gpu_only=True):\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\ttotal_size = 0\n",
    "\t\tfor obj in gc.get_objects():\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\t\tdel obj\n",
    "\t\t\t\t\t\tgc.collect()\n",
    "\t\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\t\tdel obj\n",
    "\t\t\t\t\t\tgc.collect()\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(nx.pagerank(G).items(), key=lambda x: x[0])\n",
    "sizes = np.array(list(zip(*sorted_list))[1])\n",
    "sizes = (sizes-sizes.min())/(sizes.max()-sizes.min())\n",
    "sizes = np.power(np.sqrt(sizes*300), 2)\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(nx.info(G))\n",
    "nx.draw(G, with_labels=True, node_size=sizes,pos=nx.spring_layout(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
