{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from TGA.utils import Dataset, GraphsizePretrained\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:23, 17351.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 727 ms, total: 23.4 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,stopwords='keep',\n",
    "                   pretrained_vec='/home/mangaravite/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test'), 313569)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/imdb_reviews/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=False))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    self.N = len(X)\n",
    "    y_train = self.le.fit( sorted(list(set(y))) ).transform(y)\n",
    "    self.n_class = len(self.le.classes_)\n",
    "    \n",
    "    self.class_term_freqs = Counter()\n",
    "    self.term_freqs = Counter()\n",
    "    self.class_freqs = Counter(y_train)\n",
    "\n",
    "    docs = list(map(self.analyzer.build_analyzer(), self.progress_bar(X)))\n",
    "    self.node_mapper = dict()\n",
    "    pairs = list( zip( docs, y_train ) )\n",
    "    for (doc,y) in self.progress_bar(pairs):\n",
    "\n",
    "        doc_in_terms = set(filter( lambda x: x in self.embeddings_dict, doc))\n",
    "        terms_by_nid = list(map(lambda x: self.node_mapper.setdefault(x, len(self.node_mapper)), doc_in_terms))\n",
    "\n",
    "        self.term_freqs.update( terms_by_nid )\n",
    "        \n",
    "        list_of_edges = list(map( lambda x: (y, x), terms_by_nid ))\n",
    "        self.class_term_freqs.update( list_of_edges )\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313569/313569 [04:24<00:00, 1186.34it/s]\n",
      "100%|██████████| 313569/313569 [01:06<00:00, 4747.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(encoding=None,\n",
       "                    pretrained_vec='/home/mangaravite/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt',\n",
       "                    stopwords='keep', verbose=None)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(graph_builder,fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples= [ (x,y,v) for ((y,x), v) in graph_builder.class_term_freqs.items() ]\n",
    "terms, clss, fresq = list(zip(*triples))\n",
    "N_all  = np.sum(fresq)\n",
    "n_terms= len(graph_builder.node_mapper)\n",
    "n_class=graph_builder.n_class\n",
    "cjct_terms_lbls = coo_matrix((fresq, (terms, clss)), shape=(n_terms, n_class))/N_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_priors = cjct_terms_lbls.sum(axis=0)\n",
    "terms_priors  = cjct_terms_lbls.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of indepencende P(x)*P(y)\n",
    "indp_mtrx = terms_priors.reshape(-1,1)*labels_priors.reshape(-1,1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82928, 1), (1, 10), (82928, 10), (82928, 10))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_priors.shape, labels_priors.shape, cjct_terms_lbls.shape, indp_mtrx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82292229, -0.71277767, -0.5917258 , ...,  0.0288091 ,\n",
       "         0.30158907,  0.59117272],\n",
       "       [-0.98877499, -0.8403062 , -0.67573364, ...,  0.23836716,\n",
       "         0.24228313,  0.10020134],\n",
       "       [ 0.44066138,  0.41918953,  0.31333562, ..., -0.13999841,\n",
       "        -0.17431109, -0.22273485],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         3.02815832,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi = coo_matrix(cjct_terms_lbls/indp_mtrx)\n",
    "pmi.data = np.log2(pmi.data)\n",
    "pmi.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.26433051821771325,\n",
       " 0.25828231590288275,\n",
       " array([[-0.05776433, -0.03964136, -0.03977587, ...,  0.00145702,\n",
       "          0.02578555,  0.0340682 ],\n",
       "        [-0.06223679, -0.06590377, -0.05239255, ...,  0.01695977,\n",
       "          0.01569586,  0.00595598],\n",
       "        [ 0.03906779,  0.0249523 ,  0.01515927, ..., -0.01126963,\n",
       "         -0.01545525, -0.0187372 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.11862313,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hxy = coo_matrix(cjct_terms_lbls)\n",
    "hxy.data = -np.log2(hxy.data) # h(x,y)= -log2 P(x,y)\n",
    "\n",
    "npmi= coo_matrix(pmi)\n",
    "npmi.data = pmi.data/hxy.data #npmi(x,y) = pmi(x,y)/h(x,y)\n",
    "npmi.min(), npmi.max(), npmi.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplos de termos disciminantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44529, 46731, 34765, ...,   468,    37,    50])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_abs = np.abs(npmi.A).sum(axis=1)\n",
    "sorted_index = (-sum_abs).argsort()\n",
    "sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_mapper = { v:k for (k,v) in graph_builder.node_mapper.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44529,\n",
       " 'hobgoblins',\n",
       " array([ 0.21451797,  0.1323334 ,  0.02350575, -0.09971513, -0.13465781,\n",
       "        -0.16801612, -0.16669893, -0.13003633,  0.        , -0.07867374]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_abs.argmax(), inverted_mapper[sum_abs.argmax()], npmi.A[sum_abs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hobgoblins', [0.2, 0.1, 0.0, -0.1, -0.1, -0.2, -0.2, -0.1, 0.0, -0.1]),\n",
       " ('mst', [0.1, 0.1, 0.1, 0.0, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('bloodrayne', [0.2, 0.1, 0.0, 0.0, 0.0, -0.0, -0.0, -0.1, -0.2, -0.1]),\n",
       " ('insultingly', [0.1, 0.1, 0.2, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1, -0.2]),\n",
       " ('hayao', [-0.2, -0.1, -0.2, -0.2, -0.1, -0.1, -0.0, 0.0, 0.1, 0.1]),\n",
       " ('stupidest', [0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.2, -0.1]),\n",
       " ('mavens', [0.0, 0.0, -0.1, -0.1, -0.1, -0.1, -0.1, -0.2, -0.1, 0.1]),\n",
       " ('stupider', [0.1, 0.1, 0.0, 0.1, 0.0, -0.1, -0.1, -0.1, -0.1, -0.2]),\n",
       " ('turd', [0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('zatoichi', [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.0, 0.1, 0.0, -0.1]),\n",
       " ('unengaging', [0.1, 0.1, 0.1, 0.1, 0.0, 0.0, -0.1, -0.1, -0.2, 0.0]),\n",
       " ('masochists', [0.2, 0.0, 0.1, -0.1, 0.0, -0.1, -0.1, 0.0, -0.1, -0.1]),\n",
       " ('moreland', [-0.0, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, -0.1, -0.2, -0.1]),\n",
       " ('godawful', [0.2, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('flawlessly', [-0.2, -0.1, -0.2, -0.1, -0.1, -0.0, -0.0, 0.0, 0.0, 0.1]),\n",
       " ('intolerably', [0.1, 0.1, 0.1, 0.0, -0.0, 0.0, -0.1, -0.0, -0.1, -0.2]),\n",
       " ('balrog', [-0.1, -0.1, -0.1, -0.1, -0.1, -0.0, -0.1, -0.0, 0.0, 0.1]),\n",
       " ('delicacy', [-0.2, -0.1, -0.1, -0.2, -0.1, -0.1, -0.0, 0.0, 0.1, 0.0]),\n",
       " ('wookie', [0.1, 0.0, 0.0, 0.0, -0.0, -0.1, -0.1, -0.2, -0.1, -0.0]),\n",
       " ('shoddily', [0.1, 0.1, -0.0, 0.1, 0.1, 0.0, -0.2, -0.1, 0.0, -0.1]),\n",
       " ('oooo', [0.1, 0.1, 0.1, -0.0, 0.0, -0.1, -0.1, 0.0, -0.1, -0.1]),\n",
       " ('manos', [0.1, 0.1, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('uninspired', [0.1, 0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.2]),\n",
       " ('seamlessly', [-0.2, -0.1, -0.1, -0.1, -0.1, -0.0, -0.0, 0.0, 0.0, 0.0]),\n",
       " ('babbles', [0.2, 0.1, 0.1, 0.1, -0.1, -0.0, -0.1, -0.1, -0.0, -0.0]),\n",
       " ('feebly', [0.2, 0.1, 0.1, -0.1, 0.0, -0.1, 0.0, -0.0, -0.1, -0.1]),\n",
       " ('refund', [0.1, 0.1, 0.1, -0.0, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('amateurishly', [0.1, 0.1, 0.0, 0.0, 0.0, -0.0, -0.0, -0.2, 0.0, -0.2]),\n",
       " ('macht', [0.1, 0.1, 0.0, 0.0, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('magnificently', [-0.1, -0.2, -0.1, -0.1, -0.1, -0.0, -0.0, 0.0, 0.0, 0.0]),\n",
       " ('dribble', [0.1, 0.1, 0.1, 0.1, 0.0, -0.1, -0.0, -0.2, -0.1, -0.1]),\n",
       " ('captivates', [-0.1, -0.1, -0.2, -0.2, -0.1, -0.0, -0.0, -0.0, 0.1, 0.0]),\n",
       " ('terl', [0.1, 0.1, 0.1, 0.0, 0.0, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('constipation', [0.1, 0.1, 0.1, 0.1, -0.0, -0.1, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('worser', [0.1, 0.0, 0.1, -0.0, 0.1, 0.0, -0.1, -0.0, -0.1, -0.1]),\n",
       " ('stealer', [-0.2, -0.2, -0.2, -0.1, -0.1, -0.0, 0.0, 0.0, 0.1, 0.0]),\n",
       " ('loken', [0.1, 0.1, 0.1, 0.1, -0.0, -0.0, -0.0, -0.1, -0.1, -0.2]),\n",
       " ('stunk', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('araki', [-0.1, -0.0, -0.0, -0.1, -0.1, -0.2, -0.1, 0.0, 0.1, 0.1]),\n",
       " ('delightful', [-0.1, -0.2, -0.1, -0.1, -0.1, -0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('gratingly', [0.1, 0.1, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1, 0.0, -0.1]),\n",
       " ('galadriel', [-0.1, -0.1, -0.1, -0.1, -0.1, -0.0, -0.1, -0.0, 0.0, 0.1]),\n",
       " ('snoozer', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1, 0.0]),\n",
       " ('flatulent', [0.1, 0.2, 0.0, -0.1, 0.0, -0.1, 0.0, -0.2, -0.0, -0.1]),\n",
       " ('baldrick', [0.0, -0.0, -0.1, 0.0, -0.1, -0.1, -0.2, 0.0, 0.1, 0.1]),\n",
       " ('leaden', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.2]),\n",
       " ('druid', [0.0, 0.1, 0.1, 0.1, 0.1, -0.0, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('flaccid', [0.1, 0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.2]),\n",
       " ('bulma', [0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.2, -0.2, -0.0, -0.1]),\n",
       " ('clunker', [0.1, 0.1, 0.1, 0.1, -0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('ofelia', [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.0, 0.1, 0.1]),\n",
       " ('marvelously', [-0.1, -0.1, -0.2, -0.1, -0.1, -0.1, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('stinker', [0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('sorbonne', [0.0, 0.0, 0.0, -0.2, -0.2, -0.1, -0.1, 0.0, 0.1, 0.1]),\n",
       " ('boll', [0.1, 0.1, 0.1, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('sevier', [0.0, 0.2, 0.1, 0.1, 0.0, -0.1, 0.0, -0.1, 0.0, -0.1]),\n",
       " ('conveys', [-0.2, -0.1, -0.1, -0.1, -0.1, -0.0, -0.0, 0.0, 0.0, 0.0]),\n",
       " ('redeemable', [0.1, 0.1, 0.1, 0.0, 0.0, -0.0, -0.0, -0.0, -0.2, -0.1]),\n",
       " ('grieco', [0.1, 0.1, 0.1, 0.0, 0.1, -0.1, -0.1, -0.0, 0.0, -0.1]),\n",
       " ('yawns', [0.1, 0.1, 0.1, 0.1, 0.1, -0.1, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('wobbles', [0.1, 0.1, -0.1, 0.1, 0.1, 0.0, 0.0, -0.2, -0.1, -0.1]),\n",
       " ('unendurable', [0.1, 0.1, 0.1, 0.1, -0.1, -0.1, -0.1, -0.0, -0.1, -0.1]),\n",
       " ('heartbreaking', [-0.2, -0.1, -0.1, -0.1, -0.1, -0.0, -0.0, 0.0, 0.0, 0.0]),\n",
       " ('poorly', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('plods', [0.0, 0.1, 0.1, 0.1, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('lestrange', [-0.1, -0.0, -0.1, -0.1, -0.2, 0.0, 0.0, 0.0, 0.1, -0.0]),\n",
       " ('migraine', [0.1, 0.1, 0.1, -0.0, -0.0, -0.0, -0.1, -0.1, -0.2, -0.0]),\n",
       " ('billingsley', [-0.1, -0.0, 0.1, 0.1, 0.0, 0.1, -0.0, -0.1, -0.1, 0.0]),\n",
       " ('howlers', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.2]),\n",
       " ('nauseam', [0.1, 0.1, -0.0, 0.1, 0.0, -0.1, 0.0, -0.1, -0.1, -0.2]),\n",
       " ('bathory', [-0.0, 0.1, 0.1, 0.0, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('razzies', [0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.0, -0.1, -0.1, -0.0]),\n",
       " ('jaffe', [0.0, -0.1, -0.2, -0.1, -0.2, -0.0, -0.0, 0.0, 0.0, 0.1]),\n",
       " ('strider', [-0.1, -0.0, -0.1, -0.1, -0.1, -0.1, -0.1, -0.0, 0.0, 0.1]),\n",
       " ('numbingly', [0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('puzo', [-0.0, -0.1, -0.1, 0.0, -0.1, -0.1, -0.1, -0.1, -0.0, 0.1]),\n",
       " ('unparalleled', [-0.1, -0.1, -0.0, -0.1, -0.1, -0.1, -0.0, -0.0, 0.0, 0.1]),\n",
       " ('charmless', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('br', [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.0, 0.1, 0.0]),\n",
       " ('masterfully', [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.0, 0.0, 0.0, 0.0]),\n",
       " ('hemsworth', [0.0, 0.1, 0.1, 0.1, 0.1, 0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('balances', [-0.2, -0.2, -0.1, -0.1, -0.1, -0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('transcendent', [-0.2, -0.2, -0.1, -0.1, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0]),\n",
       " ('forwarded', [0.1, 0.1, 0.1, 0.1, -0.0, -0.0, -0.1, -0.0, -0.1, -0.1]),\n",
       " ('shivery', [0.0, 0.0, -0.0, 0.0, -0.1, -0.1, -0.2, 0.1, -0.0, -0.1]),\n",
       " ('harrier', [0.1, 0.1, 0.1, -0.0, -0.1, -0.1, -0.1, -0.1, 0.0, -0.1]),\n",
       " ('unfunny', [0.1, 0.1, 0.1, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('simulator', [0.1, 0.0, 0.1, -0.1, -0.1, -0.0, -0.0, -0.1, -0.1, 0.0]),\n",
       " ('mish', [0.1, 0.1, 0.1, 0.1, 0.0, -0.0, -0.0, -0.1, -0.1, -0.1]),\n",
       " ('uwe', [0.1, 0.1, 0.1, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('dinklage', [0.0, -0.1, -0.1, 0.0, 0.1, 0.1, 0.0, 0.1, -0.0, -0.3]),\n",
       " ('grandest', [-0.1, -0.1, -0.0, -0.1, -0.0, -0.1, -0.0, -0.0, -0.0, 0.1]),\n",
       " ('dujardin', [-0.2, -0.1, -0.0, -0.1, -0.1, -0.1, -0.0, 0.0, 0.1, 0.0]),\n",
       " ('excrement', [0.2, 0.1, 0.0, 0.0, -0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('cazale', [-0.2, 0.0, 0.0, 0.0, -0.2, -0.1, -0.0, 0.0, 0.0, 0.1]),\n",
       " ('unexciting', [0.0, 0.1, 0.1, 0.1, 0.0, 0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('teleprompter', [0.1, 0.1, 0.0, 0.1, -0.0, -0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('embarassing', [0.2, 0.1, 0.0, 0.0, 0.0, 0.1, -0.1, -0.1, 0.0, -0.1]),\n",
       " ('luckinbill', [0.0, 0.1, 0.1, 0.1, 0.1, 0.0, -0.1, -0.1, -0.1, -0.1]),\n",
       " ('decoteau', [0.1, 0.1, 0.0, 0.0, 0.0, 0.0, -0.0, -0.1, -0.1, -0.1])]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=100\n",
    "list(zip([inverted_mapper[x] for x in sorted_index[:n]], np.round(npmi.A[list(sorted_index[:n])], decimals=1).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering semantic-syntatic\n",
    "\n",
    "Infer relationship between nPMI and Label->Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(vec, mask=None, dim=1, epsilon=1e-5):\n",
    "    exps = torch.exp(vec)\n",
    "    if mask is None:\n",
    "        mask = vec > 0.\n",
    "    masked_exps = exps * mask.float()\n",
    "    masked_sums = masked_exps.sum(dim, keepdim=True) + epsilon\n",
    "    return (masked_exps/masked_sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalPMI(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, drop=0.3, drop_global=.5, loss=None, device=torch.device('cuda')):\n",
    "        super(GlobalPMI, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.global_term_encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d( hidden_dim ),\n",
    "            nn.Dropout(drop_global),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.local_term_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.label_encoder = nn.Sequential(\n",
    "            nn.Linear(n_classes, hidden_dim),\n",
    "            nn.BatchNorm1d( hidden_dim ),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d( hidden_dim )\n",
    "        ).to(device)\n",
    "        \n",
    "        self.y = torch.eye( n_classes, n_classes ).to(device)\n",
    "        \n",
    "        self.att     = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        \"\"\"self.att_act = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\"\"\"\n",
    "        \n",
    "        self.doc_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_dim, n_classes),\n",
    "            nn.Softmax()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.loss = loss.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, term_emb, term_mask, doc_mask):\n",
    "        batch_size = doc_mask.max()+1\n",
    "        \n",
    "        h_label = self.label_encoder(self.y)\n",
    "        h_term  = self.global_term_encoder(term_emb)\n",
    "        \n",
    "        h_local_term = self.local_term_encoder(h_term[term_mask])\n",
    "        docs = torch.rand((batch_size, self.hidden_dim))\n",
    "        \n",
    "        for docid in range(batch_size):\n",
    "            doc_term_emb = h_local_term[doc_mask == docid]\n",
    "            alpha = self.att(doc_term_emb)\n",
    "            alpha = F.softmax(doc_term_emb)\n",
    "            \n",
    "            #act = self.att_act(alpha)\n",
    "            #act = masked_softmax( act )\n",
    "            #docs[docid] = (act*alpha*doc_term_emb).mean(dim=0)\n",
    "            \n",
    "            docs[docid] = (alpha*doc_term_emb).mean(dim=0)\n",
    "            \n",
    "        docs = self.doc_encoder( docs.to(self.device) )\n",
    "        docs_probs = self.fc( docs )\n",
    "        return h_term, h_label, docs, docs_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy\n",
    "class NpairLoss(nn.Module):\n",
    "    \"\"\"the multi-class n-pair loss\"\"\"\n",
    "    def __init__(self, l2_reg=0.02):\n",
    "        super(NpairLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, anchor, target, positive=None):\n",
    "        batch_size = anchor.size(0)\n",
    "        target = target.view(target.size(0), 1)\n",
    "\n",
    "        target = (target == torch.transpose(target, 0, 1)).float()\n",
    "        target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
    "\n",
    "        if positive is not None:\n",
    "            logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
    "        else:\n",
    "            logit = torch.matmul(anchor, torch.transpose(anchor, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size\n",
    "        \n",
    "        loss_ce = cross_entropy(logit, target)\n",
    "\n",
    "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
    "        return loss\n",
    "class TGALoss(torch.nn.Module):\n",
    "    def __init__(self, l2_reg=5e-4):\n",
    "        super(TGALoss, self).__init__()\n",
    "        self.sigma    = nn.Parameter(torch.ones(4))\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss  = nn.CrossEntropyLoss()\n",
    "        self.npl_loss = NpairLoss(l2_reg=l2_reg)\n",
    "        self.dst_loss = SelfDistLoss()\n",
    "\n",
    "    def forward(self, loss_term_npmi, loss_doc_class, loss_cross_ent, loss_self_dist, explain=False):\n",
    "        \n",
    "        loss_term_npmi = loss_term_npmi/(self.sigma[0]**2) + torch.log(self.sigma[0])\n",
    "        loss_doc_class = loss_doc_class/(2.*self.sigma[1]**2) + torch.log(self.sigma[1])\n",
    "        loss_cross_ent = loss_cross_ent/(2.*self.sigma[2]**2) + torch.log(self.sigma[2])\n",
    "        loss_self_dist = loss_self_dist/(self.sigma[3]**2) + torch.log(self.sigma[3])\n",
    "        \n",
    "        loss = (loss_term_npmi + loss_doc_class + loss_cross_ent + loss_self_dist).mean()\n",
    "        \n",
    "        if explain:\n",
    "            explanation = {\n",
    "                'term_npmi': (loss_term_npmi.mean().item(), self.sigma[0].item()),\n",
    "                'doc_class': (loss_doc_class.mean().item(), self.sigma[1].item()),\n",
    "                'cross_ent': (loss_cross_ent.mean().item(), self.sigma[2].item()),\n",
    "                'self_dist': (loss_self_dist.mean().item(), self.sigma[3].item()),\n",
    "            }\n",
    "            return loss, explanation\n",
    "        return loss\n",
    "class SelfDistLoss(nn.Module):\n",
    "    def __init__(self, l2_reg=0.02, eps = 0.00003):\n",
    "        super(SelfDistLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, hiddens):\n",
    "        L = torch.matmul(hiddens.T, hiddens)\n",
    "        L = F.sigmoid(L)\n",
    "        L = (L - L.diag()).float()\n",
    "        L = F.relu(L)\n",
    "        L = ( L > 0. ).float() * torch.exp( L )\n",
    "        #L = F.normalize(L)\n",
    "        \n",
    "        values = L.sum(axis=1)\n",
    "        svalue = max((values > 0.).sum(), self.eps)\n",
    "\n",
    "        return values.sum()/svalue # AVG of non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlobalPMI(\n",
       "  (global_term_encoder): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (local_term_encoder): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (label_encoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=300, bias=True)\n",
       "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (att): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (doc_encoder): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=300, out_features=10, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       "  (loss): TGALoss(\n",
       "    (mse_loss): MSELoss()\n",
       "    (ce_loss): CrossEntropyLoss()\n",
       "    (npl_loss): NpairLoss()\n",
       "    (dst_loss): SelfDistLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgaloss = TGALoss()\n",
    "\n",
    "globalpmi = GlobalPMI(300, 300, dataset.nclass, drop=0.1, loss=tgaloss)\n",
    "globalpmi = globalpmi.cuda()\n",
    "globalpmi.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "    analyzer = self.analyzer.build_analyzer()\n",
    "    local_mapper = dict()\n",
    "    term_emb = []\n",
    "    term_mask = []\n",
    "    doc_mask = []\n",
    "    for i,doc in enumerate(X):\n",
    "        doc_in_terms = analyzer(doc)\n",
    "        doc_in_terms = set(filter( lambda x: x in self.embeddings_dict, doc_in_terms))\n",
    "        #print(doc_in_terms)\n",
    "        for tid in doc_in_terms:\n",
    "            if tid not in local_mapper:\n",
    "                local_mapper[tid] = len(local_mapper)\n",
    "                term_emb.append( self.embeddings_dict[tid] )\n",
    "            term_mask.append( local_mapper[tid] )\n",
    "            doc_mask.append( i )\n",
    "    return term_emb, term_mask, doc_mask, local_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    term_emb, term_mask, doc_mask, local_mapper = transform(graph_builder, X)\n",
    "    \n",
    "    return term_emb, term_mask, doc_mask, local_mapper, y\n",
    "\n",
    "def get_term_emb( local_mapper, term_emb ):\n",
    "    local_terms = list(zip(*sorted(local_mapper.items(), key=lambda x: x[1] )))[0]\n",
    "\n",
    "    idx_local  = [ local_mapper[x] for x in local_terms if x in graph_builder.node_mapper ] \n",
    "    idx_global = [ graph_builder.node_mapper[x] for x in local_terms if x in graph_builder.node_mapper ]\n",
    "\n",
    "    filtered_npmi = npmi.A[ idx_global ]\n",
    "\n",
    "    sum_abs = np.abs(filtered_npmi).sum(axis=1)\n",
    "    sorted_index = (-sum_abs).argsort()\n",
    "    sorted_index = sorted_index[ :int(p*len(idx_global)) ]\n",
    "\n",
    "    term_emb_pred  = term_emb[[idx_local[x] for x in sorted_index]]\n",
    "    term_npmi      = torch.Tensor(filtered_npmi[sorted_index])\n",
    "    \n",
    "    return term_emb_pred, term_npmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "p=0.3\n",
    "margin=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD( globalpmi.parameters(), lr=5e-3, momentum=0.9)\n",
    "optimizer = optim.AdamW( globalpmi.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "\n",
    "mse_loss = nn.MSELoss().to(device)\n",
    "ce_loss  = nn.CrossEntropyLoss().to(device)\n",
    "npl_loss = NpairLoss(l2_reg=5e-4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112126ceee0c4c24b0f98710e7801cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8777e531f74ce388a165be6273088a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=313569.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.2769, Lss1: 0.5, Lss2: 3.918, Lss3: 2.222, Lss4: 1.002033\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 3.95 GiB total capacity; 3.12 GiB already allocated; 7.06 MiB free; 3.31 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-113d7cf89679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 3.95 GiB total capacity; 3.12 GiB already allocated; 7.06 MiB free; 3.31 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "best = None\n",
    "nepochs = 10\n",
    "globalpmi.train()\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_loss1 = 0\n",
    "    epoch_loss2 = 0\n",
    "    epoch_loss3 = 0\n",
    "    epoch_loss4 = 0\n",
    "    TP = 0\n",
    "    Ndocs = 0\n",
    "    y_train = graph_builder.le.transform(fold.y_train)\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=4)\n",
    "    with tqdm(total=len(fold.y_train), smoothing=0.) as pbar:\n",
    "        total = 1\n",
    "        correct = 0\n",
    "        for i, (term_emb, term_mask, doc_mask, local_mapper, y) in enumerate(data_loader):\n",
    "            term_emb = torch.Tensor(term_emb).to(device)\n",
    "            term_mask = torch.LongTensor(term_mask).to(device)\n",
    "            doc_mask = torch.LongTensor(doc_mask).to(device)\n",
    "            y = torch.LongTensor(y).to(device)\n",
    "            \n",
    "            h_term, h_label, h_docs, y_probs = globalpmi(term_emb, term_mask, doc_mask)\n",
    "            \n",
    "            h_term, h_label, h_docs, y_probs = h_term.to(device), h_label.to(device), h_docs.to(device), y_probs.to(device)\n",
    "            \n",
    "            term_emb_pred, term_npmi = get_term_emb( local_mapper, term_emb )\n",
    "            term_emb_pred, term_npmi = term_emb_pred.to(device), term_npmi.to(device)\n",
    "            \n",
    "            loss1 = tgaloss.mse_loss( term_npmi, torch.matmul( term_emb_pred, h_label.T ) )\n",
    "            loss2 = tgaloss.npl_loss(h_docs, y, h_label[y])\n",
    "            loss3 = tgaloss.ce_loss( y_probs, y )\n",
    "            loss4 = tgaloss.dst_loss( h_label )\n",
    "            \n",
    "            epoch_loss1 += loss1\n",
    "            epoch_loss2 += loss2\n",
    "            epoch_loss3 += loss3\n",
    "            epoch_loss4 += loss4\n",
    "            \n",
    "            if loss1 < margin:\n",
    "                loss1 = margin\n",
    "            \n",
    "            #loss = loss2 + loss3\n",
    "            loss = tgaloss( loss1, loss2, loss3, loss4 )\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            TP += (y_probs.argmax(dim=1)==y).sum()\n",
    "            Ndocs += len(y)\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'Lss: {epoch_loss/(i+1):.4} \\\n",
    "                    Lss_i: ({epoch_loss1/(i+1):.4};{epoch_loss2/(i+1):.4};{epoch_loss3/(i+1):.4};{epoch_loss4/(i+1):.4})')\n",
    "            print(f'Acc: {TP/Ndocs:.4}, Lss1: {loss1:.4}, Lss2: {loss2:.4}, Lss3: {loss3:.4}, Lss4: {loss4:.4}' , end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACM: Acc: 0.5857, Lss1: 0.01755, Lss2: 0.004775\n",
    "self_label = F.sigmoid(torch.matmul(h_label.T, h_label))\n",
    "F.relu(self_label-self_label.diag()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_dist = SelfDistLoss()\n",
    "self_dist( h_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5052, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_label.shape, y, graph_builder.n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_terms_emb = term_emb[ term_mask[doc_mask == 0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_terms_emb.sum(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(doc_terms_emb.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs.argmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
