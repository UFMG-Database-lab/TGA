{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import Dataset, GraphsizePretrained\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/acm/')\n",
    "fold = next(dataset.get_fold_instances(10))\n",
    "fold._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:23, 16679.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.7 s, sys: 579 ms, total: 24.3 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19907/19907 [00:05<00:00, 3693.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.62 s, sys: 52.3 ms, total: 6.67 s\n",
      "Wall time: 6.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt',\n",
       "          stopwords='remove', verbose=None, w=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126449, 34676)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2921),\n",
       " (1, 8317),\n",
       " (2, 10134),\n",
       " (3, 15852),\n",
       " (4, 1086),\n",
       " (5, 5801),\n",
       " (6, 5148),\n",
       " (7, 14561),\n",
       " (8, 13217),\n",
       " (9, 3465),\n",
       " (10, 11293)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 n_heads=8, n_convs=2, drop=.5, first_hidden='emb', attn_drop=.5,\n",
    "                 encoders={'term','label'}, device='cuda:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        h = G.ndata[self.first_hidden].float()\n",
    "        for (k, mask) in kwargs.items():\n",
    "            if k in self.encoders:\n",
    "                if mask is not None:\n",
    "                    h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                else:\n",
    "                    h = self.encoders[k]( h )\n",
    "        \n",
    "        for l, conv in enumerate(self.layers):\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "            h = self.down_proj[l]( h )\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_class,\n",
    "                  n_heads=8, drop=.5, attn_drop=.5,\n",
    "                  device='cuda:0'):\n",
    "        super(TGA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.device = torch.device(device)\n",
    "        self.gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                                     encoders={'label'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        \n",
    "        self.gat_local  = GenericGAT(hidden_dim, hidden_dim, \n",
    "                                     encoders={'term'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     first_hidden='emb',\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "\n",
    "        self.lin = nn.Linear( hidden_dim, 1).to(self.device)\n",
    "        # Depois tentar alguma ativação (ReLU, por exemplo, pode \"desativar\" alguns termos no softmax)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear( hidden_dim, hidden_dim//2).to(self.device)\n",
    "        self.fc2 = nn.Linear( hidden_dim//2, hidden_dim//4).to(self.device)\n",
    "        self.fc3 = nn.Linear( hidden_dim//4, self.n_class).to(self.device)\n",
    "    def forward(self, G, gs):\n",
    "        #h_global           = self.gat_global( G, label=G.ndata['label'].nonzero().flatten() )\n",
    "        #gs.ndata['weight'] = h_global[ gs.ndata['idx'] ] # Tentar concatenando\n",
    "        h_local            = self.gat_local(gs, term=None)\n",
    "        #h_local            = torch.cat((h_local, h_global[ gs.ndata['idx'] ]), 1)\n",
    "        h_local            = self.pooling( gs, h_local )\n",
    "        h_local            = self.fc1( h_local )\n",
    "        h_local            = self.fc2( h_local )\n",
    "        h_local            = self.fc3( h_local )\n",
    "        return h_local\n",
    "# torch.Size([3652, 300]) torch.Size([3652, 300]) torch.Size([128, 300])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=300\n",
    "hidden_dim=300\n",
    "n_heads=8\n",
    "drop=0.3\n",
    "attn_drop=0.5\n",
    "batch_size=128\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat_global): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gat_local): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (term): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=300, out_features=2400, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3)\n",
       "        (attn_drop): Dropout(p=0.5)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lin): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (pooling): GlobalAttentionPooling(\n",
       "    (gate_nn): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=300, out_features=150, bias=True)\n",
       "  (fc2): Linear(in_features=150, out_features=75, bias=True)\n",
       "  (fc3): Linear(in_features=75, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TGA( in_dim, hidden_dim, graph_builder.n_class,\n",
    "            n_heads=n_heads, drop=drop, attn_drop=attn_drop )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl_list = []\n",
    "    idx_terms = { l for l in graph_builder.label_ids }\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl_list.append( g_dgl )\n",
    "        \n",
    "        idx_terms = idx_terms.union( set(nx.get_node_attributes(g,'idx').values()) )\n",
    "    \n",
    "    Gs_dgl = dgl.batch(Gs_dgl_list)\n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    \n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    #subgraph = graph_builder.g.subgraph(idx_terms)\n",
    "    #big_graph_dgl.from_networkx(subgraph, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, Gs_dgl, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905e37995bb640f8b8370f0b62b90c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ffe67ac1c142ae99d78bf84dfe0d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3544a33e39c7454eb3b8b125c9c77516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf0b9b034d54cd18e8621db8107f234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac78db1a004823bc0b827b0f5a3b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49283adc5a714e7cbf744b386ac713bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0971a3409c634e51bad2efda59c61709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a707ef0f0604c9aad285b37b285a570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cbd64cc8ee42c1b1e27282fe89881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da7e8535033433aa723abcd7c0729cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19907.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=2)\n",
    "    with tqdm(total=len(fold.y_train)) as pbar:\n",
    "        total = 1\n",
    "        correct = 1\n",
    "        model.train()\n",
    "        for G, gs, y in data_loader:\n",
    "            G = G.to( torch.device('cuda:0') )\n",
    "            gs = gs.to( torch.device('cuda:0') )\n",
    "            y = y.to( torch.device('cuda:0') )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model( G, gs )\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            total += y.size(0)\n",
    "            correct += (sampled_Y == y).sum().item()\n",
    "            \n",
    "            del probs_Y, sampled_Y, G, gs\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {epoch} Acc train: {correct/total:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_nodes_from( [ (0, {'idx': 0}), (1, {'idx': 1}), (2, {'idx': 2}) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nx.get_node_attributes(g,'idx').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5621e-01, -2.7914e+00, -5.0857e-01,  2.0302e+00, -3.2203e+00,\n",
       "         -1.3610e+00, -1.6609e+00,  1.9490e+00,  1.4226e+00, -5.8304e-01,\n",
       "          1.3319e+00],\n",
       "        [-1.8599e+00, -1.6571e-01,  1.3800e+00,  3.0833e+00, -9.9206e-01,\n",
       "         -4.4450e-01,  7.0561e-01, -1.2386e+00, -5.9356e-01, -3.3527e+00,\n",
       "         -5.3752e-01],\n",
       "        [ 3.1851e+00, -1.8506e+00, -1.1523e+00,  8.4189e-01, -2.2230e+00,\n",
       "         -1.1341e+00, -2.2522e+00,  1.3125e+00,  6.6847e-01,  1.0213e+00,\n",
       "          8.4633e-01],\n",
       "        [-6.1414e-01, -3.2895e+00, -5.8638e-01,  2.3248e+00, -4.0843e+00,\n",
       "         -2.6888e+00, -1.5182e+00,  1.4223e+00,  2.3233e+00, -4.1613e-01,\n",
       "          3.2309e+00],\n",
       "        [-6.5610e-02, -1.3356e-01, -5.1689e-01,  1.2052e+00, -4.5027e+00,\n",
       "         -2.0272e+00, -3.9816e+00,  2.7224e+00,  2.2447e+00,  1.6045e+00,\n",
       "          8.0511e-01],\n",
       "        [-1.0919e+00, -9.6540e-01,  2.7508e+00,  2.1081e+00, -2.0723e+00,\n",
       "         -1.1534e+00, -1.2076e+00,  2.0568e+00, -1.3149e-01, -2.4500e+00,\n",
       "         -6.0324e-01],\n",
       "        [ 4.8390e-01, -2.4247e+00,  3.3505e+00,  8.1755e-01, -1.7615e+00,\n",
       "         -3.0050e+00, -1.7905e+00,  2.5489e+00, -2.8909e-01, -9.8099e-01,\n",
       "          1.7617e+00],\n",
       "        [ 1.8798e+00, -4.5684e+00, -7.1944e-01,  1.6603e+00, -5.7703e+00,\n",
       "         -2.1619e+00, -5.3332e+00,  7.5197e+00,  2.9847e+00,  1.3738e+00,\n",
       "          2.1230e+00],\n",
       "        [-1.0664e+00, -5.2149e-02, -1.1572e-01,  6.1419e-01, -1.5351e+00,\n",
       "          4.0778e-01,  4.8563e-01, -3.8599e-01,  1.0155e+00, -1.0098e+00,\n",
       "         -7.5169e-01],\n",
       "        [-2.3455e+00,  8.5199e-01, -1.3429e+00, -7.4733e-02,  3.0504e+00,\n",
       "          5.0081e+00,  4.7816e+00, -2.3027e+00, -1.0015e+00, -3.1356e+00,\n",
       "         -3.2077e+00],\n",
       "        [-7.1634e-01, -1.2638e-01, -6.6347e-01,  2.8139e+00, -2.9660e+00,\n",
       "         -8.1379e-01, -1.6807e+00,  3.5259e-01,  9.8222e-01, -9.1532e-01,\n",
       "         -6.8513e-03],\n",
       "        [ 1.3048e-01, -3.9357e+00,  1.1018e+00,  1.3565e+00, -5.7472e+00,\n",
       "         -3.0890e+00, -3.6685e+00,  5.2835e+00,  3.0994e+00,  2.4443e-01,\n",
       "          2.2648e+00],\n",
       "        [-2.2039e+00, -4.3964e-01,  9.8868e-01,  1.5346e+00, -2.7794e+00,\n",
       "          7.8498e-01,  4.7409e-01,  8.7814e-01,  1.5657e+00, -2.6180e+00,\n",
       "         -2.0257e+00],\n",
       "        [-3.0581e+00,  3.5519e+00,  5.5390e+00, -1.4654e+00, -1.7966e+00,\n",
       "         -1.2759e+00,  8.1525e-01, -1.9393e+00,  4.9449e-01, -1.7903e+00,\n",
       "         -3.3033e+00],\n",
       "        [-2.8535e+00,  4.8912e+00, -5.2158e-02,  6.5434e-01, -1.7917e+00,\n",
       "          5.3218e-01, -1.2358e+00, -1.6743e+00,  7.1798e-01,  2.6298e-01,\n",
       "         -2.8117e+00],\n",
       "        [-4.2487e-01, -1.7946e+00,  2.6812e+00,  1.8162e+00, -3.0784e+00,\n",
       "         -4.1987e+00, -2.8118e+00,  1.9783e+00,  4.3494e-01, -3.8691e-01,\n",
       "          2.8075e+00],\n",
       "        [-3.2113e+00,  5.4425e+00,  1.4590e-01,  8.6469e-01, -4.8374e+00,\n",
       "         -1.1031e+00, -3.7359e+00, -2.8269e-01,  2.5018e+00,  1.6108e+00,\n",
       "         -2.5406e+00],\n",
       "        [-2.5915e+00, -9.1610e-01,  2.4821e+00,  1.3986e+00, -1.2518e+00,\n",
       "          4.6727e-01,  2.3010e+00, -6.0670e-01,  4.4523e-01, -4.0066e+00,\n",
       "         -1.5886e+00],\n",
       "        [-2.1299e+00, -1.4973e+00,  7.6172e-01,  9.7869e-01,  5.3680e-01,\n",
       "          1.1768e+00,  4.8561e+00, -3.6495e+00,  1.0133e-01, -4.1240e+00,\n",
       "         -5.0724e-01],\n",
       "        [-2.9070e+00,  3.6842e-01,  5.1954e-01,  2.1507e+00, -1.0485e+00,\n",
       "          1.1248e+00,  8.1379e-01,  3.7570e-02,  2.3097e-01, -2.8026e+00,\n",
       "         -1.5248e+00],\n",
       "        [-2.7393e+00,  2.7082e+00,  6.6563e+00,  8.5073e-01, -1.2167e+00,\n",
       "         -3.3415e+00, -8.4501e-01, -1.1317e+00, -1.4239e+00, -2.4943e+00,\n",
       "         -1.2423e+00],\n",
       "        [-1.7279e+00, -1.9924e+00,  8.8353e-01, -2.7850e-01, -2.7437e+00,\n",
       "          2.3696e+00,  2.3279e+00,  1.5668e+00,  2.6659e+00, -2.9820e+00,\n",
       "         -2.9930e+00],\n",
       "        [-3.5794e+00,  9.1366e-01, -1.0830e-01,  2.0989e+00, -4.4317e+00,\n",
       "          2.6099e-01, -1.1230e+00,  1.2104e+00,  2.7655e+00, -1.4107e+00,\n",
       "         -1.6868e+00],\n",
       "        [-3.6885e+00,  1.3350e+00, -2.3920e-01,  4.4212e+00, -2.2863e+00,\n",
       "          1.3033e+00,  5.3770e-01, -1.2199e+00,  5.0251e-01, -3.9623e+00,\n",
       "         -2.4924e+00],\n",
       "        [-1.6904e+00, -9.7494e-02,  5.0214e-01,  1.8355e+00, -3.1486e+00,\n",
       "         -5.2414e-01, -1.2680e+00,  1.2570e+00,  1.4345e+00, -1.1733e+00,\n",
       "         -6.5512e-01],\n",
       "        [ 4.6203e-01, -1.6142e+00,  5.1534e+00,  1.2571e+00, -2.1300e+00,\n",
       "         -4.3049e+00, -2.2621e+00,  1.7919e+00, -8.5066e-01, -1.5213e+00,\n",
       "          1.4693e+00],\n",
       "        [-1.2419e+00, -5.0519e-01,  1.2830e+00,  1.2105e+00, -2.9429e+00,\n",
       "          3.8270e-01, -1.2798e+00,  2.8689e+00,  1.2796e+00, -1.5900e+00,\n",
       "         -1.9053e+00],\n",
       "        [-2.4998e+00, -5.6081e-01, -7.6527e-01,  8.7215e-01,  2.4809e-01,\n",
       "          3.5026e+00,  3.3946e+00, -6.9176e-01,  5.5553e-01, -3.3319e+00,\n",
       "         -2.4989e+00],\n",
       "        [ 1.4376e+00, -2.5507e+00,  1.4229e+00,  2.1565e+00, -4.5402e+00,\n",
       "         -3.2517e+00, -5.3586e+00,  5.6244e+00,  1.1080e+00,  6.4184e-01,\n",
       "          1.6701e+00],\n",
       "        [-2.6685e+00,  4.3717e+00, -5.0197e-01, -1.1682e+00, -7.3671e-01,\n",
       "          2.2536e+00,  3.1294e-01, -1.3111e+00,  1.0368e+00,  4.5903e-01,\n",
       "         -3.5879e+00],\n",
       "        [-3.6266e+00,  6.7503e+00,  1.4693e+00, -8.1086e-01,  4.9971e-01,\n",
       "          9.6895e-01,  3.2231e-01, -3.5484e+00, -6.8193e-01,  1.6604e-02,\n",
       "         -3.6783e+00],\n",
       "        [-3.6093e+00, -7.4371e-01,  6.0595e-01,  4.4240e+00, -7.2515e+00,\n",
       "         -2.4708e+00, -2.4921e+00,  1.3772e+00,  3.7451e+00, -2.2368e+00,\n",
       "          1.5078e-01],\n",
       "        [-2.5818e+00,  3.0675e+00,  1.1968e+00,  2.9686e-01, -8.1007e-01,\n",
       "          2.1248e-01,  4.8094e-01, -2.1006e+00,  1.7380e-01, -9.8288e-01,\n",
       "         -1.9918e+00],\n",
       "        [-1.0568e+00, -1.8139e+00,  3.3331e-01,  1.7818e+00,  1.0235e+00,\n",
       "          2.9380e+00,  3.6601e+00, -8.7547e-01, -8.0221e-01, -4.5951e+00,\n",
       "         -2.2427e+00],\n",
       "        [ 9.2685e-01, -3.4258e+00,  6.1899e-01,  8.5329e-01, -4.9865e-01,\n",
       "         -4.1488e+00, -1.3580e+00,  6.2821e-01, -4.4864e-01,  8.4122e-01,\n",
       "          5.7204e+00],\n",
       "        [ 6.0465e-01, -1.0818e+00,  4.8279e-01,  6.5027e-01, -4.8414e+00,\n",
       "         -1.0001e+00, -3.8165e+00,  4.6143e+00,  2.4655e+00,  7.4250e-01,\n",
       "         -8.3496e-01],\n",
       "        [-4.0435e+00, -7.9153e-01,  2.3320e-03,  1.2830e+00, -6.5590e+00,\n",
       "         -4.3700e-01,  1.3084e+00, -9.9220e-01,  5.3260e+00, -2.2056e+00,\n",
       "         -1.1279e+00],\n",
       "        [-1.2013e+00, -8.4099e-01,  3.3725e+00,  2.0724e+00, -1.3805e+00,\n",
       "         -1.9600e+00, -6.4954e-01,  7.6776e-01, -7.3516e-01, -2.6221e+00,\n",
       "          1.9768e-01],\n",
       "        [-4.1819e+00,  4.3352e+00,  2.4617e+00,  1.3351e+00, -1.9961e+00,\n",
       "          4.1617e-01,  4.6957e-01, -2.3486e+00,  4.9594e-01, -2.4877e+00,\n",
       "         -3.8639e+00],\n",
       "        [ 2.0243e+00, -4.3516e+00,  9.8796e-01,  3.0047e+00, -6.1682e+00,\n",
       "         -2.1364e+00, -7.2150e+00,  1.0152e+01,  1.9502e+00,  2.8704e-01,\n",
       "          6.4155e-01],\n",
       "        [-3.9101e+00,  1.5968e+00,  1.8539e+00,  2.8984e+00, -1.3475e+00,\n",
       "         -7.3944e-01,  1.2170e+00, -2.7581e+00, -1.0496e-01, -3.4064e+00,\n",
       "         -8.4655e-01],\n",
       "        [-2.2563e-01, -1.7527e+00, -5.3946e-01,  3.7494e-01, -2.8268e+00,\n",
       "         -5.2747e+00, -2.9410e+00,  2.4601e-01,  1.6091e+00,  3.0100e+00,\n",
       "          6.3604e+00],\n",
       "        [ 3.4680e-01, -3.2023e+00,  2.4321e+00,  1.9429e+00, -3.9970e+00,\n",
       "         -1.0251e+00, -1.3647e+00,  3.6027e+00,  1.4384e+00, -2.6181e+00,\n",
       "         -8.7847e-01],\n",
       "        [ 1.9647e+00, -3.3579e+00,  1.5171e+00,  1.8882e+00, -3.6333e+00,\n",
       "         -1.5993e+00, -4.6879e+00,  6.9591e+00,  6.1170e-01, -2.3211e-01,\n",
       "          3.9925e-01],\n",
       "        [-2.1671e+00,  6.2649e-01,  1.0277e+00,  3.5291e+00, -2.6987e+00,\n",
       "         -1.3626e+00, -9.9645e-01, -7.3789e-01,  3.2795e-01, -2.3740e+00,\n",
       "         -3.4045e-01],\n",
       "        [-4.0586e+00,  8.1742e-01,  1.4994e+00,  9.8038e-01, -2.7001e+00,\n",
       "          1.1692e+00,  2.0164e+00, -9.0384e-01,  2.0456e+00, -3.2200e+00,\n",
       "         -2.7704e+00],\n",
       "        [-1.6506e+00, -3.8059e-01, -4.8232e-01,  3.4019e+00, -3.2465e+00,\n",
       "          1.9095e-01, -1.2344e+00,  1.1540e+00,  1.2075e+00, -2.1148e+00,\n",
       "         -1.0588e+00],\n",
       "        [ 2.3075e-01, -3.1364e+00,  3.4080e-01,  1.9757e+00, -5.1273e+00,\n",
       "         -1.3831e+00, -3.2985e+00,  5.0800e+00,  2.5688e+00, -4.8773e-01,\n",
       "          4.9018e-01],\n",
       "        [-2.8912e+00,  1.2612e+00,  2.6707e+00, -3.0389e-01, -3.2819e+00,\n",
       "         -9.1767e-01,  8.2901e-01, -1.2402e+00,  2.1857e+00, -1.5741e+00,\n",
       "         -1.7516e+00],\n",
       "        [-3.8206e+00, -4.6903e+00, -1.5173e+00,  3.5121e+00, -9.1875e+00,\n",
       "         -1.8968e-01, -4.4496e-01,  3.6891e+00,  6.9849e+00, -2.9520e+00,\n",
       "         -4.2253e-03],\n",
       "        [-1.9458e+00,  4.8976e-01,  4.1773e+00,  2.9758e+00, -1.2409e+00,\n",
       "         -2.1163e+00, -1.2186e+00,  5.3422e-01, -1.5102e+00, -3.2112e+00,\n",
       "         -5.6750e-01],\n",
       "        [ 6.8174e+00, -2.9289e+00, -2.1939e+00, -5.6175e-02, -7.6262e-01,\n",
       "          5.1330e-01, -2.2864e+00,  2.3099e+00, -3.6879e-01,  1.5634e+00,\n",
       "         -3.1503e-01],\n",
       "        [-8.4295e-01, -1.7838e+00,  8.6050e-01,  7.0286e+00, -1.9673e+00,\n",
       "         -1.9510e-01, -5.4512e-01,  1.3758e-01, -1.4627e+00, -5.7353e+00,\n",
       "         -1.1060e+00],\n",
       "        [ 1.3598e+00, -4.2162e+00, -1.3069e+00,  3.4051e+00, -5.8922e+00,\n",
       "         -1.4480e+00, -5.6089e+00,  7.6045e+00,  2.5831e+00,  3.2203e-01,\n",
       "          1.3522e+00],\n",
       "        [-2.4176e+00,  1.4326e+00,  4.4787e+00,  1.5015e+00, -1.3581e+00,\n",
       "         -2.7943e+00, -6.0995e-01, -1.0273e+00, -8.5105e-01, -2.2270e+00,\n",
       "         -1.7101e-01],\n",
       "        [-2.3288e+00,  1.1929e+00,  1.0473e+00, -2.3427e-01, -3.3664e+00,\n",
       "          1.3742e-01,  6.1652e-01, -8.4378e-01,  2.5526e+00, -1.0456e+00,\n",
       "         -2.1551e+00],\n",
       "        [-2.1237e+00, -1.4536e+00,  1.2554e+00,  5.1875e+00, -2.9398e+00,\n",
       "         -5.4895e-01, -4.4793e-01,  6.6684e-01,  1.1288e-01, -4.5730e+00,\n",
       "         -7.9826e-01],\n",
       "        [ 6.8784e-01, -3.0848e+00,  5.8475e-01,  1.8095e+00, -3.4253e+00,\n",
       "         -4.4578e+00, -3.8398e+00,  3.0461e+00,  1.0400e+00,  1.2072e+00,\n",
       "          4.6109e+00],\n",
       "        [-1.0562e+00,  1.2714e+00,  3.8496e+00,  1.3048e+00, -2.1015e+00,\n",
       "         -2.7966e+00, -2.9858e+00,  1.6743e+00, -6.4722e-01, -7.0640e-01,\n",
       "         -2.9012e-01],\n",
       "        [-2.5906e+00, -2.2939e+00,  1.0338e+00,  1.5660e+00,  6.1105e-01,\n",
       "          2.3757e+00,  7.1664e+00, -4.9686e+00,  2.9154e-01, -6.3799e+00,\n",
       "         -1.7894e+00],\n",
       "        [-2.1657e+00, -9.2176e-01, -2.3006e+00,  7.2389e-02, -2.6983e-01,\n",
       "          4.5431e+00,  3.8126e+00, -5.2874e-01,  1.7445e+00, -2.6143e+00,\n",
       "         -2.8373e+00],\n",
       "        [-2.5755e+00, -1.4776e-01,  1.1674e+00,  3.4293e+00, -1.5381e+00,\n",
       "          2.5143e-01,  3.1764e-01,  2.5348e-02, -1.7068e-01, -3.6240e+00,\n",
       "         -1.2195e+00],\n",
       "        [-5.2554e-01, -1.8376e+00,  1.4036e+00,  2.2218e+00, -4.3871e+00,\n",
       "         -1.7956e+00, -2.5817e+00,  3.1846e+00,  1.7243e+00, -1.1957e+00,\n",
       "          1.4967e-01],\n",
       "        [-9.4762e-01, -2.1937e+00,  1.1479e+00,  3.1904e+00, -4.7885e+00,\n",
       "         -2.0452e+00, -2.6032e+00,  3.0211e+00,  1.8339e+00, -1.7216e+00,\n",
       "          5.6954e-01],\n",
       "        [ 1.0147e+00, -2.1459e+00,  3.4323e+00,  2.7253e-01, -1.9329e+00,\n",
       "         -4.2718e+00, -2.0405e+00,  1.4320e+00, -1.5864e-01,  1.0548e-02,\n",
       "          2.8270e+00],\n",
       "        [-2.0826e-01, -3.1296e+00, -8.2944e-01,  1.7590e+00, -2.9067e+00,\n",
       "         -2.3666e-01,  4.0826e-01,  8.2825e-01,  1.9568e+00, -1.6546e+00,\n",
       "          8.2108e-01],\n",
       "        [-2.3936e+00,  6.8815e+00,  7.4134e-03, -1.0988e+00, -2.8623e-01,\n",
       "          3.9557e-01, -1.1779e+00, -3.1858e+00, -5.9720e-02,  1.9203e+00,\n",
       "         -2.8494e+00]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_hiddens = torch.eye( 11 )\n",
    "labels_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CosineEmbeddingLoss()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5356, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
