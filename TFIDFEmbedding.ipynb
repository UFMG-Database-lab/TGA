{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b406ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import Pool\n",
    "from collections import namedtuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import networkx as nx\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords as stopwords_by_lang\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4cb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.utils import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test', 'X_val', 'y_val'), 19907)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/Documents/datasets/acm/')\n",
    "g = dataset.get_fold_instances(10, with_val=True)\n",
    "fold = next(g)\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc81a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f03afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "replace_patterns = [\n",
    "    ('<[^>]*>', ''),                                    # remove HTML tags\n",
    "    ('(\\D)\\d\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d:\\d\\d(\\D)', '\\\\1 ParsedTime \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D\\D)\\d\\d\\d\\D\\D\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedPhoneNum \\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\-\\d\\d\\d\\d(\\D)', '\\\\1 ParsedZipcodePlusFour \\\\2'),\n",
    "    ('(\\D)\\d(\\D)', '\\\\1ParsedOneDigit\\\\2'),\n",
    "    ('(\\D)\\d\\d(\\D)', '\\\\1ParsedTwoDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d(\\D)', '\\\\1ParsedThreeDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d(\\D)', '\\\\1ParsedFourDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedFiveDigits\\\\2'),\n",
    "    ('(\\D)\\d\\d\\d\\d\\d\\d(\\D)', '\\\\1ParsedSixDigits\\\\2'),\n",
    "    ('\\d+', 'ParsedDigits')\n",
    "]\n",
    "\n",
    "compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "\n",
    "def generate_preprocessor(replace_patterns):\n",
    "    compiled_replace_patterns = [(re.compile(p[0]), p[1]) for p in replace_patterns]\n",
    "    def preprocessor(text):\n",
    "        for pattern, replace in compiled_replace_patterns:\n",
    "            text = re.sub(pattern, replace, text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    return preprocessor\n",
    "\n",
    "generated_patters=generate_preprocessor(replace_patterns)\n",
    "\n",
    "def preprocessor(text):\n",
    "    # For each pattern, replace it with the appropriate string\n",
    "    for pattern, replace in compiled_replace_patterns:\n",
    "        text = re.sub(pattern, replace, text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mindf=2, lan='english', stopwords='nltk', model='topk', k=500, verbose=False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "        self.mindf = mindf\n",
    "        self.le = LabelEncoder()\n",
    "        self.verbose = verbose\n",
    "        self.lan = lan\n",
    "        if stopwords == 'nltk':\n",
    "            self.stopwordsSet = stopwords_by_lang.words(lan)\n",
    "        elif stopwords == 'scikit':\n",
    "            self.stopwordsSet = stop_words\n",
    "        else:\n",
    "            self.stopwordsSet = []\n",
    "        self.model =  model\n",
    "        self.k     = k\n",
    "        self.analyzer = TfidfVectorizer(preprocessor=preprocessor, min_df=mindf)#.build_analyzer()\n",
    "        self.local_analyzer = self.analyzer.build_analyzer()\n",
    "        self.analyzer.set_params( analyzer=self.local_analyzer )\n",
    "        self.node_mapper      = {}\n",
    "        \n",
    "    def analyzer_doc(self, doc):\n",
    "        return self.local_analyzer(doc)\n",
    "    def fit(self, X, y):\n",
    "        self.N = len(X)\n",
    "        y = self.le.fit_transform( y )\n",
    "        self.n_class = len(self.le.classes_)\n",
    "        docs_in_terms = []\n",
    "        \n",
    "        with Pool(processes=18) as p:\n",
    "            #docs = map(self.local_analyzer, X)\n",
    "            for doc_in_terms in tqdm(p.imap(self.analyzer_doc, X), total=self.N, disable=not self.verbose):\n",
    "                doc_in_terms = list(set(map( self._filter_fit_, list(doc_in_terms) ))) \n",
    "                docs_in_terms.extend(doc_in_terms)\n",
    "        \n",
    "        self.term_freqs       = Counter(docs_in_terms)\n",
    "        self.term_freqs       = { term:v for (term,v) in self.term_freqs.items() if v >= self.mindf }\n",
    "        self.node_mapper      = { term: self.node_mapper.setdefault(term, len(self.node_mapper)+1)\n",
    "                                 for term in self.term_freqs.keys() }\n",
    "        self.node_mapper['<BLANK>'] = 0\n",
    "        self.term_freqs['<BLANK>']  = self.N\n",
    "        \n",
    "        self.node_mapper['<UNK>']   = len(self.node_mapper)\n",
    "        self.term_freqs['<UNK>']  = self.N\n",
    "        self.vocab_size = len(self.node_mapper)\n",
    "        \n",
    "        self.term_array = [ term for (term,term_id) in sorted(self.node_mapper.items(), key=lambda x: x[1]) ]\n",
    "        \n",
    "        self.fi_ = np.array([ np.log2( (self.N+1)/(self.term_freqs[term]+1) ) for term in self.term_array ])\n",
    "            \n",
    "        return self\n",
    "    def _filter_transform_(self, term):\n",
    "        if term in self.stopwordsSet:\n",
    "            return '<STPW>'\n",
    "        if term not in self.node_mapper:\n",
    "            return '<UNK>'\n",
    "        return term\n",
    "    def _filter_fit_(self, term):\n",
    "        if term in self.stopwordsSet:\n",
    "            return '<STPW>'\n",
    "        return term\n",
    "    def _model_(self, doc):\n",
    "        doc_counter = Counter(doc)\n",
    "        doc = np.array(list(doc_counter.keys()))\n",
    "        if len(doc) > self.k:\n",
    "            weigths = np.array([ self.fi_[t] for t in doc ])\n",
    "            weigths = softmax(weigths)\n",
    "            if self.model == 'topk':\n",
    "                doc = doc[(-weigths).argsort()[:self.k]]\n",
    "            elif self.model == 'sample':\n",
    "                doc = np.random.choice(doc, size=self.k, replace=False, p=weigths)\n",
    "        TFs = np.array([ doc_counter[tid] for tid in doc ])\n",
    "        DFs = np.array([ self.term_freqs[self.term_array[tid]] for tid in doc ])\n",
    "        return doc, TFs, DFs\n",
    "    def transform(self, X, verbose=None):\n",
    "        verbose = verbose if verbose is not None else self.verbose\n",
    "        n = len(X)\n",
    "        terms_ = []\n",
    "        for i,doc_in_terms in tqdm(enumerate(map(self.analyzer_doc, X)), total=n, disable=not verbose):\n",
    "            doc_in_terms = map( self._filter_transform_, doc_in_terms )\n",
    "            #doc_in_terms = filter( lambda x: x != '<STPW>', doc_in_terms )\n",
    "            doc_tids = [ self.node_mapper[tid] for tid in doc_in_terms ]\n",
    "            doc_tids, TFs, DFs = self._model_(doc_tids)\n",
    "            terms_.append( (doc_tids, TFs, DFs) )\n",
    "        doc_tids, TFs, DFs = list(zip(*terms_))\n",
    "        return list(doc_tids), list(TFs), list(DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1066fc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98212339d5849e7aae2d523f3ddfc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(49336, 19907)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(mindf=1, verbose=True, stopwords=None)\n",
    "tokenizer.fit(fold.X_train, fold.y_train)\n",
    "tokenizer.vocab_size, tokenizer.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ea1fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd24f8338c848539d3e76c0df073c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_tids, TFs, DFs =  tokenizer.transform( fold.X_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81776f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1960ac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 4,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 3, 4,  ..., 0, 0, 0],\n",
       "        [4, 3, 2,  ..., 0, 0, 0],\n",
       "        [1, 1, 2,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(list(map(torch.LongTensor, TFs)), batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3698a1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meting'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.term_array[15749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9afbeaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tokenizer.le.transform( fold.y_train )\n",
    "y_val   = tokenizer.le.transform( fold.y_val )\n",
    "y_test  = tokenizer.le.transform( fold.y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26c0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTFIDF_V1(nn.Module):\n",
    "    def __init__(self, vocab_size, hiddens, nclass, maxF=20, drop=.5,\n",
    "                 initrange=.5, negative_slope=99.):\n",
    "        super(AttentionTFIDF_V1, self).__init__()\n",
    "        self.hiddens        = hiddens\n",
    "        self.maxF           = maxF\n",
    "        self.value_emb      = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.query_emb      = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.key_emb        = nn.Embedding(vocab_size, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.TF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.DF_emb         = nn.Embedding(maxF, hiddens, scale_grad_by_freq=True, padding_idx=0)\n",
    "        self.fc             = nn.Linear(hiddens, nclass)\n",
    "        self.initrange      = initrange \n",
    "        self.negative_slope = negative_slope\n",
    "        self.drop_          = drop\n",
    "        self.init_weights()\n",
    "    def forward(self, doc_tids, TFs, DFs):\n",
    "        batch_size = doc_tids.size(0)\n",
    "        bx_packed  = doc_tids == 0\n",
    "        pad_mask   = bx_packed.logical_not()\n",
    "        doc_sizes  = pad_mask.sum(dim=1).view(batch_size, 1)\n",
    "        pad_mask   = pad_mask.view(*bx_packed.shape, 1)\n",
    "        pad_mask   = pad_mask.logical_and(pad_mask.transpose(1, 2))\n",
    "        \n",
    "        TFs     = torch.clamp( TFs, max=self.maxF-1 )\n",
    "        h_TFs   = self.TF_emb( TFs )\n",
    "        h_TFs   = F.dropout( h_TFs, p=self.drop_, training=self.training )\n",
    "        \n",
    "        DFs     = torch.clamp( DFs, max=self.maxF-1 )\n",
    "        h_DFs   = self.DF_emb( DFs )\n",
    "        h_DFs   = F.dropout( h_DFs, p=self.drop_, training=self.training )\n",
    "        \n",
    "        h_query = self.query_emb( doc_tids )\n",
    "        h_query = h_query + h_TFs + h_DFs\n",
    "        #h_query = torch.tanh( h_query )\n",
    "        h_query = F.dropout( h_query, p=self.drop_, training=self.training )\n",
    "        \n",
    "        h_key = self.key_emb( doc_tids )\n",
    "        h_key = h_key + h_TFs + h_DFs\n",
    "        #h_key = torch.tanh( h_key )\n",
    "        h_key = F.dropout( h_key, p=self.drop_, training=self.training )\n",
    "        \n",
    "        co_weights  = torch.bmm( h_key, h_query.transpose( 1, 2 ) )\n",
    "        #co_weights = torch.tanh( co_weights )\n",
    "        #co_weights  = co_weights / torch.pow(1.+co_weights, 2.)\n",
    "        co_weights  = F.leaky_relu( co_weights, negative_slope=self.negative_slope)\n",
    "        \n",
    "        #co_weights[pad_mask.logical_not()] = 0. # Set the 3D-pad mask values to\n",
    "        #co_weights = torch.tanh(co_weights)\n",
    "        \n",
    "        co_weights[pad_mask.logical_not()] = float('-inf') # Set the 3D-pad mask values to -inf (=0 in sigmoid)\n",
    "        co_weights = torch.sigmoid(co_weights)\n",
    "        \n",
    "        weights = co_weights.sum(axis=2) / doc_sizes\n",
    "        weights[bx_packed] = float('-inf') # Set the 2D-pad mask values to -inf  (=0 in softmax)\n",
    "        \n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        weights = torch.where(torch.isnan(weights), torch.zeros_like(weights), weights)\n",
    "        weights = weights.view( *weights.shape, 1 )\n",
    "        \n",
    "        h_value = self.value_emb( doc_tids )\n",
    "        h_value = h_value + h_TFs + h_DFs\n",
    "        h_value = F.dropout( h_value, p=self.drop_, training=self.training )\n",
    "        \n",
    "        docs_h = h_value * weights\n",
    "        docs_h = docs_h.sum(axis=1)\n",
    "        docs_h = F.dropout( docs_h, p=self.drop_, training=self.training )\n",
    "        docs_h = self.fc(docs_h)\n",
    "        return docs_h, weights, co_weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.TF_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.DF_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.query_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.key_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.value_emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.fc.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02686300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e62d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 1000\n",
    "max_epochs = 30\n",
    "drop=0.8\n",
    "max_drop=.8 # default .8\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = 32 # default 32\n",
    "k = 128 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75973e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train(param):\n",
    "    X, y = zip(*param)\n",
    "    doc_tids, TFs, DFs = tokenizer.transform(X, verbose=False)\n",
    "    doc_tids = pad_sequence(list(map(torch.LongTensor, doc_tids)), batch_first=True, padding_value=0)\n",
    "    \n",
    "    TFs = pad_sequence(list(map(torch.tensor, TFs)), batch_first=True, padding_value=0)\n",
    "    TFs = torch.LongTensor(torch.log2(TFs+1).round().long())\n",
    "    \n",
    "    DFs = pad_sequence(list(map(torch.tensor, DFs)), batch_first=True, padding_value=0)\n",
    "    DFs = torch.LongTensor(torch.log2(DFs+1).round().long())\n",
    "    \n",
    "    return doc_tids, TFs, DFs, torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53a2b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4294136070>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "282a450c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a0756a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SimpleClassifier(tokenizer.vocab_size, 300, tokenizer.n_class, dropout=drop).to( device )\n",
    "ab = AttentionTFIDF_V1(tokenizer.vocab_size, 300, tokenizer.n_class,\n",
    "                       initrange=0.2, drop=drop).to( device )\n",
    "#ab = AttentionBag(tokenizer.vocab_size, 300, tokenizer.n_class, drop=drop).to( device )\n",
    "#ab = NotTooSimpleClassifier(tokenizer.vocab_size, 300, tokenizer.n_class, dropout1=drop, dropout2=drop).to( device )\n",
    "tokenizer.k = k\n",
    "optimizer = optim.AdamW( ab.parameters(), lr=5e-2, weight_decay=5e-4)\n",
    "#optimizer = optim.AdamW( ab.parameters(), lr=5e-3, weight_decay=5e-3)\n",
    "loss_func_cel = nn.CrossEntropyLoss().to( device )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=.95,\n",
    "                                                       patience=10, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=.98, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8736bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e497917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c178d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3382e724c440c5a56370f1a1d534e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f912b92d250b4e958ce3f68dd7545890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/22402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.28/2.543 Drop: 0.2107 ACC: 0.26337                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "Val loss: 2.2793 ACC: 0.26373                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "best = 99999.\n",
    "counter = 1\n",
    "loss_val = 1.\n",
    "eps = .9\n",
    "dl_val = DataLoader(list(zip(fold.X_val, y_val)), batch_size=batch_size,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=num_workers)\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    dl_train = DataLoader(list(zip(fold.X_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_train, num_workers=num_workers)\n",
    "    loss_train  = 0.\n",
    "    with tqdm(total=len(y_train)+len(y_val), smoothing=0., desc=f\"Epoch {e+1}\") as pbar:\n",
    "        total = 0\n",
    "        correct  = 0\n",
    "        ab.train()\n",
    "        tokenizer.model = 'sample'\n",
    "        tokenizer.k = k\n",
    "        for i, (doc_tids, TFs, DFs, y) in enumerate(dl_train):\n",
    "            doc_tids = doc_tids.to( device )\n",
    "            TFs      = TFs.to( device )\n",
    "            DFs      = DFs.to( device )\n",
    "            y        = y.to( device )\n",
    "            \n",
    "            pred_docs,_,_ = ab( doc_tids, TFs, DFs )\n",
    "            pred_docs     = torch.softmax(pred_docs, dim=1)\n",
    "            loss          = loss_func_cel(pred_docs, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            total      += len(y)\n",
    "            y_pred      = pred_docs.argmax(axis=1)\n",
    "            correct    += (y_pred == y).sum().item()\n",
    "            #ab.drop_ =  np.power((correct/total),loss_val)\n",
    "            #ab.drop_ =  np.power((correct/total),4)\n",
    "            ab.drop_ =  (correct/total)*max_drop\n",
    "            \n",
    "            toprint  = f\"Train loss: {loss_train/(i+1):.5}/{loss.item():.5} \"\n",
    "            toprint += f'Drop: {ab.drop_:.5} '\n",
    "            toprint += f'ACC: {correct/total:.5} '\n",
    "            \n",
    "            print(toprint, end=f\"{' '*100}\\r\")\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            del doc_tids, TFs\n",
    "            del DFs, y, pred_docs\n",
    "            del loss, y_pred\n",
    "        loss_train = loss_train/(i+1)\n",
    "        print()\n",
    "        #print(ab.drop_)\n",
    "        total = 0\n",
    "        correct  = 0\n",
    "        ab.eval()\n",
    "        tokenizer.model = 'topk'\n",
    "        tokenizer.k = 512\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.\n",
    "            for i, (doc_tids, TFs, DFs, y) in enumerate(dl_val):\n",
    "                doc_tids = doc_tids.to( device )\n",
    "                TFs      = TFs.to( device )\n",
    "                DFs      = DFs.to( device )\n",
    "                y        = y.to( device )\n",
    "\n",
    "                pred_docs,_,_ = ab( doc_tids, TFs, DFs )\n",
    "                pred_docs     = torch.softmax(pred_docs, dim=1)\n",
    "                loss          = loss_func_cel(pred_docs, y)\n",
    "\n",
    "                loss_val   += loss.item()\n",
    "                total      += len(y)\n",
    "                y_pred      = pred_docs.argmax(axis=1)\n",
    "                correct    += (y_pred == y).sum().item()\n",
    "                \n",
    "                print(f'Val loss: {loss_val/(i+1):.5} ACC: {correct/total:.5}', end=f\"{' '*100}\\r\")\n",
    "                pbar.update( len(y) )\n",
    "                \n",
    "                del doc_tids, TFs, DFs, y\n",
    "                del pred_docs, loss\n",
    "            print()\n",
    "            loss_val   = (loss_val/(i+1))\n",
    "            scheduler.step(loss_val)\n",
    "\n",
    "            if best-loss_val > 0.0001 :\n",
    "                best = loss_val\n",
    "                counter = 1\n",
    "                print(f'New Best Val loss: {best:.5}', end=f\"{' '*100}\\n\")\n",
    "                best_model = copy.deepcopy(ab).to('cpu')\n",
    "            elif counter > max_epochs:\n",
    "                print(f'Best Val loss: {best:.5}', end=f\"{' '*100}\\n\")\n",
    "                break\n",
    "            else:\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_test = 'cpu'\n",
    "ab = copy.deepcopy(best_model).to(device_test)\n",
    "ab.eval()\n",
    "loss_total = 0\n",
    "correct_t = 0\n",
    "total_t = 0\n",
    "dl_test = DataLoader(list(zip(fold.X_test, y_test)), batch_size=128,\n",
    "                         shuffle=False, collate_fn=collate_train, num_workers=num_workers)\n",
    "tokenizer.k = 256\n",
    "for i, (docs_tids_t, TFs_t, DFs_t, y_t) in enumerate(dl_test):\n",
    "    docs_tids_t = docs_tids_t.to( device_test )\n",
    "    TFs_t       = TFs_t.to( device_test )\n",
    "    DFs_t       = DFs_t.to( device_test )\n",
    "    y_t         = y_t.to( device_test )\n",
    "\n",
    "    pred_docs_t,weigths,coweights = ab( docs_tids_t, TFs_t, DFs_t )\n",
    "    sofmax_docs_t = torch.softmax(pred_docs_t, dim=1)\n",
    "\n",
    "    y_pred_t    = sofmax_docs_t.argmax(axis=1)\n",
    "    correct_t  += (y_pred_t == y_t).sum().item()\n",
    "    total_t    += len(y_t)\n",
    "    loss_total += loss_func_cel(sofmax_docs_t, y_t)\n",
    "\n",
    "    print(f'Test loss: {loss_total.item()/(i+1):.5} ACC: {correct_t/total_t:.5}', end=f\"{' '*100}\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = torch.FloatTensor([[1,0,0],[0,1,0],[0,0,1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
