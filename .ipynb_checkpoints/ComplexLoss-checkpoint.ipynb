{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from TGA.utils import Dataset, GraphsizePretrained\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:25, 15386.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 s, sys: 685 ms, total: 26.3 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('/home/Documentos/datasets/classification/datasets/webkb/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=True))\n",
    "fold._fields, len(fold.X_train)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(fold.y_train)\n",
    "y_val = le.transform(fold.y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6553/6553 [00:05<00:00, 1164.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.96 s, sys: 95.9 ms, total: 7.06 s\n",
      "Wall time: 7.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsizePretrained(encoding=None,\n",
       "                    pretrained_vec='/home/Documentos/Universidade/LBD/pretrained_vectors/glove/glove.6B.300d.txt',\n",
       "                    verbose=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88285, 20749)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11585),\n",
       " (1, 19771),\n",
       " (2, 8994),\n",
       " (3, 3714),\n",
       " (4, 8478),\n",
       " (5, 12273),\n",
       " (6, 2735)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 drop=.5, n_heads=8, attn_drop=.5,\n",
    "                 activation=F.leaky_relu, n_convs=2,\n",
    "                 first_hidden='emb', encoders={'term','label'},\n",
    "                 device='cpu:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.n_hiddens = hidden_dim\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        #self.norm = nn.BatchNorm1d(hidden_dim).to(self.device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=activation,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        self.norm_projs = [\n",
    "            nn.BatchNorm1d(hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        with G.local_scope():\n",
    "            h = G.ndata[self.first_hidden].float()\n",
    "            H = torch.zeros( h.shape[0], self.n_hiddens )\n",
    "            for (k, mask) in kwargs.items():\n",
    "                if k in self.encoders:\n",
    "                    if mask is not None:\n",
    "                        bla = self.encoders[k]( h[ mask ] )\n",
    "                        h[ mask ] = bla\n",
    "                    else:\n",
    "                        h = self.encoders[k]( h )\n",
    "\n",
    "            for l, conv in enumerate(self.layers):\n",
    "                h = conv(G, h)\n",
    "                h = h.view(h.shape[0], -1)\n",
    "                h = self.down_proj[l]( h )\n",
    "                h = self.norm_projs[l]( h )\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, n_heads=16, drop=.5, attn_drop=.5, device='cuda:0'):\n",
    "        super(ClassifierGAT, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(in_dim, hidden_dim).to(torch.device(device))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device)),\n",
    "            GATConv(n_heads*hidden_dim, hidden_dim, num_heads=n_heads, activation=F.leaky_relu,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(torch.device(device))\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(n_heads*hidden_dim + hidden_dim, 1).to(torch.device(device))\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(torch.device(device))\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d( n_heads*hidden_dim + hidden_dim )\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        self.classify = nn.Linear( n_heads*hidden_dim + hidden_dim, n_classes).to(torch.device(device))\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = G.ndata['emb'].float()\n",
    "        he = self.encoder(h)\n",
    "        h = he\n",
    "        for conv in self.layers:\n",
    "            h = conv(G, h)\n",
    "            h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        # CONCAT he E hg\n",
    "        hg = torch.cat((h,he), 1)\n",
    "        hg = self.norm( hg )\n",
    "        hg = self.drop( hg )\n",
    "        hg = self.pooling(G, hg)\n",
    "        \n",
    "        pred = self.classify( hg )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl_list = []\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        if len(g) > 0:\n",
    "            g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl_list.append( g_dgl )\n",
    "    \n",
    "    Gs_dgl = dgl.batch(Gs_dgl_list)\n",
    "    \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    #subgraph = graph_builder.g.subgraph(idx_terms)\n",
    "    #big_graph_dgl.from_networkx(subgraph, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, Gs_dgl, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(torch.nn.Module):\n",
    "    def __init__(self, input_l, hidden_l, nclass, n_heads=1,\n",
    "                drop=0.5, attn_drop=0.5, loss=None, n_convs=1,activation=None,\n",
    "                 device='cuda:0'):\n",
    "        \n",
    "        super(TGA, self).__init__()\n",
    "        \n",
    "        self.gat_global = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop, n_convs=n_convs,\n",
    "                 activation=activation, device=device ).to(device)\n",
    "        \n",
    "        \n",
    "        self.gat_local = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop, n_convs=n_convs, encoders={'terms'},\n",
    "                 activation=activation, device=device ).to(device)\n",
    "        \n",
    "        self.norm_label = nn.BatchNorm1d(hidden_l).to(device)\n",
    "        #self.norm_docs = nn.BatchNorm1d(hidden_l).to(device)\n",
    "\n",
    "        #self.gate = nn.Linear( input_l+hidden_l, 1 ).to(device)\n",
    "        self.gate = nn.Sequential(\n",
    "          nn.Linear( input_l+hidden_l, hidden_l ),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear( hidden_l, 1 )\n",
    "        ).to(device)\n",
    "        self.feat = nn.Linear( input_l+hidden_l, hidden_l ).to(device)\n",
    "        self.gap  = GlobalAttentionPooling(self.gate, feat_nn=self.feat).to(device)\n",
    "        \n",
    "        #self.nclass  = nclass\n",
    "        #self.fc1     = nn.Linear( hidden_l, hidden_l ).to(device)\n",
    "        #self.fc2     = nn.Linear(  hidden_l//2, self.nclass ).to(device)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.fc_global = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear( hidden_l, hidden_l )\n",
    "        )\n",
    "        \n",
    "        self.fc_local = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear( hidden_l, hidden_l )\n",
    "        )\n",
    "        \n",
    "        self.fc_local_classifier = nn.Sequential(\n",
    "          nn.Linear( hidden_l, hidden_l ),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear( hidden_l, nclass )\n",
    "        )\n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, G, gs, label_idx=None):\n",
    "        if label_idx is None:\n",
    "            label_idx = G.ndata['label'].nonzero().flatten()\n",
    "            \n",
    "        terms_idx = range(len(label_idx),len(graph_builder.g))\n",
    "        \n",
    "        h_global  = self.gat_global(G, label=label_idx, term=terms_idx)\n",
    "\n",
    "        h_labels  = h_global[label_idx]\n",
    "        #h_labels  = self.norm_label(h_labels)\n",
    "        h_labels  = self.fc_global(h_labels)\n",
    "\n",
    "        gs.ndata['emb'] = h_global[gs.ndata['idx'].reshape(-1)]\n",
    "        h_local         = self.gat_local(gs, terms=None)\n",
    "        \n",
    "        terms_to_pred   = G.ndata['emb'][gs.ndata['idx'].reshape(-1)].float()\n",
    "        \n",
    "        h_docs          = self.gap( gs, torch.cat((h_local, terms_to_pred), 1 ) )\n",
    "        h_docs          = self.fc_local(h_docs)\n",
    "        #h_docs          = self.norm_docs(h_docs)\n",
    "        pred_docs       = self.fc_local_classifier(h_docs)\n",
    "        \n",
    "        #h_docs_pred = self.fc1(h_docs)\n",
    "        #h_docs_pred = self.fc2(h_docs_pred)\n",
    "        #h_docs_pred = nn.softmax(h_docs_pred, 1)\n",
    "        \n",
    "        return h_docs, pred_docs, h_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 64\n",
    "input_l = 300\n",
    "n_heads = 4\n",
    "drop=0.3\n",
    "batch_size=64\n",
    "attn_drop=0.3\n",
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5., 6.],\n",
       "        [4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat( (torch.Tensor([[1,2,3],[4,5,6]]),torch.Tensor([[4,5,6],[7,8,9]])), 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGA(\n",
       "  (gat_global): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (label): Linear(in_features=300, out_features=64, bias=True)\n",
       "      (term): Linear(in_features=300, out_features=64, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gat_local): GenericGAT(\n",
       "    (encoders): ModuleDict(\n",
       "      (terms): Linear(in_features=300, out_features=64, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GATConv(\n",
       "        (fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "      (1): GATConv(\n",
       "        (fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "        (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.3, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        (res_fc): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_label): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gate): Sequential(\n",
       "    (0): Linear(in_features=364, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (feat): Linear(in_features=364, out_features=64, bias=True)\n",
       "  (gap): GlobalAttentionPooling(\n",
       "    (gate_nn): Sequential(\n",
       "      (0): Linear(in_features=364, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "    (feat_nn): Linear(in_features=364, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc_global): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc_local): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc_local_classifier): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tga = TGA(input_l, hidden_l, nclass=graph_builder.n_class,\n",
    "          activation=None,\n",
    "          n_heads=n_heads, drop=drop, attn_drop=attn_drop, n_convs=2).to(device)\n",
    "tga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TGA.lossweight import cross_entropy\n",
    "class NpairLoss(nn.Module):\n",
    "    \"\"\"the multi-class n-pair loss\"\"\"\n",
    "    def __init__(self, l2_reg=0.02):\n",
    "        super(NpairLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, anchor, target=None, positive=None):\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = target.view(target.size(0), 1)\n",
    "            target = (target == torch.transpose(target, 0, 1)).float()\n",
    "            target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
    "        else:\n",
    "            target = torch.eye(batch_size).to(anchor.device)\n",
    "\n",
    "        if positive is not None:\n",
    "            logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
    "        else:\n",
    "            logit = torch.matmul(anchor, torch.transpose(anchor, 0, 1))\n",
    "            l2_loss = torch.sum(anchor**2) / batch_size\n",
    "        \n",
    "        loss_ce = cross_entropy(logit, target)\n",
    "\n",
    "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
    "        return loss.float()\n",
    "class SelfDistLoss(nn.Module):\n",
    "    def __init__(self, l2_reg=0.02, eps = 0.00003, margin=-0.3):\n",
    "        super(SelfDistLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.eps = eps\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, hiddens):\n",
    "        L = torch.matmul(hiddens, hiddens.T)\n",
    "        L = F.sigmoid(L)\n",
    "        L = (L - (L.diag()-self.margin)).float()\n",
    "        L = F.relu(L)\n",
    "        #L = torch.exp( L )\n",
    "        L = ( L > 0. ).float() * torch.exp( L )\n",
    "        #L = F.normalize(L)\n",
    "        \n",
    "        values = L.sum(axis=1)\n",
    "        svalue = max((values > 0.).sum(), self.eps)\n",
    "\n",
    "        return values.sum()/svalue # AVG of non-zero values\n",
    "    \n",
    "class SelfDistLoss2(nn.Module):\n",
    "    def __init__(self, l2_reg=0.02, eps = 0.00003):\n",
    "        super(SelfDistLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, hiddens):\n",
    "        L = torch.matmul(hiddens, hiddens.T)\n",
    "        L = (L - L.diag()).float()\n",
    "        L = F.sigmoid(L)\n",
    "        #L = F.relu(L)\n",
    "        #L = torch.exp( L )\n",
    "        L = torch.exp( L )\n",
    "        #L = F.normalize(L)\n",
    "\n",
    "        return L.mean(axis=1).mean()/np.e # AVG of non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.AdamW( tga.parameters(), lr=5e-3, weight_decay=5e-4)\n",
    "\n",
    "loss_func_npl = NpairLoss(l2_reg=5e-4)\n",
    "loss_func_cel = nn.CrossEntropyLoss()\n",
    "loss_func_spc = SelfDistLoss()\n",
    "loss_func_self_npl = NpairLoss(l2_reg=5e-4)\n",
    "\n",
    "#RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(10).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6165daf744074e119a711b39bf683ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e26532301f41c8a80ad6ea1016b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7376.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [7, 64] cannot be broadcast to indexing result of shape [7, 300]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-fa0d43ff98f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mh_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtga\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m#h_docs, pred_docs = tga( G, gs, y )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-1b74a7a00ddf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G, gs, label_idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mterms_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mh_global\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterms_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mh_labels\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mh_global\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d2de00f0a80a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mbla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [7, 64] cannot be broadcast to indexing result of shape [7, 300]"
     ]
    }
   ],
   "source": [
    "\n",
    "best = None\n",
    "nepochs = 25\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=3)\n",
    "    total_loss  = 0.\n",
    "    total_loss1 = 0.\n",
    "    total_loss2 = 0.\n",
    "    total_loss3 = 0.\n",
    "    with tqdm(total=len(y_train)+len(y_val), smoothing=0.) as pbar:\n",
    "        total = 1\n",
    "        correct_class = 0\n",
    "        correct_repre = 0\n",
    "        correct_both  = 0\n",
    "        tga.train()\n",
    "        for i, (G, gs, y) in enumerate(data_loader):\n",
    "            G = G.to( device )\n",
    "            gs = gs.to( device )\n",
    "            y = y.to( device )\n",
    "            \n",
    "            h_docs, pred_docs, h_labels = tga( G, gs )\n",
    "            #h_docs, pred_docs = tga( G, gs, y )\n",
    "            \n",
    "            \n",
    "            pred_docs = F.softmax(pred_docs)\n",
    "            pred_docs2 = F.softmax(torch.matmul(h_docs, h_labels.T))\n",
    "            pred_docs3 = pred_docs+pred_docs2\n",
    "            \n",
    "            loss1 = loss_func_npl( h_docs, y, positive=h_labels[y] )\n",
    "            loss2 = loss_func_cel(pred_docs, y)\n",
    "            loss3 = loss_func_self_npl(h_labels)\n",
    "            #loss3 = loss_func_spc(h_labels)\n",
    "            \n",
    "            loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total      += len(y)\n",
    "            \n",
    "            y_pred            = pred_docs.argmax(axis=1)\n",
    "            correct_repre    += (y_pred == y).sum()\n",
    "            total_loss1      += loss1.item()\n",
    "            \n",
    "            y_pred = pred_docs2.argmax(axis=1)\n",
    "            correct_class    += (y_pred == y).sum()\n",
    "            total_loss2      += loss2.item()\n",
    "            \n",
    "            y_pred = pred_docs3.argmax(axis=1)\n",
    "            correct_both     += (y_pred == y).sum()\n",
    "            total_loss3      += loss3.item()\n",
    "            \n",
    "            to_print   = f'(L)oss: {loss.item():.3}/{total_loss /(i+1):.3} '\n",
    "            to_print  += f'NP-L: {loss1.item():.3}/{total_loss1/(i+1):.3} '\n",
    "            to_print  += f'CE-L: {loss2.item():.3}/{total_loss2/(i+1):.3} '\n",
    "            to_print  += f'Sp-L: {loss3.item():.3}/{total_loss3/(i+1):.3} '\n",
    "            to_print  += f'Acc Cls: {(1.*correct_class/total).item():.3} '\n",
    "            to_print  += f'Repr: {(1.*correct_repre/total).item():.3} '\n",
    "            to_print  += f'Both: {(1.*correct_both/total).item():.3}'\n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {e} Acc: {(1.*correct_both/total).item():.3}')\n",
    "            \n",
    "            #break\n",
    "            if best is None or best > (total_loss/(i+1)):\n",
    "                hiddens_labels = h_labels\n",
    "                hiddens_docs = h_docs\n",
    "                best = total_loss/(i+1)\n",
    "            #del loss, h_labels, G, gs, loss1, loss2, pred_docs, h_docs\n",
    "            print(to_print, end=f\"{' '*100}\\r\")\n",
    "            del G, gs, y\n",
    "            del h_docs, pred_docs, h_labels\n",
    "            del  pred_docs2, pred_docs3\n",
    "            del loss1, loss2, loss3\n",
    "            #break\n",
    "        to_print   = f'(L)oss: {total_loss /(i+1):.3} '\n",
    "        to_print  += f'NP-L: {total_loss1/(i+1):.3} '\n",
    "        to_print  += f'CE-L: {total_loss2/(i+1):.3} '\n",
    "        to_print  += f'Sp-L: {total_loss3/(i+1):.3} '\n",
    "        to_print  += f'Acc Cls: {(1.*correct_class/total).item():.3} '\n",
    "        to_print  += f'Repr: {(1.*correct_repre/total).item():.3} '\n",
    "        to_print  += f'Both: {(1.*correct_both/total).item():.3}'\n",
    "        print(to_print, end=f\"{' '*100}\\n\")\n",
    "        \n",
    "        ############################################# EVALUATION #############################################\n",
    "        data_loader = DataLoader(list(zip(fold.X_val, y_val)), batch_size=batch_size,\n",
    "                                 shuffle=False, collate_fn=collate, num_workers=3)\n",
    "        total = 0\n",
    "        correct_class = 0\n",
    "        correct_repre = 0\n",
    "        correct_both  = 0\n",
    "        tga.eval()\n",
    "        for i, (G, gs, y) in enumerate(data_loader):\n",
    "            G = G.to( device )\n",
    "            gs = gs.to( device )\n",
    "            y = y.to( device )\n",
    "            \n",
    "            h_docs, pred_docs, h_labels = tga( G, gs )\n",
    "            \n",
    "            pred_docs = F.softmax(pred_docs)\n",
    "            pred_docs2 = F.softmax(torch.matmul(h_docs, h_labels.T))\n",
    "            pred_docs3 = pred_docs+pred_docs2\n",
    "            \n",
    "            total      += len(y)\n",
    "            \n",
    "            y_pred            = pred_docs.argmax(axis=1)\n",
    "            correct_repre    += (y_pred == y).sum()\n",
    "            \n",
    "            y_pred = pred_docs2.argmax(axis=1)\n",
    "            correct_class    += (y_pred == y).sum()\n",
    "            \n",
    "            y_pred = pred_docs3.argmax(axis=1)\n",
    "            correct_both     += (y_pred == y).sum()\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {e} Acc: {(1.*correct_both/total).item():.3}')\n",
    "            \n",
    "            del G, gs, y\n",
    "            del h_docs, pred_docs, h_labels\n",
    "            del pred_docs2, pred_docs3\n",
    "        to_print   = f'EVAL:  Acc Cls: {(1.*correct_class/total).item():.3} '\n",
    "        to_print  += f'Repr: {(1.*correct_repre/total).item():.3} '\n",
    "        to_print  += f'Both: {(1.*correct_both/total).item():.3}'\n",
    "        print(to_print, end=f\"{' '*100}\\r\")\n",
    "    del data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACM\n",
    "# (L)oss: 6.05 NP-L: 4.41 CE-L: 1.64 Sp-L: 0.0 Acc Cls: 0.922 Repr: 0.907 Both: 0.932                                                                                                                       \n",
    "# ALL: [ *77.84, *79.92, 70.70, 66.85, 67.33, 50.46, 76.20, 74.10, 74.20, 76.73, 73.10, 76.60, *78.08, 76.99]\n",
    "\n",
    "#reut\n",
    "# (L)oss: 9.44 NP-L: 3.34 CE-L: 3.88 Sp-L: 2.22 Acc Cls: 0.732 Repr: 0.639 Both: 0.662                                                                                                                        \n",
    "\n",
    "# 20ng\n",
    "# (L)oss: 4.41 NP-L: 2.2 CE-L: 2.18 Sp-L: 0.0226 Acc Cls: 0.917 Repr: 0.896 Both: 0.915                \n",
    "\n",
    "# WebKB\n",
    "# (L)oss: 4.94 NP-L: 3.64 CE-L: 1.31 Sp-L: 0.0 Acc Cls: 0.867 Repr: 0.858 Both: 0.865                                                                                                                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webkb (MaxF1=85.54)\n",
    "# exp     (b=128) webkb (L)oss: 5.53 NP-L: 3.68 CE-L: 1.34 Sp-L: 0.521 Acc Cls: 0.847 Repr: 0.831 Both: 0.837                                                                                                                        \n",
    "# exp     (b=16)  webkb (L)oss: 4.39 NP-L: 1.90 CE-L: 1.38 Sp-L: 1.110 Acc Cls: 0.743 Repr: 0.782 Both: 0.784                                                                                                                         \n",
    "# (b=512, nh=1)   webkb (L)oss: 8.96 NP-L: 5.00 CE-L: 1.33 Sp-L: 2.620 Acc Cls: 0.858 Repr: 0.833 Both: 0.843                                                                                                                         \n",
    "\n",
    "\n",
    "# ACM (MaxF1=79.92)\n",
    "# exp           acm   (L)oss: 4.94 NP-L: 3.17 CE-L: 1.69 Sp-L: 0.085 Acc Cls: 0.870 Repr: 0.856 Both: 0.885    \n",
    "# exp           acm   (L)oss: 4.88 NP-L: 3.05 CE-L: 1.61 Sp-L: 0.220 Acc Cls: 0.915 Repr: 0.929 Both: 0.931    \n",
    "# (b=512, nh=1) acm   (L)oss: 6.15 NP-L: 4.51 CE-L: 1.64 Sp-L: 0.000 Acc Cls: 0.882 Repr: 0.902 Both: 0.904    \n",
    "# (b=512, nh=2) acm   \n",
    "\n",
    "# reut (MaxF1=79.11)\n",
    "# exp           reut (L)oss: 8.15 NP-L: 2.80 CE-L: 3.87 Sp-L: 1.48 Acc Cls: 0.665 Repr: 0.651 Both: 0.658\n",
    "# (b=512, nh=1) reut (L)oss: 10.9 NP-L: 4.09 CE-L: 3.92 Sp-L: 2.90 Acc Cls: 0.673 Repr: 0.600 Both: 0.620                                                                                                                            \n",
    "# (b=512, nh=2) reut (L)oss: 11.2 NP-L: 4.06 CE-L: 3.87 Sp-L: 3.32 Acc Cls: 0.690 Repr: 0.650 Both: 0.668                                                                                                                          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = h_labels\n",
    "\n",
    "\n",
    "L = torch.matmul(hiddens, hiddens.T)\n",
    "L = F.sigmoid(L)\n",
    "L_mapper = (L >= L.diag()).float()\n",
    "ind = torch.eye(L_mapper.size(0),L_mapper.size(1)).to(L_mapper.device)\n",
    "L_mapper *= (1-ind)\n",
    "L_mapper = F.normalize(L_mapper)\n",
    "values = (L_mapper * L).sum(axis=1)\n",
    "svalue = max((values > 0.).sum(), 0.0000001)\n",
    "\n",
    "values.sum()/svalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = h_labels\n",
    "\n",
    "\n",
    "L = torch.matmul(hiddens, hiddens.T)\n",
    "L = F.sigmoid(L)\n",
    "L = (L - L.diag()).float()\n",
    "L = F.relu(L)\n",
    "L = F.normalize(L)\n",
    "values = L.sum(axis=1)\n",
    "svalue = max((values > 0.).sum(), 0.0000001)\n",
    "\n",
    "values.sum()/svalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svalue = max((values > 0.).sum(), 0.)\n",
    "svalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svalues = (values > 0.).sum()\n",
    "values = values.sum()/svalues\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svalue = torch.max(, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(values > 0.).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
