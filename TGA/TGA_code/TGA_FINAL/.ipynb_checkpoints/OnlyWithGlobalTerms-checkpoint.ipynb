{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import Dataset, GraphsizePretrained\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('X_train', 'y_train', 'X_test', 'y_test'), 630000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset('/home/mangaravite/Documentos/datasets/classification/datasets/yelp_2015/')\n",
    "fold = next(dataset.get_fold_instances(10, with_val=False))\n",
    "fold._fields, len(fold.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14498.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 818 ms, total: 24.3 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder = GraphsizePretrained(w=2, verbose=True,\n",
    "                   pretrained_vec='/home/mangaravite/Documentos/pretrained_vectors/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import networkx as nx\n",
    "from dgl.nn.pytorch.conv import GraphConv, GATConv\n",
    "from dgl.nn.pytorch.glob import GlobalAttentionPooling\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [04:18<00:00, 2436.92it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_builder.fit(fold.X_train, fold.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph_builder.g.edges), len(graph_builder.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: (x,graph_builder.g.degree()[x]), graph_builder.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 drop=.5, n_heads=8, attn_drop=.5,\n",
    "                 activation=F.leaky_relu, n_convs=2,\n",
    "                 first_hidden='emb', encoders={'term','label'},\n",
    "                 device='cpu:0'):\n",
    "        super(GenericGAT, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first_hidden = first_hidden\n",
    "        \n",
    "        self.encoders = nn.ModuleDict({\n",
    "            k: nn.Linear(in_dim, hidden_dim).to(self.device) for k in encoders\n",
    "        })\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GATConv(hidden_dim, hidden_dim, residual=True, num_heads=n_heads, activation=activation,\n",
    "                    feat_drop=drop, attn_drop=attn_drop).to(self.device) for _ in range(n_convs)\n",
    "        ])\n",
    "        self.down_proj = [\n",
    "            nn.Linear(n_heads*hidden_dim, hidden_dim).to(self.device) for _ in range(n_convs)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, G, **kwargs):\n",
    "        with G.local_scope():\n",
    "            h = G.ndata[self.first_hidden].float()\n",
    "            for (k, mask) in kwargs.items():\n",
    "                if k in self.encoders:\n",
    "                    if mask is not None:\n",
    "                        h[ mask ] = self.encoders[k]( h[ mask ] )\n",
    "                    else:\n",
    "                        h = self.encoders[k]( h )\n",
    "\n",
    "            for l, conv in enumerate(self.layers):\n",
    "                h = conv(G, h)\n",
    "                h = h.view(h.shape[0], -1)\n",
    "                h = self.down_proj[l]( h )\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 300\n",
    "input_l = 300\n",
    "n_heads = 1\n",
    "drop=0.1\n",
    "attn_drop=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GenericGAT( input_l, hidden_l, n_heads=n_heads,\n",
    "                 drop=drop, attn_drop=attn_drop,\n",
    "                 activation=None, device='cuda:0' ).to(torch.device('cuda:0'))\n",
    "norm = nn.BatchNorm1d(hidden_l).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_graph_dgl = dgl.DGLGraph()\n",
    "big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "big_graph_dgl = big_graph_dgl.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CosineEmbeddingLoss(reduction='mean').to(torch.device('cuda:0'))\n",
    "optimizer = optim.Adam( gat.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458143408c7d4432aa6be93ab96dd8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.80502 min_loss: 1.80502\n",
      "epoch: 1, loss: 1.48892 min_loss: 1.48892\n",
      "epoch: 2, loss: 1.38716 min_loss: 1.38716\n",
      "epoch: 3, loss: 1.43224 min_loss: 1.38716\n",
      "epoch: 4, loss: 1.36546 min_loss: 1.36546\n",
      "epoch: 5, loss: 1.33608 min_loss: 1.33608\n",
      "epoch: 6, loss: 1.29218 min_loss: 1.29218\n",
      "epoch: 7, loss: 1.34255 min_loss: 1.29218\n",
      "epoch: 8, loss: 1.49798 min_loss: 1.29218\n",
      "epoch: 9, loss: 1.55063 min_loss: 1.29218\n",
      "epoch: 10, loss: 1.47628 min_loss: 1.29218\n",
      "epoch: 11, loss: 1.26797 min_loss: 1.26797\n",
      "epoch: 12, loss: 1.22883 min_loss: 1.22883\n",
      "epoch: 13, loss: 1.5424 min_loss: 1.22883\n",
      "epoch: 14, loss: 1.73784 min_loss: 1.22883\n",
      "epoch: 15, loss: 1.83025 min_loss: 1.22883\n",
      "epoch: 16, loss: 1.86778 min_loss: 1.22883\n",
      "epoch: 17, loss: 1.88311 min_loss: 1.22883\n",
      "epoch: 18, loss: 1.88669 min_loss: 1.22883\n",
      "epoch: 19, loss: 1.88263 min_loss: 1.22883\n",
      "epoch: 20, loss: 1.88381 min_loss: 1.22883\n",
      "epoch: 21, loss: 1.8828 min_loss: 1.22883\n",
      "epoch: 22, loss: 1.85857 min_loss: 1.22883\n",
      "epoch: 23, loss: 1.8307 min_loss: 1.22883\n",
      "epoch: 24, loss: 1.80189 min_loss: 1.22883\n",
      "epoch: 25, loss: 1.72537 min_loss: 1.22883\n",
      "epoch: 26, loss: 1.57169 min_loss: 1.22883\n",
      "epoch: 27, loss: 1.46054 min_loss: 1.22883\n",
      "epoch: 28, loss: 1.77345 min_loss: 1.22883\n",
      "epoch: 29, loss: 1.80172 min_loss: 1.22883\n",
      "epoch: 30, loss: 1.82471 min_loss: 1.22883\n",
      "epoch: 31, loss: 1.81603 min_loss: 1.22883\n",
      "epoch: 32, loss: 1.77502 min_loss: 1.22883\n",
      "epoch: 33, loss: 1.56613 min_loss: 1.22883\n",
      "epoch: 34, loss: 1.55925 min_loss: 1.22883\n",
      "epoch: 35, loss: 1.63445 min_loss: 1.22883\n",
      "epoch: 36, loss: 1.68032 min_loss: 1.22883\n",
      "epoch: 37, loss: 1.68008 min_loss: 1.22883\n",
      "epoch: 38, loss: 1.5364 min_loss: 1.22883\n",
      "epoch: 39, loss: 1.42326 min_loss: 1.22883\n",
      "epoch: 40, loss: 1.44901 min_loss: 1.22883\n",
      "epoch: 41, loss: 1.48111 min_loss: 1.22883\n",
      "epoch: 42, loss: 1.39215 min_loss: 1.22883\n",
      "epoch: 43, loss: 1.43752 min_loss: 1.22883\n",
      "epoch: 44, loss: 1.38783 min_loss: 1.22883\n",
      "epoch: 45, loss: 1.43392 min_loss: 1.22883\n",
      "epoch: 46, loss: 1.47827 min_loss: 1.22883\n",
      "epoch: 47, loss: 1.12112 min_loss: 1.12112\n",
      "epoch: 48, loss: 1.20296 min_loss: 1.12112\n",
      "epoch: 49, loss: 1.26916 min_loss: 1.12112\n",
      "epoch: 50, loss: 1.24375 min_loss: 1.12112\n",
      "epoch: 51, loss: 1.195 min_loss: 1.12112\n",
      "epoch: 52, loss: 1.20016 min_loss: 1.12112\n",
      "epoch: 53, loss: 1.15117 min_loss: 1.12112\n",
      "epoch: 54, loss: 1.17652 min_loss: 1.12112\n",
      "epoch: 55, loss: 1.19992 min_loss: 1.12112\n",
      "epoch: 56, loss: 1.16668 min_loss: 1.12112\n",
      "epoch: 57, loss: 1.1809 min_loss: 1.12112\n",
      "epoch: 58, loss: 1.15296 min_loss: 1.12112\n",
      "epoch: 59, loss: 1.15363 min_loss: 1.12112\n",
      "epoch: 60, loss: 1.10303 min_loss: 1.10303\n",
      "epoch: 61, loss: 1.14008 min_loss: 1.10303\n",
      "epoch: 62, loss: 1.12948 min_loss: 1.10303\n",
      "epoch: 63, loss: 1.15678 min_loss: 1.10303\n",
      "epoch: 64, loss: 1.14324 min_loss: 1.10303\n",
      "epoch: 65, loss: 1.09004 min_loss: 1.09004\n",
      "epoch: 66, loss: 1.13338 min_loss: 1.09004\n",
      "epoch: 67, loss: 1.08703 min_loss: 1.08703\n",
      "epoch: 68, loss: 1.12605 min_loss: 1.08703\n",
      "epoch: 69, loss: 1.06251 min_loss: 1.06251\n",
      "epoch: 70, loss: 1.07989 min_loss: 1.06251\n",
      "epoch: 71, loss: 1.14821 min_loss: 1.06251\n",
      "epoch: 72, loss: 1.13415 min_loss: 1.06251\n",
      "epoch: 73, loss: 1.06764 min_loss: 1.06251\n",
      "epoch: 74, loss: 1.13963 min_loss: 1.06251\n",
      "epoch: 75, loss: 1.12203 min_loss: 1.06251\n",
      "epoch: 76, loss: 1.14258 min_loss: 1.06251\n",
      "epoch: 77, loss: 1.13714 min_loss: 1.06251\n",
      "epoch: 78, loss: 1.1053 min_loss: 1.06251\n",
      "epoch: 79, loss: 1.14922 min_loss: 1.06251\n",
      "epoch: 80, loss: 1.10684 min_loss: 1.06251\n",
      "epoch: 81, loss: 1.11289 min_loss: 1.06251\n",
      "epoch: 82, loss: 1.12116 min_loss: 1.06251\n",
      "epoch: 83, loss: 1.08919 min_loss: 1.06251\n",
      "epoch: 84, loss: 1.19888 min_loss: 1.06251\n",
      "epoch: 85, loss: 1.14497 min_loss: 1.06251\n",
      "epoch: 86, loss: 1.13084 min_loss: 1.06251\n",
      "epoch: 87, loss: 1.08541 min_loss: 1.06251\n",
      "epoch: 88, loss: 1.05644 min_loss: 1.05644\n",
      "epoch: 89, loss: 1.23799 min_loss: 1.05644\n",
      "epoch: 90, loss: 0.985554 min_loss: 0.985554\n",
      "epoch: 91, loss: 1.15039 min_loss: 0.985554\n",
      "epoch: 92, loss: 1.23698 min_loss: 0.985554\n",
      "epoch: 93, loss: 1.05793 min_loss: 0.985554\n",
      "epoch: 94, loss: 1.08244 min_loss: 0.985554\n",
      "epoch: 95, loss: 1.06095 min_loss: 0.985554\n",
      "epoch: 96, loss: 1.07621 min_loss: 0.985554\n",
      "epoch: 97, loss: 0.996192 min_loss: 0.985554\n",
      "epoch: 98, loss: 1.18661 min_loss: 0.985554\n",
      "epoch: 99, loss: 1.11803 min_loss: 0.985554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gat.train()\n",
    "best = None\n",
    "label_idx = big_graph_dgl.ndata['label'].nonzero().flatten()\n",
    "nepochs = 100\n",
    "for e in tqdm(range(nepochs), total=nepochs):\n",
    "    h = gat(big_graph_dgl, label=label_idx, term=range(max(label_idx),len(graph_builder.g)))\n",
    "    h = norm(h)\n",
    "    labels_hiddens = h[label_idx]\n",
    "    A = []\n",
    "    B = []\n",
    "    y = []\n",
    "    for i in range(labels_hiddens.size()[0]):\n",
    "        for j in range(labels_hiddens.size()[0]):\n",
    "            if i != j:\n",
    "                A.append( labels_hiddens[i] )\n",
    "                B.append( -1.*labels_hiddens[j] )\n",
    "                y.append( 1 )\n",
    "\n",
    "    B=torch.cat(B).reshape( len(y), hidden_l ).to(torch.device('cuda:0'))\n",
    "    A=torch.cat(A).reshape( len(y), hidden_l ).to(torch.device('cuda:0'))\n",
    "    y=torch.Tensor(y).to(torch.device('cuda:0'))\n",
    "    loss = loss_func(A, B, target=y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if best is None or loss.item() < best:\n",
    "        best = loss.item()\n",
    "        h_best = h\n",
    "    print(f\"epoch: {e}, loss: {loss.item():.6} min_loss: {best:.6}\")\n",
    "    del labels_hiddens, loss, A, B, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5354, -0.5028, -0.5080,  ..., -0.4964,  0.4754, -0.4008],\n",
       "        [ 0.7030, -1.2909, -1.6529,  ..., -1.6897,  1.6250, -1.7665],\n",
       "        [-0.7637,  0.8068,  0.9326,  ...,  1.0443, -0.9287,  0.9200],\n",
       "        ...,\n",
       "        [ 0.7127, -1.3228, -1.6368,  ..., -1.6751,  1.6088, -1.7635],\n",
       "        [ 0.3317, -0.2485, -0.9960,  ..., -1.3316,  0.9007, -0.9528],\n",
       "        [ 0.7035, -1.2930, -1.6516,  ..., -1.6952,  1.6156, -1.7668]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                        # embedding dos termos novos\n",
    "h_best[max(label_idx):] # o mapeamento é (o inverso de) node_mapper[big_graph.ndata['idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1972, -1.4628, -1.6480,  ..., -1.8050,  1.4865, -1.6164],\n",
       "        [-0.2668,  0.2272,  0.3561,  ...,  0.4399, -0.4981,  0.4651],\n",
       "        [-1.0959,  0.7677,  0.9481,  ..., -0.3196, -1.4137,  1.3107],\n",
       "        ...,\n",
       "        [-0.1302, -0.9735, -0.3024,  ..., -0.3871,  0.1087, -0.0579],\n",
       "        [ 1.1995, -1.3963, -1.7410,  ..., -1.7015,  1.5383, -1.6510],\n",
       "        [ 0.5354, -0.5028, -0.5080,  ..., -0.4964,  0.4754, -0.4008]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_best[label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7596,  0.7932,  0.9402,  ...,  1.0531, -0.9367,  0.9215],\n",
       "        [-0.7945,  0.8056,  0.9314,  ...,  1.0430, -0.9239,  0.9240],\n",
       "        [-0.7648,  0.8128,  0.9231,  ...,  1.0358, -0.9195,  0.9217],\n",
       "        [-0.7666,  0.8024,  0.9314,  ...,  1.0409, -0.9189,  0.9247],\n",
       "        [-0.7519,  0.8076,  0.9358,  ...,  1.0398, -0.9330,  0.9264]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_best[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0614,  0.8113, -0.3445,  ..., -0.5139,  0.1250,  0.2937],\n",
       "        [ 0.2751,  0.5761,  0.0962,  ..., -0.3611, -0.0418, -0.3528],\n",
       "        [-0.4215,  0.6595,  0.2708,  ..., -0.5453,  0.2746, -0.1018],\n",
       "        [ 0.1521,  0.5283,  0.4525,  ..., -0.4160,  0.2137, -0.1511],\n",
       "        [ 0.3702, -0.2849,  0.7021,  ..., -0.6174, -0.0713, -0.2659]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_graph_dgl.ndata['emb'][100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zdfgsrvsdtvhsfhvg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-828a336d0d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzdfgsrvsdtvhsfhvg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'zdfgsrvsdtvhsfhvg' is not defined"
     ]
    }
   ],
   "source": [
    "zdfgsrvsdtvhsfhvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = h_best[label_idx].detach().numpy().T\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x,y = h_best[max(label_idx):].detach().numpy().T\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = big_graph_dgl.ndata['label'].nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = gat(big_graph_dgl)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hiddens = h[label_idx]\n",
    "labels_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "B = []\n",
    "y = []\n",
    "for i in range(labels_hiddens.size()[0]):\n",
    "    for j in range(labels_hiddens.size()[0]):\n",
    "        if i != j:\n",
    "            A.append( labels_hiddens[i] )\n",
    "            B.append( -1.*labels_hiddens[j] )\n",
    "            y.append( 1 )\n",
    "            \n",
    "B=torch.cat(B).reshape( len(y), 300 )\n",
    "A=torch.cat(A).reshape( len(y), 300 )\n",
    "y=torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(B, A, target=y), loss_func(A, B, target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGA(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_class,\n",
    "                  n_heads=8, drop=.5, attn_drop=.5,\n",
    "                  device='cuda:0'):\n",
    "        super(TGA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.device = torch.device(device)\n",
    "        self.gat_global = GenericGAT(in_dim, hidden_dim, \n",
    "                                     encoders={'label'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "        \n",
    "        self.gat_local  = GenericGAT(hidden_dim, hidden_dim, \n",
    "                                     encoders={'term'}, \n",
    "                                     n_heads=n_heads, drop=drop,\n",
    "                                     first_hidden='emb',\n",
    "                                     attn_drop=attn_drop, device=self.device)\n",
    "\n",
    "        self.lin = nn.Linear( hidden_dim, 1).to(self.device)\n",
    "        # Depois tentar alguma ativação (ReLU, por exemplo, pode \"desativar\" alguns termos no softmax)\n",
    "        self.pooling = GlobalAttentionPooling( self.lin ).to(self.device)\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear( hidden_dim, hidden_dim//2).to(self.device)\n",
    "        self.fc2 = nn.Linear( hidden_dim//2, hidden_dim//4).to(self.device)\n",
    "        self.fc3 = nn.Linear( hidden_dim//4, self.n_class).to(self.device)\n",
    "    def forward(self, G, gs):\n",
    "        #h_global           = self.gat_global( G, label=G.ndata['label'].nonzero().flatten() )\n",
    "        #gs.ndata['weight'] = h_global[ gs.ndata['idx'] ] # Tentar concatenando\n",
    "        h_local            = self.gat_local(gs, term=None)\n",
    "        #h_local            = torch.cat((h_local, h_global[ gs.ndata['idx'] ]), 1)\n",
    "        h_local            = self.pooling( gs, h_local )\n",
    "        h_local            = self.fc1( h_local )\n",
    "        h_local            = self.fc2( h_local )\n",
    "        h_local            = self.fc3( h_local )\n",
    "        return h_local\n",
    "# torch.Size([3652, 300]) torch.Size([3652, 300]) torch.Size([128, 300])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=300\n",
    "hidden_dim=2\n",
    "n_heads=8\n",
    "drop=0.3\n",
    "attn_drop=0.5\n",
    "batch_size=128\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TGA( in_dim, hidden_dim, graph_builder.n_class,\n",
    "            n_heads=n_heads, drop=drop, attn_drop=attn_drop )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(param):\n",
    "    X, y = zip(*param)\n",
    "    Gs_nx = graph_builder.transform(X)\n",
    "    \n",
    "    Gs_dgl_list = []\n",
    "    idx_terms = { l for l in graph_builder.label_ids }\n",
    "    for g in Gs_nx:\n",
    "        g_dgl = dgl.DGLGraph()\n",
    "        g_dgl.from_networkx(g, node_attrs=['emb', 'idx'] )\n",
    "        Gs_dgl_list.append( g_dgl )\n",
    "        \n",
    "        idx_terms = idx_terms.union( set(nx.get_node_attributes(g,'idx').values()) )\n",
    "    \n",
    "    Gs_dgl = dgl.batch(Gs_dgl_list)\n",
    "    \n",
    "    big_graph_dgl = dgl.DGLGraph()\n",
    "    big_graph_dgl.from_networkx(graph_builder.g, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    #subgraph = graph_builder.g.subgraph(idx_terms)\n",
    "    #big_graph_dgl.from_networkx(subgraph, node_attrs=['emb', 'label', 'idx'] )\n",
    "    \n",
    "    return big_graph_dgl, Gs_dgl, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = optim.AdamW( model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "#optimizer = optim.RMSprop( model.parameters(), lr=0.0001 )\n",
    "\n",
    "model.train()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(list(zip(fold.X_train, fold.y_train)), batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate, num_workers=2)\n",
    "    with tqdm(total=len(fold.y_train)) as pbar:\n",
    "        total = 1\n",
    "        correct = 1\n",
    "        model.train()\n",
    "        for G, gs, y in data_loader:\n",
    "            G = G.to( torch.device('cuda:0') )\n",
    "            gs = gs.to( torch.device('cuda:0') )\n",
    "            y = y.to( torch.device('cuda:0') )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model( G, gs )\n",
    "            probs_Y = torch.softmax(outputs, 1)\n",
    "            sampled_Y = torch.argmax(probs_Y, 1).reshape(-1)\n",
    "            \n",
    "            total += y.size(0)\n",
    "            correct += (sampled_Y == y).sum().item()\n",
    "            \n",
    "            del probs_Y, sampled_Y, G, gs\n",
    "            \n",
    "            # NN backprop phase\n",
    "            loss = loss_func(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            pbar.update( len(y) )\n",
    "            pbar.set_description_str(f'iter {epoch} Acc train: {correct/total:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_nodes_from( [ (0, {'idx': 0}), (1, {'idx': 1}), (2, {'idx': 2}) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.get_node_attributes(g,'idx').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hiddens = torch.eye( 11 )\n",
    "labels_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
